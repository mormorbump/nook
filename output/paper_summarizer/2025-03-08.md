
# LLM as a Broken Telephone: Iterative Generation Distorts Information

[View Paper](http://arxiv.org/abs/2502.20258v1)

## 1. 既存研究では何ができなかったのか

既存研究は、LLMによる反復生成における情報歪曲の包括的な分析が不足していました。具体的には、以下の点が不十分でした。

*   **翻訳タスクにおける情報歪曲の加速**: 既存研究では、LLMの重要な応用である翻訳における反復的な情報歪曲の加速について十分に調査されていませんでした。
*   **異種モデルチェーンの分析**: 既存研究は、単一モデルの反復に焦点を当てており、異なるモデルが連鎖的に連携する場合の情報歪曲への影響を考慮していませんでした。
*   **複雑な言い換えチェーンへの拡張**: 既存研究は、言い換え、継続、刺激的なタスクにおけるテキスト特性の進化を分析しましたが、翻訳という重要なLLMアプリケーションを見落としていました。
*   **テキストの類似性と事実性の体系的な評価**: 既存研究では、反復的なLLM伝送チェーンにおける毒性、積極性、難易度、長さの変化を探求しましたが、テキストの類似性と事実性の体系的な評価は見過ごされていました。特に複数の反復にわたる累積的な劣化を捉えられていませんでした。
*   **マルチエージェント設定への考慮**: 既存研究は、LLM間の協調的相互作用を活用するコミュニケーションフレームワークにおける情報保持の前提を疑問視していませんでした。
*   **多様なドメインにおける検証**: ニュース、書籍、映画脚本という比較的一般的なドメインに焦点を当てており、専門的なドメインにおける検証が不足していました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、LLMにおける反復生成による情報歪曲を調査するために、以下の多角的なアプローチを採用しました。

1.  **翻訳ベースの実験**: 「broken telephone」ゲームを模倣し、LLMによる翻訳タスクを通じて情報歪曲を測定しました。英語のドキュメントを複数言語に翻訳し、再度英語に戻すプロセスを反復し、元のドキュメントとの関連性と事実性を比較しました。

2.  **実験設定**: 以下の3つの実験設定を構築しました。
    *   **Bilingual Self-Loop**: 単一のモデルが英語とフランス語またはタイ語の間で反復的に翻訳します。
    *   **Bilingual Two-Player**: 2つの異なるモデルが、英語とフランス語または英語とタイ語の翻訳チェーンで連携します。
    *   **Multilingual Multiplayer**: 複数の言語とモデルを含む複雑な翻訳チェーンを構築し、言語とモデルの多様性の増加が情報歪曲を加速するかどうかを調べます。

3.  **評価指標**: テキストの関連性と事実性の2つの評価指標を用いました。
    *   **テキストの関連性**: BLEU、METEOR、chrF、BERTScore を使用して、語彙、構文、意味のずれを定量化しました。
    *   **事実性**: FActScore を使用して、生成されたテキストが元の情報にどれだけ忠実であるかを評価しました。

4.  **パラメータ調整**:
    *   **Temperature**: モデルの生成温度を調整し、情報歪曲への影響を評価しました。
    *   **プロンプト**: プロンプトの制約レベルを変更し、モデルの生成に対する影響を分析しました。制約の少ないプロンプトはより創造的な出力を促し、制約の多いプロンプトは元の意味からの逸脱を抑制します。

5.  **データセット**: ニュース記事 (News2024)、書籍の要約 (BookSum)、映画の脚本など、異なるドメインのデータセットを使用しました。

6.  **数式 (Python風疑似コード)**:

```python
def translation_chain(document, languages, models, iterations):
  """
  反復翻訳チェーンをシミュレートする関数
  Args:
    document: 翻訳する元のドキュメント
    languages: 翻訳に使用する言語のリスト (例: ["EN", "FR", "DE", "EN"])
    models: 各翻訳ステップで使用するモデルのリスト
    iterations: 翻訳を繰り返す回数
  Returns:
    翻訳されたドキュメントのリスト (各反復後)
  """
  translated_documents = [document] # 初期ドキュメントをリストに追加
  current_document = document

  for i in range(iterations):
    # 各言語ペアと言語に対応するモデルで翻訳
    for j in range(len(languages) - 1):
      source_language = languages[j]
      target_language = languages[j+1]
      model = models[j]

      # source_language から target_language へ翻訳
      current_document = translate(current_document, source_language, target_language, model)

    translated_documents.append(current_document)
  return translated_documents

def translate(document, source_language, target_language, model):
  """
  ドキュメントをある言語から別の言語に翻訳する関数
  Args:
    document: 翻訳するドキュメント
    source_language: 元の言語
    target_language: 翻訳先の言語
    model: 使用する翻訳モデル
  Returns:
    翻訳されたドキュメント
  """
  # 翻訳モデルにドキュメント、元の言語、翻訳先の言語を入力
  translated_document = model.translate(document, source_language, target_language)
  return translated_document
```

## 3. 結果、何が達成できたのか

本研究により、以下の成果が得られました。

1.  **情報歪曲の確認**: LLMによる反復生成が、人間のコミュニケーションにおける「broken telephone」効果と同様に、徐々に情報歪曲を引き起こすことを確認しました。

2.  **言語の影響**: 中間言語の選択が情報歪曲の程度に影響を与えることを示しました。ソース言語との言語的類似性が高い言語ほど歪曲が少なく、類似性が低い言語ほど歪曲が大きいことがわかりました。

3.  **チェーンの複雑さの影響**: チェーンの複雑さが増すほど、歪曲が増幅されることを示しました。言語やモデルを追加すると、反復チェーンの種類に関係なく、劣化が進みました。

4.  **歪曲軽減策**: モデルの生成温度の制御と制約のあるプロンプトの使用により、情報歪曲を軽減できることを示しました。

5.  **モデル間の相互作用**: 異なるモデルが協調して翻訳する場合、情報保持に影響を与えることを示しました。言語によっては、協調により歪曲が増加する一方、別の言語では歪曲が減少しました。

6.  **事実性劣化速度**: 実験設定ごとに事実性の劣化速度が異なり、複数の言語とモデルを含む設定が最も急激な劣化を示しました。

## 4. Limitationや問題点は何か

本研究には、以下の制限事項と問題点があります。

*   **データセットの一般性**: 使用したデータセット (ニュース、書籍、映画脚本) は類似の特性を持ち、特殊なドメインに特有のまれな情報やロングテール情報を捉えられていない可能性があります。特殊なドメインからのデータセットでは、観察された情報歪曲のレベルが異なる可能性があります。
*   **モデルサイズの制限**: 計算リソースの制約により、70億から90億のパラメータを持つモデルに焦点を当てました。より大きなモデルは異なる挙動を示す可能性があり、将来の研究で調査する必要があります。
*   **固定されたプロンプト**: プロンプトエンジニアリングはモデルのパフォーマンスに大きな影響を与えます。本研究で使用したプロンプトが最適とは限らず、プロンプトの種類による影響を完全に排除できていません。
*   **評価指標の限界**: 使用した評価指標 (BLEU、METEOR、FActScoreなど) は、特定の種類の情報歪曲を捉えるのに適していますが、人間の判断との完全な相関はありません。
*   **翻訳以外のタスクへの一般化**: 主に翻訳タスクに焦点を当てています。この結果が、要約やテキスト生成など、他の反復生成タスクにどの程度一般化できるかは不明です。
*   **ドメイン知識の欠如**: モデルが特定のドメインの知識を持っていない場合、反復処理によって事実が歪曲されやすくなる可能性があります。ドメイン知識を注入することで、この問題を軽減できるかどうかの調査が必要です。
*   **倫理的な懸念**: 本研究は情報歪曲に焦点を当てていますが、モデルが生成するコンテンツのバイアスやハルシネーションの問題は考慮されていません。反復生成がこれらの問題をどのように悪化させるかを調査する必要があります。

## 5. 技術的な詳細について

*   **モデル**: 実験には、70億から90億のパラメータを持つLLMを使用しました (具体的なモデル名は本文参照)。
*   **翻訳パイプライン**: 英語から中間言語、そして英語に戻す翻訳プロセスは、LLMのAPIを介して実行しました。
*   **評価**: テキスト関連性評価には、BLEU、METEOR、chrF、BERTScore を使用しました。事実性評価には、FActScore を使用しました。
*   **温度**: モデルの出力の多様性を制御するために、さまざまな温度設定 (0.000001, 0.25, 0.5, 0.75, 1.0) を試しました。
*   **プロンプト**: 異なる制約レベルを持つプロンプトを使用しました (Simple, Base, Constrained)。

## 6. コストや物理的な詳細について

*   **GPU**: NVIDIA A100 (40GB VRAM) および A10 (24GB VRAM) GPUクラスターを使用しました。
*   **計算時間**: 合計で 54 GPU日 (8xA100ノードで 36 GPU日、4xA10ノードで 18 GPU日) を使用しました。
*   **データセット**: データセットのサイズは、BookSum、Movie Script Summarization、News2024それぞれ150ドキュメントで、各ドキュメントは100〜200単語です。
*   **モデルサイズ**: 本文中にモデルサイズの詳細な記載はありませんが、70億から90億のパラメータを持つモデルが使用された旨が記述されています。

## 7. 参考文献のうち、特に参照すべきもの

以下の参考文献は、本研究の理解を深めるために特に重要です。

*   **FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.** (Min et al., 2023): 本研究で使用した事実性評価指標であるFActScoreの詳細。
*   **The curse of recursion: Training on generated data makes models forget.** (Shumailov et al., 2023): 自己生成データで学習したモデルの崩壊に関する研究。
*   **When llms play the telephone game: Cumulative changes and attractors in iterated cultural transmissions.** (Perez et al., 2024): LLMにおける反復的な文化伝播に関する研究。
*   **METEOR: An automatic metric for MT evaluation with improved correlation with human judgments** (Banerjee and Lavie, 2005): テキスト関連性評価に使用したMETEORの詳細。
*   **BERTScore: Evaluating text generation with bert** (Zhang et al., 2019): テキスト関連性評価に使用したBERTScoreの詳細。

## 8. この論文を140字以内のツイートで要約すると？

LLMが自身の出力を繰り返し処理すると情報が歪む！翻訳実験で、言語の類似性や連鎖の複雑さが歪みに影響。温度制御やプロンプトで軽減可能。AI生成コンテンツの信頼性に警鐘🚨 #LLM #AI #自然言語処理 #情報歪曲


---


# Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer

[View Paper](http://arxiv.org/abs/2503.02495v2)

## 1. 既存研究では何ができなかったのか

この論文のAbstractのみから判断すると、既存研究のMoE（Mixture of Experts）モデルや、Full Attentionモデル、効率的なTransformerモデル（DeepSeek-V3を含む）は、計算コストと性能のバランスにおいて最適とは言えませんでした。既存のMoEモデルは、高い性能を達成するために多くの計算リソースを必要とする場合があり、Full Attentionモデルや効率的なTransformerモデルは、特定のタスクにおいて性能が劣る可能性があります。つまり、既存研究では、計算効率と性能の両方を同時に高いレベルで達成することが困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

著者らは、Union-of-Experts (UoE)という新しいアプローチを提案しました。これは、Transformerを等価なエキスパートグループに分解し、入力データとエキスパートに対して選択的なルーティングを実装するというものです。具体的には、以下の4つの主要なイノベーションを取り入れています。

1.  **エキスパートの等価分解:** 行列分割に基づき、MLPブロックとAttentionブロックの両方でエキスパートの等価分解を行いました。これは、モデルを並列処理に適した形に分割し、効率的な計算を可能にするためです。
2.  **ルーティングパラダイム:** パッチ単位のデータ選択とエキスパート選択という2つのルーティングパラダイムを開発し、異なるレベルでルーティングを適用しました。これにより、入力データに応じて適切なエキスパートを選択的に利用し、計算コストを削減できます。
3.  **UoEモデルのアーキテクチャ:** Selective Multi-Head Attention (SMHA)とUnion-of-MLP-Experts (UoME)を含むUoEモデルのアーキテクチャを設計しました。SMHAは、Attentionメカニズムを選択的に適用し、UoMEは、MLP層をエキスパートの連合として構成します。
4.  **並列実装:** UoEのルーティングと計算操作の並列実装を開発し、ハードウェア処理の分析に基づいて効率を最適化しました。

## 3. 結果、何が達成できたのか

実験結果から、UoEモデルは、画像および自然言語領域の複数のタスクにおいて、Full Attention、最先端のMoE、および効率的なTransformer（DeepSeek-V3を含む）を上回ることが示されました。

*   **言語モデリング:** 既存の最高のMoE手法と比較して、平均で2.38のperplexity削減を、平均76%のFLOPsで達成しました。
*   **Long Range Arenaベンチマーク:** Full Attention、MoE、およびTransformerバリアントを含むすべての比較モデルよりも、平均で少なくとも0.68%高いスコアを、最高のMoE手法のわずか50%のFLOPsで記録しました。
*   **画像分類:** 既存の最高モデルよりも平均で1.75%高い精度向上を、同等のFLOPsで達成しました。

## 4. Limitationや問題点は何か

Abstractのみからは、具体的なLimitationや問題点は明らかではありません。しかし、一般的にMoEモデルやルーティングメカニズムには以下のような課題が考えられます。

*   **ルーティングのオーバーヘッド:** ルーティング処理自体に計算コストがかかる可能性があります。特に、複雑なルーティングアルゴリズムを使用する場合、そのオーバーヘッドは無視できません。
*   **エキスパート間の負荷分散:** 各エキスパートへの負荷が均等に分散されない場合、一部のエキスパートがボトルネックとなり、全体の性能が低下する可能性があります。
*   **学習の不安定性:** MoEモデルは、学習が不安定になる傾向があります。特に、エキスパートの初期化やルーティングパラメータの調整が難しい場合があります。
*   **ハイパーパラメータの調整:** UoEモデルは、ルーティングパラメータ、エキスパート数、モデルサイズなど、多くのハイパーパラメータを持っており、それらの最適な組み合わせを見つけるのが難しい場合があります。
*   **汎化性能:** 特定のタスクやデータセットに最適化されたUoEモデルが、他のタスクやデータセットにどの程度汎化できるかは不明です。

## 5. 技術的な詳細について

UoEモデルは、Transformerアーキテクチャをベースに、エキスパートの概念とルーティングメカニズムを導入したものです。

1.  **エキスパート分解:** MLP層とAttention層を複数のエキスパートに分割します。これは、tensor parallelismにおける行列分割の考え方に基づいています。例えば、MLP層の重み行列 `W` を `N` 個のエキスパートに分割する場合、各エキスパート `i` は `W_i` という部分行列を持ちます。

    ```python
    # 疑似コード
    N = num_experts  # エキスパート数
    W = model.mlp.weight  # MLP層の重み行列
    W_i = split(W, N)  # WをN個の部分行列に分割
    ```

2.  **ルーティング:** 入力データ（パッチ単位）またはエキスパートを選択するルーティングメカニズムを導入します。パッチ単位のデータ選択では、各パッチに対して最も関連性の高いエキスパートを選択します。エキスパート選択では、特定の入力に対して最も適切なエキスパートを動的に選択します。

    ```python
    # パッチ単位のルーティング（疑似コード）
    for patch in input_patches:
        expert_index = routing_function(patch) # パッチに基づいてエキスパートを選択
        output = experts[expert_index](patch)  # 選択されたエキスパートで処理
        # experts はエキスパートのリスト
    ```

3.  **SMHA (Selective Multi-Head Attention):** 従来のMulti-Head Attentionを拡張し、Attentionヘッドを選択的に適用します。これにより、計算コストを削減しつつ、重要な情報を効率的に処理できます。

4.  **UoME (Union-of-MLP-Experts):** MLP層を複数のエキスパートの連合として構成します。各エキスパートは、入力データの一部を処理し、その結果を統合します。

## 6. コストや物理的な詳細について

Abstractおよび提供された情報からは、トレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど、具体的なコストや物理的な詳細に関する情報は得られません。論文の本文を参照するか、著者らに直接問い合わせる必要があります。ただし、一般的にMoEモデルは、モデルサイズが大きくなる傾向があり、トレーニングには多くの計算リソースと時間を必要とします。

## 7. 参考文献のうち、特に参照すべきもの

この論文から参照すべき参考文献を特定することはできません。本文が提供されていないためです。

## 8. この論文を140字以内のツイートで要約すると？

UoE: Transformerをエキスパートに分解し、選択的ルーティングで計算効率UP！言語モデルでperplexity大幅改善、Long Range Arenaで高スコア、画像分類でも精度向上。低コストで高性能なTransformer！ #MoE #Transformer #AI


---


# FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion

[View Paper](http://arxiv.org/abs/2503.04222v1)

## 1. 既存研究では何ができなかったのか

既存研究における大規模言語モデル(LLM)の融合手法は、以下の点で課題がありました。

*   **個々のモデルの弱点の克服:** 個々のLLMは得意分野と苦手分野があり、サイズや学習データに制約があると、特定の領域でのパフォーマンスが低い場合がありました。例えば、創造的なコンテンツ生成が得意なモデルでも、技術的な説明の正確さに欠ける、あるいはその逆のケースがありました。
*   **Ensemble法の計算コスト:** 複数のモデルの予測を組み合わせるEnsemble法は、パフォーマンスとロバスト性を向上させますが、推論時にすべてのモデルをアクティブにする必要があり、計算コストとメモリコストが大きくなります。
*   **Routing法の汎用性の欠如:** LLM Routingは、クエリに応じて最適なLLMを選択するため効率的ですが、タスクごとにルーターを訓練する必要があり、未知のタスクへの汎化能力が制限されます。
*   **モデルパラメータの直接結合の制約:** パラメータ空間でモデルを直接結合する方法は、ロバスト性と汎化能力を高めますが、同じアーキテクチャを持つモデルに限定されます。
*   **Explicit Model Fusion (EMF)の課題:** 知識蒸留を用いたEMFは、異なるモデル構造やサイズに対応できますが、語彙のアライメントや分布のマージといった課題があり、融合プロセスが複雑になり、エラーが発生する可能性があります。
*   **Preference Learningにおけるバイアスと分散:** 異なるモデルから生成された肯定的な例と否定的な例を組み合わせる場合、報酬アノテーションのバイアスと分散が導入される可能性があり、最適化を損なう可能性があります。ZephyrのようにGPT-4でランキングを作成する場合も同様です。

## 2. どのようなアプローチでそれを解決しようとしたか

FuseChat-3.0では、これらの課題を解決するために、以下のImplicit Model Fusion (IMF)に基づくアプローチを採用しました。

1.  **Implicit Model Fusion (IMF)の導入:** 複数の強力なLLMの出力を活用し、よりコンパクトなターゲットモデルをfine-tuningします。このIMFは、より強いLLMの出力に埋め込まれた豊富な情報を様々なメカニズムを通じて利用します。
2.  **データ構築における工夫:** 様々なソースモデルから複数のレスポンスを生成し、外部の報酬モデルまたはルールベースの手法で評価します。これにより、ターゲットモデルは多様なソースモデルの知識を学習できます。
3.  **Supervised Fine-Tuning (SFT)の実施:** 異質なLLMの出力に直接Preference Learningを適用する際の分布のずれに対処するために、SFTステージを設けます。SFTでは、各プロンプトに対するソースモデルからの最適なレスポンスを使用してターゲットモデルをfine-tuningします。
4.  **Direct Preference Optimization (DPO)の適用:** SFTで初期化されたモデルに対して、同一ソースのレスポンスペアから制御されたPreference信号を組み込むDPOステージを設けます。これにより、モデルのロバスト性を高め、異質なレスポンスデータに起因するバイアスと分散を軽減します。
5.  **同一ソースモデル内でのPreferenceペア構築:** 各Preferenceペアを同一のソースモデルによって生成された最良のレスポンスと最悪のレスポンスから構築します。これにより、異質なレスポンススタイルに起因する報酬バイアスを排除し、報酬ハッキングを防ぎ、より制御されたPreference信号を提供します。

Python風疑似コードで表現すると、以下のようになります。

```python
def implicit_model_fusion(source_models, target_model, dataset):
    # 1. データ生成
    generated_responses = {}
    for prompt in dataset:
        generated_responses[prompt] = {}
        for model in source_models:
            generated_responses[prompt][model] = generate_multiple_responses(model, prompt) # 複数レスポンス生成

    # 2. 評価
    evaluated_responses = {}
    for prompt in generated_responses:
        evaluated_responses[prompt] = {}
        for model in generated_responses[prompt]:
            evaluated_responses[prompt][model] = evaluate_responses(generated_responses[prompt][model]) # 評価

    # 3. SFT
    sft_dataset = create_sft_dataset(evaluated_responses) # SFTデータセット作成
    target_model = supervised_finetune(target_model, sft_dataset)  # SFT実施

    # 4. DPO
    dpo_dataset = create_dpo_dataset(evaluated_responses) # DPOデータセット作成
    target_model = direct_preference_optimization(target_model, dpo_dataset) # DPO実施

    return target_model

def create_dpo_dataset(evaluated_responses):
    dpo_dataset = []
    for prompt in evaluated_responses:
        for model in evaluated_responses[prompt]:
            best_response = get_best_response(evaluated_responses[prompt][model])
            worst_response = get_worst_response(evaluated_responses[prompt][model])
            if best_response and worst_response: # 良いレスポンスと悪いレスポンスがある場合
                dpo_dataset.append((prompt, best_response, worst_response)) # ペア作成
    return dpo_dataset
```

## 3. 結果、何が達成できたのか

FuseChat-3.0は、以下の成果を達成しました。

*   **パフォーマンスの向上:** FuseChat-3.0モデルは、Instruction Following、一般的な知識、数学、コーディングなどのタスクにおいて、大幅なパフォーマンス向上を示しました。
*   **平均改善:** Llama-3.1-8B-Instructをターゲットモデルとして使用した場合、14のベンチマークで平均6.8ポイントの改善を達成しました。
*   **Instruction Followingの改善:** AlpacaEval-2で37.1ポイント、Arena-Hardで30.1ポイントという顕著な改善を達成しました。
*   **最先端のパフォーマンス:** 8BパラメータLLMにおいて、新たな最先端のパフォーマンスを達成しました。
*   **データセットの公開:** コード、モデル、およびデータセットを公開しました(https://github.com/SLIT-AI/FuseChat-3.0)。

## 4. Limitationや問題点は何か

FuseChat-3.0のLimitationsと問題点：

*   **計算コスト:** データ生成時に複数のソースモデルを使用するため、計算コストが高くなる可能性があります。特に、ソースモデルのサイズが大きい場合、計算資源の消費が大きくなります。
*   **データ依存性:** データセットの質と多様性に大きく依存します。データセットの偏りやノイズが、モデルの性能に影響を与える可能性があります。特に中国語能力の向上においてDPOフェーズを省略している点は、データセットの不足に起因しており、今後の課題となります。
*   **Reward Modelの依存性:** Instruction Followingなどのタスクにおいて、報酬モデル(RM)の性能がデータ構築に影響します。不正確な報酬モデルを使用すると、モデルの学習が阻害される可能性があります。
*   **ハイパーパラメータ調整:** SFTおよびDPOステージにおけるハイパーパラメータの調整は、モデルの性能に大きな影響を与えます。最適なハイパーパラメータの設定には、時間と計算資源が必要です。
*   **汎用性:** 特定のタスクやドメインに特化したデータセットで学習した場合、他のタスクやドメインへの汎用性が低い可能性があります。
*   **潜在的なバイアス:** ソースモデルが持つバイアスが、ターゲットモデルに伝播する可能性があります。特に、倫理的な問題や社会的な偏見を含むデータで学習した場合、注意が必要です。
*   **評価の限界:** 評価に使用したベンチマークは、LLMの能力の一部しか評価できません。より包括的な評価を行うためには、多様な評価指標が必要です。
*   **ブラックボックス性:** モデル融合により、モデルの挙動がより複雑になり、解釈が難しくなる可能性があります。

## 5. 技術的な詳細について

FuseChat-3.0の技術的な詳細：

*   **Implicit Model Fusion (IMF):** 異質なソースLLMの出力を活用して、ターゲットLLMを学習させる手法です。ソースモデルの知識を、ターゲットモデルに暗黙的に転移させます。
*   **データ構築:**
    *   Instruction Following: UltraFeedbackなどを使用し、コードと数学関連のデータを除外
    *   数学: OpenMathInstruct-2を使用し、正解が検証されたものを使用
    *   コーディング: LeetCodeを使用し、包括的なテストケースがある問題を選択
    *   中国語: Alpaca-GPT4-Zhを使用し、コードと数学関連のデータを除外
*   **レスポンスサンプリング:**
    *   ソースモデル: Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instructなどを使用
    *   サンプリング戦略: Instruction Followingと数学では各ソースモデルから5つのレスポンス、コーディングでは8つのレスポンスを生成
    *   パラメーター: Gemma-2-27B-it, Mistral-Large-Instruct-2407, Llama-3.1-70B-Instructではtemperature=0.8, top-p=0.95、Qwen-2.5-(Math)-72B-Instructではtemperature=0.7, top-p=0.8, repetition_penalty=1.05
*   **Supervised Fine-Tuning (SFT):** ターゲットモデルの分布をソースモデルの分布に合わせるために使用されます。最適なレスポンスを使用してfine-tuningを行います。Loss関数はnegative log-likelihoodです。
    ```python
    def sft_loss(model, x, y):
      y_pred = model(x)
      loss = -torch.sum(torch.log(y_pred) * y)
      return loss
    ```
*   **Direct Preference Optimization (DPO):** 人間の好みに基づいてモデルを最適化します。好ましいレスポンスと好ましくないレスポンスのペアを使用して、モデルをfine-tuningします。目的関数は、Bradley-Terryモデルに基づいています。length normalizationを行う SimPO のバリアントも試しています。
    ```python
    def dpo_loss(model, x, y_w, y_l, beta):
      pi_theta_yw = model(x, y_w) # 好ましいレスポンスの確率
      pi_theta_yl = model(x, y_l) # 好ましくないレスポンスの確率
      pi_ref_yw = ref_model(x, y_w) # 参照モデルの好ましいレスポンスの確率
      pi_ref_yl = ref_model(x, y_l) # 参照モデルの好ましくないレスポンスの確率

      log_ratio_yw = torch.log(pi_theta_yw / pi_ref_yw)
      log_ratio_yl = torch.log(pi_theta_yl / pi_ref_yl)

      loss = -torch.log(torch.sigmoid(beta * (log_ratio_yw - log_ratio_yl)))
      return loss

    def ln_dpo_loss(model, x, y_w, y_l, beta): # length normalization DPO
        pi_theta_yw = model(x, y_w)
        pi_theta_yl = model(x, y_l)
        pi_ref_yw = ref_model(x, y_w)
        pi_ref_yl = ref_model(x, y_l)

        len_yw = len(y_w)
        len_yl = len(y_l)

        log_ratio_yw = torch.log(pi_theta_yw / pi_ref_yw)
        log_ratio_yl = torch.log(pi_theta_yl / pi_ref_yl)

        loss = -torch.log(torch.sigmoid(beta/len_yw * log_ratio_yw - beta/len_yl * log_ratio_yl))
        return loss
    ```
*   **実装:** Llama-Factoryライブラリとalignment-handbookを使用。

## 6. コストや物理的な詳細について

論文中に明示的な記載はありませんが、以下の点が推測できます。

*   **GPU:** 複数の大規模言語モデル(27B～123B)の推論とfine-tuningを行うため、複数の高性能GPU(A100, H100など)を使用していると考えられます。
*   **時間:** データセットの作成、SFT、DPOには、それぞれ数時間から数日程度の時間が必要であると考えられます。特に、大規模なデータセットを使用した場合や、モデルのサイズが大きい場合は、学習時間が長くなります。
*   **データセット:**
    *   SFTフェーズ: 94,539エントリー
    *   DPOフェーズ: 64,128ペア
    *   合計：158,667エントリー
*   **モデルサイズ:**
    *   ソースモデル: 27B～123Bパラメータ
    *   ターゲットモデル: 1B～9Bパラメータ

具体的なGPUの数や詳細な学習時間については、論文には記載されていません。githubで公開されている場合は、README等に記載があるかもしれません。

## 7. 参考文献のうち、特に参照すべきもの

FuseChat-3.0を理解する上で、以下の参考文献は特に重要です。

*   **Direct Preference Optimization (DPO):** DPOは、FuseChat-3.0のPreference Learningの基礎となる技術です。DPOの論文を読むことで、DPOの理論的な背景や利点を理解することができます。
*   **Zephyr:** GPT-4を用いてLLMの応答をランキングし、Preferenceデータセットを作成する手法です。FuseChat-3.0では、異なるモデルからの応答ではなく、同一モデルからの応答ペアを使用している点が異なります。
*   **WRPO:** Implicit Model Fusion (IMF)の概念を導入した論文です。FuseChat-3.0では、WRPOのIMFをさらに発展させ、Preference Learningにおけるバイアスと分散を軽減する手法を提案しています。
*   **Llama-Factoryとalignment-handbook:** SFTとDPOの実装に使用されているライブラリです。これらのライブラリのドキュメントを読むことで、FuseChat-3.0の実装の詳細を理解することができます。

## 8. この論文を140字以内のツイートで要約すると？

FuseChat-3.0：異種LLMの強みを融合！SFTとDPOで小型モデルを強化。同一モデル応答ペアでバイアス軽減。Llama-3.1-8Bで14ベンチ平均6.8pt↑、AlpacaEval-2で37.1pt↑！コードは[https://github.com/SLIT-AI/FuseChat-3.0](https://github.com/SLIT-AI/FuseChat-3.0) #LLM #AI


---


# Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities

[View Paper](http://arxiv.org/abs/2503.03983v1)

## 1. 既存研究では何ができなかったのか

既存のAudio-Language Models (ALMs) は、以下の点で課題がありました。

*   **専門家レベルの推論能力の不足:** 既存のLALM（Large ALMs）は、イベント分類のような基礎的なタスクと比較して、専門家レベルの推論タスクで性能が低い。例えば、Gemini-1.5-ProはMMAUのサウンドおよび音楽サブセットで54.4%と48.5%の精度しか達成できていない。
*   **高品質な学習データの不足:** LLMのオーディオ理解・推論能力向上の課題は、高品質な学習データとロバストなオーディオ表現の不足に起因する。
*   **長尺オーディオ理解の欠如:** 既存のALMは、最大でも30秒程度のオーディオしか扱えず、より長いオーディオの理解と推論が未開拓だった。既存モデルの多くは最大10秒のオーディオに制限されている。これは、モデルが短いオーディオエンコーディングしかサポートしていないことと、データの大部分が最大10秒しかないことに起因する。
*   **データ品質の向上よりもモデルのスケーリングに重点:** 多くの研究がモデルサイズとデータセットのスケーリングに重点を置いており、データ品質やオーディオエンコーダの表現に関する進歩は少ない。

## 2. どのようなアプローチでそれを解決しようとしたか

Audio Flamingo 2 (AF2) は、以下の要素を取り入れることで上記の課題を解決しようとしました。

1.  **カスタムCLAPモデル (AF-CLAP) の開発:**

    *   大規模で高品質なオーディオ-キャプションペアのデータセットを構築。8Mの10秒オーディオ-キャプションペアにスケールし、多様性と精度を重視した。具体的には、オープンソースのオーディオおよびビデオデータセットからデータを収集し、長尺ビデオデータセット(MiraData)から10秒のクリップを抽出し、Qwen2-VL-2B-InstructとQwen2-Audioを用いてキャプションを生成。さらに、GPT-4oを用いてオーディオ中心のキャプションを生成した。
    *   言語的な多様性と構成的な推論能力に対するロバスト性を向上させるために、改良された対照学習損失を提案。データセット内のすべてのキャプションに対して、意味と構成が同一の言語的に多様なキャプションを複数生成し、それらを正例として扱う。また、時間的または属性的な構成が変更されたバリエーションを負例として導入。
2.  **合成Audio QAデータ (AudioSkills) の活用:**

    *   複雑で推論を必要とする質問とオーディオを組み合わせた大規模なスキル固有のAQAトレーニングデータセットを設計。時間的関係の理解、属性認識、イベントのカウント、目的の識別など、さまざまなスキルを対象とする質問を作成。
3.  **マルチステージカリキュラム学習戦略:**

    *   3段階のカリキュラムトレーニング戦略を提案。
        *   **Stage 1 (Multi-Modal Alignment):** 大規模でノイズを含む可能性のある分類およびキャプションデータセットを活用して、オーディオ表現とLLMを連携させる。CLAPとLLMのレイヤーを凍結し、オーディオ表現変換とゲート付きクロスアテンションレイヤーのみをトレーニング可能にする。オーディオコンテキストは10秒に制限。
        *   **Stage 2 (Skill Learning):** 高品質の短尺オーディオ分類、キャプション、およびQAデータセットを使用して、オーディオの理解と推論を改善する。LLMレイヤーのみを凍結し、CLAPモデル、オーディオ表現変換レイヤー、およびゲート付きクロスアテンションレイヤーをトレーニング可能にする。オーディオコンテキスト長は30秒に増加。
        *   **Stage 3 (Long Audio Understanding):** 提案したLongAudioデータセットを利用して、コンテキスト長の拡張と長尺オーディオでの推論を可能にするスキルを教える。オーディオ表現変換レイヤーとゲート付きクロスアテンションレイヤーをトレーニング可能な状態にし、オーディオコンテキスト長を最大5分に拡張。
4.  **長尺オーディオ理解データセット (LongAudio) の導入:**

    *   30秒から5分のオーディオを含む、26万以上のAQAインスタンスからなる新しいデータセットLongAudioを導入。LongAudioは10以上のオーディオカテゴリにまたがり、キャプション、ストーリーQA、時間的QA、ニードルヘイスタックQA、サブシーンQA、オープンエンドQAを含む6つのタスクをサポート。
5.  **専門家による注釈付きベンチマーク (LongAudioBench) の提案:**

    *   長尺オーディオ理解におけるALMを評価するための、専門家による注釈付きベンチマークLongAudioBenchを提案。

## 3. 結果、何が達成できたのか

AF2は、以下の成果を達成しました。

*   **最先端性能:** 3Bパラメータの小さな言語モデルのみを使用し、20以上のベンチマークで大規模なオープンソースモデルやプロプライエタリモデルを上回る最先端の性能を達成。
*   **長尺オーディオ理解の実現:** 初めてオーディオ理解を長尺オーディオセグメント（30秒から5分）に拡張。LongAudioでのAF2のファインチューニングにより、提案されたLongAudioBenchで優れた性能を発揮。
*   **データ、アーキテクチャ、表現学習、トレーニング戦略における革新の提案:** データ生成、アーキテクチャ設計、表現学習、およびトレーニング戦略における革新を提案。
*   **優れた汎化性能:** AF2は、小規模であり、パブリックデータセットのみでトレーニングされているにもかかわらず、大規模なプロプライエタリなLALMを上回る性能を示す。

## 4. Limitationや問題点は何か

AF2のLimitationと問題点は以下の通りです。

*   **長尺オーディオの処理における計算コスト:** 長尺オーディオを処理する場合、計算コストが増加する可能性がある。スライディングウィンドウを使用しているため、オーディオが長くなるほど処理時間が増加する。
*   **LongAudioBenchのデータ規模:** LongAudioBenchは新規なベンチマークだが、他の確立されたベンチマークと比較してデータ規模がまだ小さい。より大規模なデータセットでの評価が必要となる可能性がある。
*   **特定のスキルへの依存:** AudioSkillsデータセットは、特定のスキルに焦点を当てているため、他の種類の推論タスクに対する汎化性能が制限される可能性がある。
*   **誤りの伝播:** 長尺オーディオに対する評価として、短いセグメントのキャプションを生成し、そのキャプションに基づいてQAを行うというカスケードアプローチをとっている。このアプローチは、キャプション生成段階での誤りがQAの精度に影響を与える可能性がある。
*   **汎化性能に関する懸念:** プロプライエタリなデータセットを使用しているGeminiのようなモデルと比較して、AF2はパブリックデータセットのみで訓練されているため、未知のオーディオに対する汎化性能が劣る可能性がある。

## 5. 技術的な詳細について

AF2は、以下の主要コンポーネントから構成されます。

1.  **AF-CLAP (Audio Feature Extractor):**

    *   CLAPをベースにしたオーディオエンコーダで、スライディングウィンドウによる特徴抽出を行う。
    *   大規模なオーディオ-キャプションペアで学習。
    *   損失関数は、言語バリエーションと構成的推論に対するロバスト性を高めるように修正。
        *   オリジナルキャプションに加え、意味を変えずに表現を変えたキャプションを正例として追加。
        *   時間的な順序や属性の構成を変えたキャプションを負例として追加。
    *   疑似コード:

        ```python
        def contrastive_loss(audio_embeddings, text_embeddings, temperature):
            # audio_embeddings: (batch_size, feature_dim)
            # text_embeddings: (batch_size, feature_dim)
            # temperature: a scalar value

            batch_size = audio_embeddings.shape[0]
            
            # Calculate cosine similarity matrix
            similarity_matrix = torch.matmul(audio_embeddings, text_embeddings.T) / temperature
            
            # Create ground truth labels (diagonal elements are positive pairs)
            labels = torch.arange(batch_size)
            
            # Calculate cross-entropy loss
            loss = F.cross_entropy(similarity_matrix, labels)
            
            return loss

        def modified_contrastive_loss(audio_embeddings, text_embeddings, linguistic_variants, compositional_negatives, temperature):
            # audio_embeddings: (batch_size, feature_dim)
            # text_embeddings: (batch_size, feature_dim) # Original Captions
            # linguistic_variants: (batch_size, num_variants, feature_dim) # Variants for each original caption
            # compositional_negatives: (batch_size, num_negatives, feature_dim) # Negatives for each original caption
            
            batch_size = audio_embeddings.shape[0]
            num_variants = linguistic_variants.shape[1]
            num_negatives = compositional_negatives.shape[1]

            # Calculate similarities with original captions
            original_similarities = torch.matmul(audio_embeddings, text_embeddings.T) / temperature
            
            # Calculate similarities with linguistic variants
            variant_similarities = torch.matmul(audio_embeddings.unsqueeze(1), linguistic_variants.transpose(1,2)).squeeze(1) / temperature # Shape: (batch_size, num_variants)
            
            # Calculate similarities with compositional negatives
            negative_similarities = torch.matmul(audio_embeddings.unsqueeze(1), compositional_negatives.transpose(1,2)).squeeze(1) / temperature # Shape: (batch_size, num_negatives)
            
            # Create a combined similarity matrix
            combined_similarities = torch.cat([original_similarities, variant_similarities, negative_similarities], dim=1)
            
            # Create ground truth labels.  The first element corresponds to original caption, others corresponds to variants
            labels = torch.zeros(batch_size, dtype=torch.long) # The ground truth is always the original captions
            
            # Calculate cross-entropy loss
            loss = F.cross_entropy(combined_similarities, labels)

            return loss
        ```
2.  **Audio Representation Transformation Layers:**

    *   AF-CLAPの出力特徴量 (`h ∈ R^(64 × 2048)`) に対して、3層のself-attention層を適用してモデルの表現能力を向上。各層は8つのヘッドと2048次元の内部次元を持つ。
3.  **Gated Cross-Attention Dense (XATTN-Dense) Layers:**

    *   Flamingoから導入されたXATTN-Denseレイヤーを使用して、オーディオ表現をLLMに条件付け。
    *   各レイヤーは、クロスアテンションとtanhゲーティングを持つ残差ブロックと、denseレイヤーとtanhゲーティングを持つ残差ブロックで構成される。
    *   XATTN-Denseレイヤーにより、prefix tuningにおける二次的な注意計算量が線形計算量に削減される。
    *   疑似コード:

        ```python
        class XAttnDense(nn.Module):
            def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.0):
                super().__init__()
                self.cross_attn = CrossAttention(dim, num_heads, dropout=dropout)
                self.mlp = Mlp(dim, mlp_ratio, dropout=dropout)
                self.gate = nn.Tanh()
            
            def forward(self, audio_features, text_features):
                # audio_features: (batch_size, seq_len_audio, dim)
                # text_features: (batch_size, seq_len_text, dim)

                # Cross-Attention Block
                attn_output = self.cross_attn(audio_features, text_features, text_features) # Query: audio_features, Key/Value: text_features
                gated_attn_output = self.gate(attn_output)
                audio_features = audio_features + gated_attn_output # Residual Connection

                # Dense Layer Block
                mlp_output = self.mlp(audio_features)
                gated_mlp_output = self.gate(mlp_output)
                audio_features = audio_features + gated_mlp_output # Residual Connection

                return audio_features
        ```
4.  **LLM (Language Model):**

    *   3Bパラメータのdecoder-only causal LLMを使用。36の隠れ層と16の注意ヘッドを持つ。
    *   AudioSkillsにおける専門的な推論を可能にするのに十分な能力を持ちながら、軽量であることが選択の理由。

## 6. コストや物理的な詳細について

*   **トレーニングに使用したGPU:** 128 NVIDIA A100 80GB GPUs
*   **バッチサイズ:**
    *   Pre-training: 有効バッチサイズ 1024
    *   Fine-tuning and Long fine-tuning: 可変バッチサイズ (オーディオ長に基づいて動的に調整。128から1024の範囲)
*   **オプティマイザ:** AdamW (学習率 = 1e-4, weight decay = 0.1)
*   **精度:** bf16 (automatic mixed precision)
*   **AudioSkills:** 長さ2-8秒のオーディオを使用
*   **LongAudio:** 80K unique audios, 263K AQA pairs.

## 7. 参考文献のうち、特に参照すべきもの

*   **Contrastive Language-Audio Pre-training (CLAP):** 初期のエンコーダのみのALMとして、オーディオと言語の橋渡しに貢献。AF2のAF-CLAPのベースとなっている。
*   **Flamingo:** ゲート付きクロスアテンション構造がAF2に採用されている。
*   **Qwen2-Audio:** キャプション生成に利用
*   **GPT-4o:** QAペア生成と評価に利用

## 8. この論文を140字以内のツイートで要約すると？

Audio Flamingo 2 (AF2) 発表！長尺オーディオ理解と高度な推論を実現する軽量ALM。カスタムCLAP、合成QAデータ、カリキュラム学習でSOTA達成。LongAudioBenchで実証された長尺音声処理能力がすごい！ #AudioAI #LLM #NVIDIA


---


# START: Self-taught Reasoner with Tools

[View Paper](http://arxiv.org/abs/2503.04625v1)

## 1. 既存研究では何ができなかったのか

大規模言語推論モデル（LRM、例：OpenAI-o1, DeepSeek-R1）は、長い Chain-of-thought (CoT) を用いて複雑な推論タスクで優れた能力を示していますが、以下の点で課題がありました。

*   **ハルシネーション (hallucinations):** LRM は内部の推論プロセスのみに依存するため、事実に基づかない情報を生成することがあります。
*   **非効率性:** 複雑なタスクにおいて、ツールを使用せずに内部推論のみに頼るため、計算コストが高く、時間がかかる場合があります。
*   **外部知識の活用不足:** 外部のツールや知識を利用することで、より正確で効率的な推論が可能になる場合がありますが、既存の LRM はその能力を十分に活用できていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本論文では、外部ツールを活用することで推論能力を大幅に向上させる新しいツール統合型の長 CoT 推論 LLM である START (Self-Taught Reasoner with Tools) を提案しています。START の主なアプローチは以下の通りです。

1.  **Hint-infer:** LRM の推論プロセス中に、人工的に設計されたヒント（例：「ここでPythonを使うのは良い考えかもしれません。」）を挿入することで、デモンストレーションデータなしで外部ツールを利用する能力を効果的に刺激します。Hint-infer は、シンプルかつ効果的な逐次的なテスト時のスケーリング手法としても機能します。
2.  **Hint Rejection Sampling Fine-Tuning (Hint-RFT):** Hint-infer と Rejection Sampling Fine-Tuning (RFT) を組み合わせ、LRM が Hint-infer を介して生成したツール呼び出しを含む推論軌跡をスコアリング、フィルタリング、修正し、その後 LRM をファインチューニングします。

この自己学習フレームワークを通じて、LRMがツールを使いこなせるように学習させ、上記の課題を克服しようとしました。具体的には、コード実行による複雑な計算、自己チェック、多様な手法の探索、自己デバッグを可能にしました。

## 3. 結果、何が達成できたのか

START を QwQ-32B モデルにファインチューニングすることで、以下のベンチマークで優れた結果を達成しました。

*   **GPQA (PhD-level science QA):** 63.6% の精度
*   **AMC23 (competition-level math benchmarks):** 95.0% の精度
*   **AIME24 (competition-level math benchmarks):** 66.7% の精度
*   **AIME25 (competition-level math benchmarks):** 47.1% の精度
*   **LiveCodeBench (competition-level code benchmark):** 47.3% の精度

START は、ベースモデルである QwQ-32B を大幅に上回り、最先端のオープンウェイトモデル R1-Distill-Qwen-32B およびプロプライエタリモデル o1-Preview と同等の性能を達成しました。

## 4. Limitationや問題点は何か

*   **ヒントの設計:** Hint-infer の効果は、ヒントの質に大きく依存します。最適なヒントを自動的に生成する方法はまだ確立されていません。汎用的なヒントがどれだけ効果的かは不明です。
*   **Hint-RFT の計算コスト:** 推論軌跡のスコアリング、フィルタリング、修正には、かなりの計算リソースが必要です。特に大規模なモデルやデータセットの場合、コストが課題となる可能性があります。
*   **汎化性能:** 特定のベンチマークで優れた結果を示していますが、START の汎化性能はまだ検証が必要です。未知のタスクやドメインへの適用可能性は不明です。
*   **ツール選択の偏り:** 特定のツールが過度に利用される傾向があるかもしれません。ツール選択の多様性を確保するためのメカニズムが必要です。
*   **安全性:** 外部ツールを利用する際に、セキュリティ上のリスクが生じる可能性があります。特に、悪意のあるコードが実行される可能性を考慮する必要があります。サンドボックス化などの対策が必要になります。
*   **テキストしか読めないことによる制約:** 今回の論文の本文がHTML, LaTeX, または変換に失敗していることが原因で内容が空であるため、論文の内容から推測できる範囲での回答となっています。

## 5. 技術的な詳細について

START の技術的な詳細を以下に示します。

1.  **Hint-infer:**

    *   LRM に対して、入力テキストに加えて、ツール利用を促すようなヒントを付加します。
    *   ヒントは、例えば "Calculate the square root using Python" のように、具体的なツール名と操作を指示する形式とします。
    *   ヒント挿入箇所は、Chain-of-thought の各ステップの前後、または特定のキーワードが出現する箇所など、戦略的に決定します。
    *   ヒントの有効性は、様々な種類のヒントを試行し、検証データでの性能を比較することで評価します。

    ```python
    def hint_infer(model, input_text, hint):
      """
      LRM にヒントを付加して推論を実行する。

      Args:
        model: LRM モデル。
        input_text: 入力テキスト。
        hint: ツール利用を促すヒント。

      Returns:
        推論結果。
      """
      # ヒントを付加した入力テキストを生成
      hinted_input = input_text + " " + hint

      # LRM で推論を実行
      output = model.generate(hinted_input)

      return output
    ```

2.  **Hint-RFT:**

    *   Hint-infer を用いて、複数の推論軌跡を生成します。
    *   各軌跡について、ツールの利用状況、推論の正確性、流暢さなどを考慮したスコアを計算します。
    *   スコアの高い軌跡を選択し、軌跡中のツール利用部分を正解データとして LRM をファインチューニングします。
    *   スコアの低い軌跡は、負例として利用することも可能です。
    *   Rejection Sampling の過程で、スコアが低い軌跡を破棄することで、質の高いデータのみでファインチューニングを行います。

    ```python
    def hint_rft(model, training_data, hint):
      """
      Hint-RFT を用いて LRM をファインチューニングする。

      Args:
        model: LRM モデル。
        training_data: 学習データ（入力テキストと正解）。
        hint: ツール利用を促すヒント。
      """
      for input_text, correct_answer in training_data:
        # Hint-infer を用いて複数の推論軌跡を生成
        trajectories = [hint_infer(model, input_text, hint) for _ in range(NUM_TRAJECTORIES)]

        # 各軌跡のスコアを計算
        scores = [calculate_score(trajectory, correct_answer) for trajectory in trajectories]

        # スコアに基づいて軌跡を選択
        selected_trajectory = select_trajectory(trajectories, scores)

        # 選択された軌跡でモデルをファインチューニング
        model.fine_tune(input_text, selected_trajectory)
    ```
## 6. コストや物理的な詳細について

本論文では、QwQ-32B モデルをベースとしています。
通常、32Bパラメータのモデルを学習/ファインチューニングするには、A100やH100などの高性能GPUを複数枚使用する必要があります。
具体的なGPUの数、時間、データセットのサイズについては記載がありません。
しかし、Hint-RFTはサンプリングとスコアリングを行うため、通常のファインチューニングよりも計算コストが高くなることが予想されます。

## 7. 参考文献のうち、特に参照すべきもの

今回の論文の本文がHTML, LaTeX, または変換に失敗していることが原因で内容が空であるため、参考文献については言及できません。

## 8. この論文を140字以内のツイートで要約すると？

LRMの推論能力をツール活用で大幅UP！🤖🔧 STARTは、ヒント挿入と軌跡フィルタリングでモデルを自己学習させ、科学QAや数学コンテストでSOTA級の性能を達成。ハルシネーション対策にも期待！#LLM #AI #ツール活用


---

# L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling

[View Paper](http://arxiv.org/abs/2503.04725v1)

## 1. 既存研究では何ができなかったのか

既存研究では、長文コンテキストにおける言語モデリングの効果的なスケーリングに関する理論的な理解が不足していました。具体的には以下の点が課題でした。

*   **長距離依存性の捕捉:** 長距離依存性を捉えるための統計的な尺度は存在しましたが、実践的な効率的なアーキテクチャ設計を導く理論が確立されていませんでした。
*   **二点間相互情報量の限界:** 従来の二点間相互情報量のスケーリングだけでは、自然言語におけるマルチトークン間の依存性を十分に捉えられないことが示唆されていました。物理システムの類似構造との関連付けもミスリーディングである可能性がありました。
*   **モデルのスケーリング指針:** 経験的な観測から、より大きなモデルの開発が進められてきましたが、コンテキスト長のスケーリングに関する理論的な枠組みが不足していました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、長文コンテキストの言語モデリングの理解における課題を解決するために、以下の貢献を行いました。

*   **二分割相互情報量のスケーリング則の確立:** 自然言語の長距離依存性を支配する、二分割相互情報量のスケーリング則を確立しました。これは、従来の二点間相互情報量とは異なり、独立してスケーリングすることが示されています。
*   **経験的検証:** 提案されたスケーリング則を、LLaMAモデルを含む様々な自然言語データセットで検証し、一貫したべき乗則の成長挙動を明らかにしました。
*   **L$^2$M条件の定式化:** 長文コンテキスト言語モデリング(L$^2$M)条件を定式化し、モデルの効果的な長文コンテキストモデリング能力と、過去の情報を保存するための潜在状態のサイズのスケーリングの関係を明らかにしました。そして、モデルの過去の情報を保存するための状態サイズは、効果的な長文コンテキストモデリングのために、二分割相互情報量のスケーリングよりも速くスケーリングする必要があることを証明しました。この事は、トランスフォーマーと状態空間モデルを用いた実験によって検証されました。
*   **モデルの状態サイズの定義:** モデルが過去の情報を格納するために利用する状態を「履歴状態」と定義しました。Transformerでは、キー・バリューペアが履歴状態に対応し、RNNや状態空間モデル(SSM)では、隠れ状態が履歴状態に対応すると説明しました。

## 3. 結果、何が達成できたのか

本研究により、以下の成果が達成されました。

*   **長距離依存性の理解:** 自然言語における長距離依存性を特徴づける、二分割相互情報量のスケーリング則が確立されました。
*   **モデル能力の指標:** L$^2$M条件により、長文コンテキストモデリングに対するモデルの基本的な能力を評価するための原則的な枠組みが提供されました。
*   **アーキテクチャ設計への指針:** 確立された理論的な基礎により、より効果的な長文コンテキストアプリケーションを可能にし、より高性能でスケーラブルなAIシステムの開発を導く、大規模言語モデルのアーキテクチャ開発を導くことが期待できます。
*   **実証的な検証:** 理論的な予測が、トランスフォーマーと状態空間モデルの両方で実験的に検証されました。

## 4. Limitationや問題点は何か

論文で言及されている制限事項:

*   **言語モデリングの他の側面への非対応:** 本研究の理論的枠組みは、長距離依存性を捉えるモデルの能力に特化しており、推論や世界知識などの言語モデリングの他の側面には対応していません。
*   **自然言語の複雑さの完全な反映の欠如:** Gaussian分布データセットでの実証的な検証は、本質的な相互情報量のスケーリング効果を捉えていますが、自然言語の複雑さを完全に反映しているわけではありません。
*   **英語テキストへの制限:** 分析は主に英語のテキストに限定されており、他の言語でのスケーリング則の検証は、異なる言語構造における潜在的な普遍的な特性に関する貴重な洞察を提供することが期待されます。
*   **自己回帰言語モデルへの焦点:** 理論は自己回帰言語モデルに焦点を当てており、離散拡散モデルやビジョンモデルなどの他のアーキテクチャへの拡張は、異なる領域における同様の情報スケーリングの挙動を明らかにすることが期待されます。
*   **オープンソースモデルへの依存:** 評価はLLaMAやDeepSeekなどのオープンソースモデルに依存しており、クローズドソースの最先端モデルを使用したさらなる検証は、研究結果の追加の検証を提供することが期待されます。

私が考える制限事項:

*   **BOSトークンの影響:** 実験において、BOS (Beginning Of Sentence) トークンの影響を完全に排除できていない可能性があります。特に短いシーケンス長の場合、BOS トークンの有無が相互情報量の推定に影響を与える可能性があります。
*   **計算コスト:** 二分割相互情報量の計算は計算コストが高く、大規模なデータセットやモデルでの検証が難しい場合があります。
*   **パラメータ調整:** 実験結果は、モデルのハイパーパラメータに依存する可能性があります。異なる設定で同様の結果が得られるかどうかを確認する必要があります。
*   **データセットのバイアス:** 使用したデータセットが、特定のドメインやスタイルに偏っている可能性があります。より多様なデータセットでの検証が必要です。
*   **評価指標:** KLダイバージェンスやNLLは、モデルの能力を完全に反映しているとは限りません。他の評価指標 (例えば、下流タスクでの性能) も考慮する必要があります。

## 5. 技術的な詳細について

本研究では、以下の技術的な詳細が重要となります。

*   **二分割相互情報量の推定:** 大規模言語モデル（LLM）を使用して自然言語の基礎となる分布を近似し、二分割相互情報量を推定します。これは、条件付き確率を効率的に計算できるLLMの自己回帰的性質を利用します。具体的な推定方法としては、以下の2つを利用しています。

    *   **ダイレクト推定:**  条件付きエントロピーと周辺エントロピーの差から相互情報量を直接推定します。ただし、LLMがBOSトークンを前提としているため、バイアス補正を行います。
    *   **vCLUB推定:**  サンプルをシャッフルして周辺分布を近似し、相互情報量の上界を推定します。

    ```python
    # ダイレクト推定の疑似コード
    def direct_mutual_information(X, Y, model):
        # X: 前半のトークン列, Y: 後半のトークン列, model: LLM
        log_q_Y_given_X = model.log_prob(Y, context=X) # P(Y|X)の対数尤度
        log_q_Y = model.log_prob(Y) # P(Y)の対数尤度
        mutual_information = mean(log_q_Y_given_X - log_q_Y)
        return mutual_information

    # vCLUB推定の疑似コード
    def vclub_mutual_information(X, Y, model):
        # X: 前半のトークン列, Y: 後半のトークン列, model: LLM
        log_q_Y_given_X = model.log_prob(Y, context=X) # P(Y|X)の対数尤度
        Y_shuffled = shuffle(Y) # Yをシャッフル
        log_q_Y_given_X_shuffled = model.log_prob(Y_shuffled, context=X) # P(Y'|X)の対数尤度
        mutual_information = mean(log_q_Y_given_X - log_q_Y_given_X_shuffled)
        return mutual_information
    ```

*   **L$^2$M条件:** モデルが長距離依存性を効果的に捉えるためには、モデルの履歴状態の次元が、二分割相互情報量のスケーリングよりも速く成長する必要があるという条件を定式化しました。数式で表すと以下のようになります。

    `dim(history_state) >= L^beta`

    ここで、`L` はシーケンス長、`beta` は二分割相互情報量のスケーリング指数です。

*   **モデルアーキテクチャの分析:** トランスフォーマー、SSM、RNNなどの異なるアーキテクチャについて、履歴状態のスケーリングを分析しました。トランスフォーマーは、キー・バリューペアを履歴として保持するため、モデルサイズを固定しても系列長に対して線形に履歴サイズが大きくなるため、L$^2$M条件を満たしやすいという結論に至りました。

*   **Sub-volume Gaussian分布:** 二分割相互情報量と二点間相互情報量の両方のスケーリングを模倣するように設計された多変量ガウス分布の族を利用しました。これにより、計算効率の良い方法で条件付き確率とKLダイバージェンスを計算できます。

*   **Bias Correction:** 二点間相互情報量の推定における系統的なバイアスを軽減するために、バイアス補正された推定量が使用されます。これには、データからの系統的なバイアスをフィッティングすることが含まれます。

## 6. コストや物理的な詳細について

*   **データセット:** 実験には、The Pileデータセット（著作権侵害のため一部削除）、Project Gutenbergデータセット、および多変量ガウス分布が使用されました。
*   **モデル:** LLaMA 3.1 405B、LLaMA 3.1 70B、DeepSeek V3 Base、GPT2、Mamba、Mamba2などのモデルが使用されました。
*   **GPU:** 主にH100 GPU (VRAMサイズは80GB〜94GB) が使用され、一部の実験ではA100 GPU (VRAM 80GB) が使用されました。
*   **その他:**
    *   DeepSeek V3 および LLaMA 3.1 405B モデルについては、FP8 バージョンを 8 基の H100 GPU (94GB VRAM) で実行しました。
    *   LLaMA 3.1 70B モデルについては、FP16 バージョンを 4 基の H100 GPU (94GB VRAM) で実行しました。
    *   ハイパーパラメータは、ピーク学習率0.00005、重み減衰0.01、2000ステップのウォームアップ、および合計500000ステップに設定されました。

## 7. 参考文献のうち、特に参照すべきもの

*   **Hilberg, W. (2015). The relaxed hilberg conjecture: A review and new experimental support.** 本研究の基礎となるヒルバーグ予想に関する包括的なレビューを提供しています。
*   **Gu, A., & Dao, T. (2024). Mamba: Linear-time sequence modeling with selective state spaces.** 状態空間モデルに関する主要な論文であり、長文系列モデリングにおける効率的なアーキテクチャの例を示しています。
*   **Vaswani, A. et al. (2017). Attention is all you need.** トランスフォーマーアーキテクチャを紹介した、自然言語処理における重要な論文です。
*   **Kraskov, A. et al. (2004). Estimating mutual information.** 相互情報量の推定に関する包括的な研究を提供しています。

## 8. この論文を140字以内のツイートで要約すると？

LLMの長文モデリング能力は相互情報量で決まる！二分割相互情報量のスケーリング則を発見し、モデルの履歴状態サイズが重要と証明。TransformerとSSMで検証済。#LLM #長文モデリング #相互情報量
'''

---


# Identifying Sensitive Weights via Post-quantization Integral

[View Paper](http://arxiv.org/abs/2503.01901v1)

## 1. 既存研究では何ができなかったのか

既存のLLMのpost-training quantization (PTQ) 研究は、重みの量子化における感度メトリックの精度が不十分であり、以下の点で課題がありました。

*   **感度メトリックの不正確さ:** 既存の勾配およびヘッセ行列ベースの感度メトリックは、量子化が損失関数に与える影響を過小評価していました。これは、Taylor展開の2次近似の収束半径が小さいためです。つまり、局所的な情報（勾配やヘッセ行列）だけでは、量子化による大きな変化を正確に予測できませんでした。
*   **量子化後の感度変化:** 量子化を行う前と後で、重みの重要度が変化する可能性があります。以前は重要だった重みが量子化後に重要でなくなることや、逆に量子化前には重要でなかった重みが重要になることがあります。既存手法は、量子化前の情報のみに基づいて感度を計算していたため、この変化に対応できませんでした。
*   **層間の依存関係の無視:** 各層を個別に量子化した場合の影響を単純に足し合わせても、全ての層を同時に量子化した場合の影響とは一致しませんでした。これは、層間の相互作用（例えば、ヘッセ行列の非対角成分）が無視されていたためです。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の問題を解決するために、以下の新しいアプローチを提案しました。

*   **Post-quantization Integral (PQI) の導入:** 量子化後の感度をより正確に推定するための新しい感度メトリックであるPQIを提案しました。PQIは、元の重みから量子化された重みへの経路に沿って損失関数の勾配を積分することで、量子化による重みの変化が損失関数に与える影響をより正確に捉えます。これにより、局所的な近似に頼る既存手法よりもロバストな推定が可能になります。

    ```python
    def calculate_PQI(w, w_quantized, loss_function, N=32):
      """
      Post-Quantization Integral (PQI) を計算する疑似コード

      Args:
        w: 元の重み (numpy array)
        w_quantized: 量子化された重み (numpy array)
        loss_function: 損失関数 (callable)
        N: 積分の分割数 (int)

      Returns:
        PQI値 (numpy array)
      """
      PQI_values = np.zeros_like(w)
      for i in range(N):
        t = i / N
        w_intermediate = (1 - t) * w + t * w_quantized
        gradient = compute_gradient(loss_function, w_intermediate) #勾配の計算は既存の手法を想定
        PQI_values += np.abs(gradient)
      PQI_values /= N
      return PQI_values
    ```

*   **ReQuantフレームワークの開発:** PQIを活用して量子化モデルの品質を向上させるためのReQuantフレームワークを開発しました。ReQuantは、以下の2つの主要なコンポーネントから構成されています。
    *   **自己適応型外れ値選択 (Self-adaptive Outlier Selection):** PQIに基づいて各層の外れ値の比率を動的に調整します。これにより、重要度の高い層にはより多くのビットを割り当て、重要度の低い層にはより少ないビットを割り当てることで、量子化の精度を向上させます。
    ```python
    def adaptive_outlier_selection(weights, pqi_values, global_outlier_ratio, temperature):
      """
      層ごとに外れ値の割合を調整する疑似コード

      Args:
        weights: モデルの各層の重み
        pqi_values: 各層のPQI値
        global_outlier_ratio: 全体での外れ値の割合
        temperature: 温度パラメータ

      Returns:
        各層の外れ値の数
      """
      layer_num = len(weights)
      outlier_nums = []
      total_pqi = np.sum(pqi_values**temperature)
      for i in range(layer_num):
        outlier_num = len(weights[i]) * global_outlier_ratio * (pqi_values[i]**temperature) / total_pqi
        outlier_nums.append(int(outlier_num))
      return outlier_nums
    ```

    *   **段階的な重要重み分離 (Step-wise Significant Weights Detach):** 量子化後に重要となる重みを段階的に分離し、高精度で保持します。これにより、量子化による精度劣化を最小限に抑えます。
    ```python
    def stepwise_significant_weights_detach(weights, weights_quantized, pqi_values, significant_ratio, beta):
      """
      段階的な重要重み分離の疑似コード

      Args:
        weights: 元の重み
        weights_quantized: 量子化された重み
        pqi_values: PQI値
        significant_ratio: 分離する重要重みの割合
        beta: 1回のパスで分離する割合

      Returns:
        分離された重み
      """
      significant_weights = np.zeros_like(weights)
      num_steps = int(significant_ratio / beta)
      for _ in range(num_steps):
        importance = pqi_values * np.abs(weights_quantized - weights)
        # 重要度の高い重みを特定する処理（例：argsortで上位β%を選択）
        # 特定された重みをsignificant_weightsに加算
        # weights_quantizedを更新（分離された重みの影響を反映）
      return significant_weights
    ```

## 3. 結果、何が達成できたのか

ReQuantを適用することで、以下の成果が得られました。

*   **量子化精度の向上:** Llama 3.2 1BモデルにReQuantを適用した結果、QTIPと組み合わせることでperplexityが2.66改善されました。
*   **Few-shot性能の向上:** MATH few-shotタスクにおいて、約3%の性能向上が見られました。
*   **既存の量子化手法との組み合わせ:** ReQuantは、AWQ、SqueezeLLM、QTIPなどの既存のPTQ手法と組み合わせることができ、それらの性能を向上させることが示されました。

## 4. Limitationや問題点は何か

*   **PQI計算の必要性:** PQIは、量子化後の重みが必要となるため、量子化前に最適な量子化方法を決定するために直接使用することはできません。論文では、まず従来の感度メトリックで量子化モデルのドラフト版を作成し、その後PQIで量子化を改善するという解決策を取っています。
*   **疎行列の格納と計算コスト:** Dense-and-Sparse分解では、疎行列（外れ値と重要な重み）を格納する必要があり、インデックス情報（行、列）も格納する必要があるため、メモリ効率が低下する可能性があります。また、疎行列の乗算は、密行列の乗算よりも計算コストが高く、推論速度が低下する可能性があります。論文では、この点を改善するために、最小限の疎性（0.5%）のみを採用しています。
*   **汎用性:** ReQuantの効果は、モデルのサイズやアーキテクチャ、タスクの種類によって異なる可能性があります。論文ではLlama 3.2の小規模モデルで検証されていますが、大規模モデルや他の種類のタスクでの効果は検証されていません。
*   **最適なハイパーパラメータの探索:** 自己適応型外れ値選択における温度パラメータや、段階的な重要重み分離における分離割合など、ReQuantにはいくつかのハイパーパラメータが存在します。これらの最適な値を決定するためには、追加の実験が必要となる可能性があります。
*   **積分計算のコスト:** PQIの計算には、重み空間上の積分が必要であり、計算コストがかかります。論文では、rectangle approximationを用いて数値積分を行っていますが、より効率的な積分方法が存在する可能性があります。
*   **推論速度への影響:** ReQuantは、量子化されたモデルの精度を向上させることができますが、外れ値と重要な重みを保持するために、疎行列演算が必要になるため、推論速度が低下する可能性があります。

## 5. 技術的な詳細について

*   **PQIの計算:** PQIは、元の重みから量子化された重みへの直線経路に沿って、損失関数の勾配を数値積分することで計算されます。積分には、rectangle approximationが用いられます。
*   **自己適応型外れ値選択:** 各層の外れ値の比率は、PQIに基づいて動的に調整されます。具体的には、温度パラメータを用いたソフトマックス関数によって、各層に割り当てる外れ値の数を決定します。
*   **段階的な重要重み分離:** 量子化後のモデルにおいて、PQIと量子化誤差の積に基づいて、重要度の高い重みを段階的に分離し、高精度で保持します。これにより、量子化による精度劣化を最小限に抑えます。
*   **既存手法との組み合わせ:** ReQuantは、既存のPTQ手法（AWQ, SqueezeLLM, QTIPなど）と組み合わせることができます。具体的には、これらの手法で量子化されたモデルに対して、ReQuantを適用することで、精度を向上させることができます。

## 6. コストや物理的な詳細について

論文中には、以下の情報が記載されています。

*   **モデル:** Llama 3.2 1B および 3B モデル
*   **データセット:**
    *   AWQ: Pile データセットから 2048 の長さの 100 シーケンスをサンプリング
    *   SqueezeLLM: WikiText-2 から 2048 シーケンス長で 100 文をサンプリング
    *   QTIP: RedPajama （元のキャリブレーション用）、Tulu 3 （教師ありファインチューニングモデルのPQI計算用）、WikiText-2 (ベースモデル用)
*   **GPU:** RTX 3090 GPUs (推論速度テストで使用)
*   **計算時間:** 積分計算に必要な GPU 時間は表に示されている(論文中に表への言及はあるものの、具体的な数値は記載されていません。)

論文にはトレーニングに関する詳細な記述はありません。

## 7. 参考文献のうち、特に参照すべきもの

*   **AWQ (Activation-Aware Weight Quantization):** 量子化においてアクティベーションを考慮した重み量子化手法であり、ReQuantと比較対象として使用されています。
*   **SqueezeLLM (Dense-and-Sparse Quantization):** Dense-and-Sparse分解の形式を採用しており、ReQuantのベースラインとして使用されています。
*   **QTIP (Quantization with Trellises and Incoherence Processing):** コードブックベースの量子化手法であり、ReQuantと組み合わせることでperplexityが改善されています。

## 8. この論文を140字以内のツイートで要約すると？

LLM量子化の精度不足を、新指標PQIとReQuantで改善！勾配積分で重み感度を正確に推定し、外れ値選択＆重要重み分離で精度向上。Llama 3.2でperplexity大幅改善！#LLM #量子化 #PQI #ReQuant


---


# Lost in Literalism: How Supervised Training Shapes Translationese in LLMs

[View Paper](http://arxiv.org/abs/2503.04369v1)

## 1. 既存研究では何ができなかったのか

既存研究では、大規模言語モデル（LLM）を用いた機械翻訳における翻訳調（translationese）の問題、つまり過度に直訳的で不自然な翻訳がLLMでも依然として発生するという問題に対して、以下のような点が十分に扱えていませんでした。

*   **LLMにおける翻訳調の体系的な評価と原因究明:** LLMは大量の自然なテキストで事前学習されているにもかかわらず、なぜ翻訳調のエラーを示すのか、その根本的な原因が明確に特定されていませんでした。特に、教師ありファインチューニング（SFT）がLLMの翻訳スタイルに与える影響についての詳細な分析が不足していました。
*   **LLM固有の翻訳調軽減策:** 従来のNMTシステムにおける翻訳調対策は研究されてきましたが、LLMの事前学習という特性を考慮した、より効果的な軽減策が提案されていませんでした。
*   **トレーニングデータにおける翻訳調の認識:** SFTに使用されるトレーニングデータ自体に翻訳調が含まれている可能性が指摘されていましたが、その実態を定量的に評価し、それがLLMの翻訳に与える影響を分析した研究は限られていました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の問題点を解決するために、以下の３つの段階を経てアプローチしました。

1.  **LLMにおける翻訳調の評価:**
    *   ニュース記事、科学論文、Wikipedia、SNSのコメントといった多様なテキストに対してLLMによる翻訳を行い、専門家によるアノテーションを実施しました。
    *   翻訳調の度合いを定量的に評価するため、Translationese Span Ratio (TSR) という指標を導入しました。これは、翻訳文の中で翻訳調のエラーを含むスパンの割合を示します。
    *   LLM自身に翻訳文のperplexityを予測させ、perplexityとTSRの相関を分析することで、LLMが翻訳調のテキストを認識できる潜在能力があるかを検証しました。
2.  **教師あり学習データにおける翻訳調の分析:**
    *   一般的にSFTに使用されるデータセットからサンプルを抽出し、専門家によるアノテーションを行い、翻訳調の頻度を測定しました。
    *   これにより、SFTデータに含まれる翻訳調がLLMの翻訳に与えるバイアスを定量的に評価しました。
3.  **翻訳調軽減策の提案と評価:**
    *   翻訳調を軽減するための2つのアプローチを提案しました。
        1.  **ゴールデンリファレンスの改善:** LLMの潜在能力を活用し、ゴールデンリファレンスに含まれる翻訳調をLLM自身に修正させました。具体的には、「polishing」というプロンプトを用いて、LLMに既存の翻訳をより自然な表現に修正させました。
        2.  **トレーニングデータのフィルタリング:** LLMを用いてトレーニングデータから不自然な翻訳をフィルタリングしました。具体的には、perplexityを用いて翻訳の自然さを評価し、perplexityが高い（不自然な）インスタンスをトレーニングデータから除外しました。
    *   これらのアプローチをLlama-3.1-8BやQwen-2.5-7BなどのLLMに適用し、翻訳の自然さを自動評価指標（perplexity、語彙密度など）と人間による評価によって検証しました。

## 3. 結果、何が達成できたのか

本研究の結果、以下の点が達成されました。

*   **LLMにおける翻訳調の存在と深刻さを定量的に示す:** 複数のLLM（GPT-4を含む）が、英語-中国語およびドイツ語-英語の翻訳において、有意な翻訳調のエラーを示すことを明らかにしました。特に、GPT-4のような高度なモデルでも、翻訳の40%以上で翻訳調が見られることが示されました。
*   **SFTデータにおける翻訳調の存在を確認:** SFTに使用されるトレーニングデータの34%以上が翻訳調のエラーを示すことを発見しました。これにより、SFTがLLMの翻訳にバイアスをもたらす主要な原因であることを裏付けました。
*   **翻訳調軽減策の有効性を示す:** ゴールデンリファレンスの改善（SFT-Polished）とトレーニングデータのフィルタリングが、翻訳の自然さを大幅に向上させることを実証しました。特に、SFT-Polishedは、自動評価指標と人間による評価の両方で一貫して高い性能を示しました。
*   **翻訳自然さの自動評価指標の可能性:** LLMによるperplexity予測が、人間による評価と正の相関を示すことを示しました。これにより、perplexityが翻訳調の自動検出に利用できる可能性が示唆されました。

## 4. Limitationや問題点は何か

論文で言及されているLimitations:

*   **言語ペアの限定:** 人手によるアノテーションのコストのため、評価は主に英語-中国語とドイツ語-英語に限定されており、他の言語ペア、特に低リソース言語や形態素が豊富な言語への一般化可能性が不明です。
*   **モデルの網羅性の欠如:** 評価対象のLLMは一部のモデルに限定されており、他のモデルやアーキテクチャについてはさらなる調査が必要です。
*   **SFT以外の要因の無視:** 翻訳調の原因はSFTだけではなく、事前学習や強化学習などの他の学習段階も影響する可能性がありますが、本研究ではSFTに焦点を当てています。
*   **評価の主観性と指標の限界:** 人手による評価には主観的なバイアスが、自動評価指標には自然さを完全に捉えられないという限界があります。

追加で考えられるLimitations:

*   **ドメインの限定:** 評価に使用したテキストのドメイン（ニュース、科学論文など）が限られており、他のドメイン（文学作品、法律文書など）への一般化可能性が不明です。
*   **翻訳調の定義の曖昧さ:** 翻訳調という概念自体が主観的であり、明確な定義が難しい場合があります。TSRという指標を導入しているものの、アノテーションのばらつきを完全に排除することは困難です。
*   **「Polishing」プロンプトの最適化:** ゴールデンリファレンスの改善に使用した「polishing」プロンプトが最適である保証はありません。より効果的なプロンプトが存在する可能性もあります。
*   **計算資源:** SFTの実験に使用したGPUの種類や台数、学習時間などの詳細な情報が不足しています。
*   **翻訳の品質評価:** COMET-QEのような参照訳なしの自動評価指標を使用しているが、翻訳の正確性や内容の一貫性を評価する指標は考慮されていません。

## 5. 技術的な詳細について

*   **モデル:**
    *   ベースモデルとして、Llama-3.1-8BおよびQwen-2.5-7Bを使用。
    *   比較対象として、ALMA-7B-R, ALMA-13B-R, Mistral-7B, GPT-3.5-Turbo, GPT-4-Turboなどの商用LLMおよびオープンソースLLMを使用。
*   **データ:**
    *   教師あり学習データとして、ALMAトレーニングセット（WMT'17 to WMT'21 and Flores-200）から抽出した31,621の並行データを使用。
    *   開発セットとして、トレーニングデータの10%をランダムに選択。
    *   評価データとして、ニュース記事、科学論文、Wikipediaエントリ、ソーシャルメディアコメントなど、多様なテキストを使用。
*   **トレーニング:**
    *   トレーニング設定はALMAに準拠。
    *   バッチサイズは16を使用。
    *   最適化アルゴリズム、学習率スケジューラ、正則化手法などの詳細なハイパーパラメータは、Appendixを参照。
    *   学習は3エポック実施し、検証損失が最小となるモデルを選択。
*   **翻訳調の評価:**
    *   専門家によるアノテーションに基づき、TSR（Translationese Span Ratio）を算出。
    *   TSRは、翻訳文中の翻訳調エラーを含むスパンの割合を示す。
    *   翻訳のperplexityをLlama-3.1-8Bで計算し、TSRとの相関を分析。

Python風の疑似コードで表現すると、以下のようになります。

```python
# TSR (Translationese Span Ratio) の計算
def calculate_tsr(translation_text, annotated_spans):
  """
  翻訳文とアノテーションされた翻訳調スパンからTSRを計算する関数
  Args:
    translation_text: 翻訳文 (string)
    annotated_spans: 翻訳調としてアノテーションされたスパンのリスト (list of (start_index, end_index))
  Returns:
    TSR (float): 翻訳調スパンの割合
  """
  total_length = len(translation_text)
  translationese_length = 0

  for start, end in annotated_spans:
    translationese_length += (end - start)

  tsr = translationese_length / total_length
  return tsr

# データフィルタリング
def filter_training_data(training_data, perplexity_threshold):
    """
    perplexityに基づいてトレーニングデータをフィルタリングする関数
    Args:
        training_data: (source_text, target_text) ペアのリスト
        perplexity_threshold: perplexityの閾値
    Returns:
        フィルタリングされたトレーニングデータ
    """
    filtered_data = []
    for source_text, target_text in training_data:
        perplexity = calculate_perplexity(target_text) # target_textのperplexityを計算する関数は別途定義
        if perplexity <= perplexity_threshold:
            filtered_data.append((source_text, target_text))
    return filtered_data

# polishing
def polish_translation(source_text, target_text, polishing_prompt):
    """
    LLMを使用して翻訳を改善する関数
    Args:
        source_text: 原文
        target_text: 翻訳文
        polishing_prompt: LLMに与えるプロンプト
    Returns:
        改善された翻訳文
    """
    # 例: プロンプトとLLMを使用してより自然な翻訳文を生成
    prompt = f"{polishing_prompt}\n原文: {source_text}\n翻訳文: {target_text}"
    polished_text = llm_generate(prompt) # llm_generate 関数はLLMを呼び出してテキストを生成する関数

    return polished_text
```

## 6. コストや物理的な詳細について

論文中に記載されている物理的な詳細は限られています。以下の情報は確認できます。

*   SFTの実験にはA100 GPUを使用。
*   バッチサイズは16。

しかし、それ以外の詳細な情報は不足しています。例えば、トレーニングに使用したA100 GPUの正確な数、各モデルのトレーニング時間、メモリ使用量、消費電力などです。これらの情報は、実験の再現性やコスト効率の評価に重要となります。

データセットに関しては、ALMAトレーニングセットを使用しており、そのサイズは31,621の並行データです。また、評価には独自のデータセットを構築しており、専門家によるアノテーションを行っています。

モデルサイズについては、Llama-3.1-8BとQwen-2.5-7Bを使用しており、それぞれ80億と70億のパラメータを持つことがわかります。

## 7. 参考文献のうち、特に参照すべきもの

*   **Vaswani et al., 2017:** Transformerアーキテクチャを提案した論文。LLMの基盤となる重要な技術です。
*   **Hu et al., 2021:** LoRA（Low-Rank Adaptation）を提案した論文。LLMの効率的なファインチューニング手法として広く利用されています。
*   **Jiang et al., 2023:** Llamaの論文。本研究で使用されているLlama-3.1-8Bのベースとなるモデルです。
*   **Kocmi et al., 2023:** 機械翻訳の品質評価に関する論文。LLMが翻訳品質の評価に利用できる可能性を示唆しています。
*   **Riley et al., 2020:** 翻訳調を言語として捉え、NMTにおける翻訳調の問題を分析した論文。本研究のモチベーションに繋がっています。

これらの参考文献は、LLM、機械翻訳、翻訳調に関する基礎知識を提供し、本研究の背景を理解する上で役立ちます。

## 8. この論文を140字以内のツイートで要約すると？

LLM翻訳は自然？実は翻訳調が課題！教師あり学習データに潜むバイアスが原因と判明。良質なデータで再学習＆データ洗浄で自然さ大幅UP！#機械翻訳 #LLM #翻訳調


---


# EgoLife: Towards Egocentric Life Assistant

[View Paper](http://arxiv.org/abs/2503.03803v1)

## 1. 既存研究では何ができなかったのか

既存のegocentric（自己中心的な視点）なAI研究は、以下の点で限界がありました。

*   **記録期間の短さ:** 既存のデータセット（例: Epic-Kitchen）は、比較的短い期間の記録に限定されていました。これにより、習慣の包括的な把握や、複雑な社会的インタラクションの長期的なダイナミクスの理解が困難でした。

*   **単一視点:** 既存のデータセットは主に単一視点（monographic perspective）であり、マルチパーソン視点からの分析が不足していました。これにより、現実世界の複雑な社会的なインタラクションを捉えることが困難でした。

*   **超長期的な文脈理解の欠如:** 週単位のような超長期的な行動パターンや、複雑な社会的インタラクションのダイナミクスを理解する能力が不足していました。

*   **マルチモーダルデータ統合の不足:** 既存研究では、視覚情報と聴覚情報を効果的に統合したロバストなモデルの開発が十分ではありませんでした。特に、egocentricなデータに対する視覚-聴覚モデルの開発が課題でした。

*   **個人認識と追跡の課題:** 正確な個人認識と追跡の技術が十分ではなく、長期的な文脈における個人特定の精度向上が求められていました。

## 2. どのようなアプローチでそれを解決しようとしたか

EgoLifeプロジェクトでは、上記の課題を解決するために、以下のアプローチを取りました。

*   **大規模データ収集:** 6人の参加者が1週間共同生活し、AIメガネを使用して毎日約8時間のegocentricなマルチモーダルビデオを記録しました。これにより、300時間の包括的なegocentricデータセット（EgoLife Dataset）を作成しました。また、15台のカメラと2台のmmWaveデバイスを用いて、第三者視点からの同期データも収集しました。

*   **EgoLifeQAタスクの導入:** 長期間の文脈を必要とする、生活に密着した質問応答タスク（EgoLifeQA）スイートを導入しました。これらのタスクは、過去の関連イベントの想起、健康習慣のモニタリング、パーソナライズされた推奨事項の提供など、実用的な質問への回答を目的としています。

*   **EgoButlerシステムの開発:** EgoGPTとEgoRAGからなる統合システム（EgoButler）を開発しました。
    *   **EgoGPT:** egocentricなデータセットで学習されたオムニモーダルモデルで、egocentricビデオ理解において最先端の性能を達成しています。
    *   **EgoRAG:** 検索ベースのコンポーネントで、超長期的な文脈を持つ質問への回答をサポートします。階層的な検索戦略を採用し、質問に関連する時間窓を特定し、関連するビデオクリップのキャプションを検索して、言語モデルによる回答生成を支援します。

*   **データセット、モデル、ベンチマークの公開:** EgoLife Dataset、EgoGPTモデル、EgoLifeQAベンチマークを公開することで、egocentric AIアシスタントの研究を促進することを目指しました。

## 3. 結果、何が達成できたのか

EgoLifeプロジェクトにより、以下の成果が得られました。

*   **EgoLife Datasetの作成:** 300時間に及ぶ、大規模なegocentric、対人的、マルチビュー、マルチモーダルな日常データセットを作成し、集中的なアノテーションを施しました。

*   **EgoLifeQAベンチマークの導入:** 長期間の文脈を必要とする質問応答タスクのスイートを導入し、パーソナライズされたAIアシスタントの有効性を評価するための新しい基準を確立しました。

*   **EgoButlerシステムの開発と評価:** EgoGPTとEgoRAGからなるEgoButlerシステムを開発し、その動作メカニズムを検証しました。実験により、クリップレベルでのオムニモーダルな理解と長期間の文脈を考慮した質問応答の両方が可能であることが示されました。

*   **EgoGPTの高性能:** EgoGPTは、既存のegocentricベンチマークで最先端の性能を達成しました。特に、個人認識とマルチモーダル情報の統合において、汎用的な商用モデルを凌駕する能力を示しました。

*   **EgoRAGによる長期間文脈の質問応答の改善:** EgoRAGは、長期間の文脈を必要とする質問応答において、ビデオ言語モデルの性能を大幅に向上させました。特に、24時間以上前の情報が必要な質問において、EgoGPTとGemini-1.5-Proを上回る性能を達成しました。

## 4. Limitationや問題点は何か

EgoLifeプロジェクトには、以下のLimitationsと問題点が存在します。

*   **データセットの偏り:** データセットは、特定の環境（共同生活を送る6人の参加者）と活動（アースデイの準備）に限定されています。このため、多様な状況や文化への一般化が難しい可能性があります。特に、データセットの主要言語が中国語であることは、言語的な多様性の観点から課題です。

*   **EgoGPTの限界:**
    *   **音声理解の不完全さ:** EgoGPTの音声理解はまだ不完全であり、人間の笑いや感情の理解に苦労する場合があります。これは、ASR（自動音声認識）でトレーニングされたデータへの依存が原因と考えられます。
    *   **個人認識の課題:** EgoGPTは、EgoLifeの最初の日のデータでファインチューニングされているため、初期の観察に過剰に適合する傾向があります。たとえば、初日に青いシャツを着ていた人を、後日別の人が青いシャツを着ていても同じ人物と誤認識する可能性があります。

*   **EgoRAGの限界:**
    *   **マルチステップ推論の欠如:** EgoRAGの検索メカニズムは、マルチステップ推論を欠いています。これは、反復的な改良や段階的な推論を行わずに単一パスで検索を行うため、直接検索から関連情報が見つからない場合に失敗しやすいことを意味します。
    *   **エラー耐性の欠如:** EgoRAGは、サポートする証拠を見つけられない場合、欠落した情報を推論するのではなく、単に回答を提供できません。

*   **倫理的な懸念:** ウェアラブルデバイスによる継続的な記録は、プライバシーに関する倫理的な懸念を引き起こす可能性があります。データの収集、保存、使用に関する透明性と適切な同意が不可欠です。

*   **計算コスト:** EgoGPTとEgoRAGのトレーニングと実行には、相当な計算リソースが必要です。特に、長期間のビデオデータを処理するため、メモリと計算能力の制約が課題となります。

*   **注釈のコスト:** 大規模なデータセットに対する高品質なアノテーションは、時間とリソースを消費します。特に、EgoLifeQAのような複雑な質問応答タスクでは、専門的な知識を持つアノテーターが必要となるため、コストが増加します。

## 5. 技術的な詳細について

EgoLifeプロジェクトで利用された主要な技術要素は以下のとおりです。

*   **EgoGPT (クリップレベルのオムニモーダル理解)**:
    *   ベースモデルとしてLLaVA-OneVisionを採用。
    *   Egocentricビデオドメインへの適応と音声理解のため、EgoIT-99Kという多様なegocentricビデオデータセットでファインチューニング。
    *   LLaVA-OneVisionがQwen2をベースとしているため、Olaと同様の音声ブランチを開発し、LibriSpeechで音声プロジェクションモジュールをトレーニング。
    *   パーソナライゼーションのため、EgoLife Day-1のビデオでEgoGPTをファインチューニングし、個人認識を強化。
    *   30秒クリップごとに視覚および聴覚入力を使用してキャプションを生成。
*   **EgoRAG (長期間文脈の質問応答)**:
    *   2段階のアプローチ:
        *   EgoGPTと統合してビデオクリップのキャプションを抽出し、タイムスタンプ付きのコンテキスト情報を効率的に検索できるように構造化されたメモリモジュールに保存。
        *   言語モデルを使用して、キャプションを継続的に生成し、時間単位および日単位で要約し、スケーラブルな検索のための多層メモリバンクを形成。
    *   質問応答時:
        *   質問に関連する時間窓を、高レベルの要約を最初に検索し、検索を日単位から時間単位に絞り込むことで仮定。
        *   選択された時間窓内で、関連性ベースのスコアリング関数を使用して詳細な検索を実行:
            ```python
            def relevance_score(question, caption, daily_summary, lambda_val):
                # question: 質問文
                # caption: ビデオクリップのキャプション
                # daily_summary: 日単位の要約
                # lambda_val: 視覚とテキストの関連性のバランスを取るための重み
                similarity_caption = similarity(question, caption) # テキストの類似度を計算
                similarity_summary = similarity(question, daily_summary) # テキストの類似度を計算
                score = similarity_caption + lambda_val * similarity_summary
                return score
            ```
        *   視覚的およびテキスト的な関連性のバランスを取るために、Similarity関数と重みlambdaを使用。
        *   言語モデル（EgoGPT、GPT-4oなど）を使用して、検索されたコンテンツを入力として、情報に基づいた応答を生成。

*   **マルチモーダルデータ同期パイプライン:**
    *   AriaメガネとGoProカメラからのマルチソースデータ（ビデオ、オーディオ、IMU）を`slac` codebaseを使用して同期。

## 6. コストや物理的な詳細について

EgoLifeプロジェクトのコストと物理的な詳細について、以下にまとめます。

*   **データ収集環境:**
    *   実験用にカスタム設計された環境（EgoLife house）を使用。
    *   BeijingとMilanの2つの異なる環境でデータ収集を実施。Beijingのデータは完全にアノテーションされ、主要な論文で詳細に議論。Milanのデータは収集済みで、今後のEgoLifeブログシリーズで詳細を説明予定。

*   **データ収集デバイス:**
    *   Meta Ariaメガネを各参加者が着用し、マルチモーダルなegocentricビデオをキャプチャ。
    *   Beijingでは15台、Milanでは6台のGoProカメラを戦略的に配置し、参加者の活動を複数の角度から記録。
    *   ミリ波レーダーを使用して、空間およびモーションデータを収集。Beijingでは2台のTI IWR6843（60GHz）mmWaveモノスタティックレーダーを使用。Milanでは2台のTI IWR6843（60GHz）mmWaveモノスタティックレーダーと1台のAWR1843（77GHz）mmWaveモノスタティックレーダーを使用。
    *   Milanでは、3台のAsus RT-AX82Uデバイスをリビングルームに配置し、空間およびモーションデータの収集に使用。

*   **データセットの詳細:**
    *   Beijingデータセット：7日間で40時間以上の日常活動をキャプチャ。
    *   Milanデータセット：1日セッションで約6時間のアクティビティをキャプチャ。
    *   すべての参加者の顔をぼかし、機密性の高いナンバープレートをぼかし、機密性の高いトピックを含むオーディオセグメントをミュート。

*   **トレーニングデータセット (EgoIT-99K):**
    *   Ego4D, HoloAssist, EGTEA Gaze+, IndustReal, EgoTaskQA, EgoProceL, Charades-Ego, EPIC-KITCHENS, ADLを含む複数のegocentricデータセットを統合。
    *   Q&Aデータ生成用のプロンプトが提示されています。

*   **その他コスト:**
    *   信頼できるアノテーションチームを見つけるのに2か月と5回のトライアルが必要。

## 7. 参考文献のうち、特に参照すべきもの

*   **Damen, D., Doughty, H., Farinella, G. M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., et al. (2018). The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence.**
    *   大規模なegocentricアクション認識における先駆的な研究であり、キッチン環境に焦点を当てています。

*   **Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., et al. (2022). Ego4d: Around the world in 3,000 hours of egocentric video. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).**
    *   より広範な日常活動を網羅し、その大規模な規模のために最も広く使用されているegocentricデータセットの1つとなっています。

*   **Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Llava-next: A strong zero-shot video understanding model, April 2024.**
    *   LLaVAのゼロショットビデオ理解モデルとしての強みを示しています。

*   **Dong, Y., Liu, Z., Sun, H. L., Yang, J., Hu, W., Rao, Y., & Liu, Z. (2024). Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment.**
    *   オムニモーダル言語モデルの進歩に関する洞察を提供します。

これらの論文は、egocentricデータセットの進化、最先端のモデル、そしてこの分野の研究の方向性についての重要な背景情報を提供します。

## 8. この論文を140字以内のツイートで要約すると？

EgoLife: AIメガネで日常を記録し、300時間のデータセットを構築。EgoGPT+EgoRAGで長期的な質問応答を可能に。日常生活支援AIへの一歩！ #AI #Egocentric #LifeAssistant


---


# The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation

[View Paper](http://arxiv.org/abs/2503.04606v1)

## 1. 既存研究では何ができなかったのか

既存のテキストからビデオ（T2V）生成モデルは、大きく分けて自己回帰言語モデル（LLM）ベースと拡散モデルベースの2つのパラダイムに分類されます。それぞれのアプローチには固有の限界がありました。

*   **LLMベース:**
    *   **視覚品質の欠如:** 情報圧縮により、低レベルの視覚的な忠実度が犠牲になり、再構成品質が低下します。
    *   **エラー蓄積:** 自己回帰的な生成のため、時間ステップを跨いでエラーが伝播しやすく、初期段階での小さな誤りがデコード中に拡大するリスクがあります。
*   **拡散モデルベース:**
    *   **意味理解の欠如:** 階層的な特徴が潜在空間で絡み合ったままになり、意味の解釈可能性が希薄化します。
    *   **因果モデリングの欠如:** 明示的な因果制約がないため、時間的な矛盾や意味的な幻覚（Semantic Hallucination）が生じやすいです。

これらの限界により、既存研究では、意味的に一貫性があり、視覚的に高品質なビデオを生成することが困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、LanDiffというハイブリッドアーキテクチャを提案することで、LLMと拡散モデルの強みを融合し、弱点を補完することを目指しました。具体的なアプローチは以下の通りです。

1.  **粗い段階 (Coarse Stage): LLMによる効率的な意味特徴生成**：LLMを活用し、コンパクトな意味特徴を効率的に生成します。
2.  **細かい段階 (Fine Stage): 拡散モデルによる知覚的詳細の追加**：拡散モデルを用いて、知覚的な詳細を追加します。

LanDiffの主要な構成要素と技術的な工夫は以下の通りです。

1.  **効率的なセマンティック・トークナイザー (Semantic Tokenizer)**：
    *   3Dの視覚特徴を効率的な意味圧縮によってコンパクトな1Dの離散表現に変換します（約14,000倍の圧縮率）。
    *   MP4ビデオエンコーディングアルゴリズムに触発され、キーフレームと非キーフレームに分割し、キーフレームに多くのトークンを割り当てます。これにより、時間的な冗長性を活用し、圧縮率を向上させます。
    *   Python風疑似コード:
        ```python
        def tokenize_video(video):
          keyframes, non_keyframes = split_into_key_and_non_keyframes(video)
          keyframe_tokens = compress(keyframes, num_tokens=330)
          non_keyframe_tokens = compress(non_keyframes, num_tokens=74)
          return keyframe_tokens + non_keyframe_tokens

        def compress(frames, num_tokens):
          features = extract_features(frames)  # Theiaモデルなどを使用
          query_tokens = initialize_query_tokens(num_tokens)
          encoded_features = transformer_encoder(features + query_tokens)
          quantized_features = vqvae_quantize(encoded_features[len(features):]) #query tokensのencode後特徴のみ利用
          return quantized_features
        ```
2.  **言語モデル (Language Model)**：
    *   テキストの説明に基づいて、ビデオの設計図となるセマンティック・トークンを生成します。
    *   生成された結果の制御可能性を高めるために、フレーム数とモーションスコア条件を導入します。
    *   Python風疑似コード:
        ```python
        def generate_semantic_tokens(text, num_frames, motion_score):
          text_embedding = text_encoder(text) #T5-XXLなどを使用
          control_conditions = [num_frames, motion_score]
          input = text_embedding + positional_encoding(tokens) + control_conditions
          semantic_tokens = llama_model(input)
          return semantic_tokens
        ```
3.  **ストリーミング拡散モデル (Streaming Diffusion Model)**：
    *   粗いセマンティクスを高忠実度のビデオに洗練します。
    *   長尺ビデオの生成をサポートするために、制限された数の履歴フレームのみを条件として使用するチャンクワイズストリーミング拡散モデルをトレーニングします。これにより、トレーニングと推論の計算コストが大幅に削減されます。
    *   Python風疑似コード:
        ```python
        def diffusion_process(semantic_tokens, vae_latent):
          # VAEを用いてsemantic tokenをsemantic featuresに変換
          semantic_features = video_tokenizer_decoder(semantic_tokens)
          
          # ノイズを加える拡散過程
          noisy_vae_latent = add_noise(vae_latent, timestep)
          
          # ノイズ予測モデル
          predicted_noise = noise_predictor(noisy_vae_latent, timestep, semantic_features) #MMDiTに類似したモデルを使用

          return predicted_noise
        ```

## 3. 結果、何が達成できたのか

LanDiffは、テキストからビデオ生成において、以下の点で優れた成果を達成しました。

*   **高品質なビデオ生成:** VBench T2Vベンチマークで85.43のスコアを獲得し、最先端のオープンソースモデルであるHunyuan Video (13B) や、Sora、Keling、Hailuoなどの商用モデルを上回りました。
*   **長尺ビデオ生成:** 長尺ビデオ生成でも最先端のパフォーマンスを達成し、この分野の他のオープンソースモデルを凌駕しました。
*   **効率的な圧縮:** 3D視覚特徴をコンパクトな1D離散表現に変換し、約14,000倍の圧縮率を達成しました。
*   **意味的一貫性:** 魚が消えない例や、氷の彫刻が溶ける例など、時間的な一貫性と意味的整合性が高く、複雑な時間的変化や物理的プロセスを正確に視覚化できます。

## 4. Limitationや問題点は何か

LanDiffは優れた性能を発揮しますが、いくつかの制限や問題点があります。

*   **データ依存性:** モデルの学習には大規模なビデオとテキストのペアデータセットが必要であり、データの品質と多様性が性能に大きく影響します。
*   **計算コスト:** 長尺ビデオ生成におけるストリーミング拡散モデルのトレーニングと推論には、依然として高い計算コストが必要です。
*   **汎用性の限界:** 特定のオブジェクトの細かい部分（衣服の細部など）の再現性には、まだ改善の余地があります。
*   **評価指標の限界:** VBenchなどの評価指標は、ビデオ生成の品質を完全に捉えるものではなく、人間の主観的な評価とのずれが生じる可能性があります。

私が考える問題点としては、以下のようなものが挙げられます。

*   **制御性の向上:** テキスト以外の条件（例えば、特定のスタイルやカメラワークの指示）による制御性をさらに高める必要があります。
*   **倫理的な考慮:** 生成されるビデオコンテンツの偏りや悪用を防ぐための対策が必要です。

## 5. 技術的な詳細について

LanDiffは、3つの主要なコンポーネントから構成されます。

1.  **ビデオ・セマンティック・トークナイザー:**
    *   **特徴抽出:** 入力ビデオから、Theiaモデルを用いて高レベルなセマンティック特徴を抽出します。Theiaは、複数の視覚タスクモデルから蒸留された、汎用的な視覚特徴抽出器です。
    *   **クエリベースのトークン化:** Transformerアーキテクチャをベースに、クエリトークンを用いて視覚特徴を集約し、離散的な表現に変換します。
    *   **ビデオ・フレーム・グルーピング:** MP4ビデオエンコーディングにヒントを得て、フレームをグループ化し、キーフレーム（I-frame）と非キーフレーム（P-frame）を区別して処理します。これにより、時間的な冗長性を活用し、圧縮率を高めます。I-frameには多くのクエリトークンを割り当て、P-frameには少数のクエリトークンを割り当てることで、フレーム間の差分を効率的に符号化します。
    *   **損失関数:** ビデオ・セマンティック特徴の再構成損失を最小化します。コードブックは、EMA（指数移動平均）を用いて更新されます。
2.  **言語モデル:**
    *   **アーキテクチャ:** LLaMAモデルをベースにした自己回帰言語モデルを使用します。
    *   **入力:** テキストエンコーダ（T5-XXL）からのテキスト埋め込み、位置エンコーディング、および制御条件（フレーム数、モーションスコア）を連結したものを入力として使用します。
    *   **損失関数:** クロスエントロピー損失を用いてモデルを学習します。
3.  **拡散モデル:**
    *   **アーキテクチャ:** MMDiTに類似したアーキテクチャを採用し、ControlNetスタイルの制御モジュールを用いてセマンティック・トークンを条件付けます。
    *   **制御モジュール:** メインモデルの前半層のパラメータをコピーし、線形層を通してメインモデルの出力に追加します。これにより、セマンティック特徴をVAE特徴空間に適合させます。
    *   **ストリーミング推論:** 長尺ビデオ生成をサポートするために、過去のフレームをプロンプトとして使用し、次のフレームを生成するチャンクワイズストリーミング戦略を導入します。
    *   **損失関数:** ノイズ予測誤差を最小化します。

## 6. コストや物理的な詳細について

論文には、以下の情報が記載されています。

*   **モデルサイズ:**
    *   LanDiff: 5B (50億パラメータ)
    *   ビデオ detokenizer: 3B (30億パラメータ)、うち学習可能な制御モジュールは1B
    *   言語モデル: 2B (20億パラメータ)
*   **データセット:**
    *   ビデオ・トークナイザーと言語モデル: 2億のビデオとテキストのペアを持つ内部データセット
    *   拡散モデル: 300万件の高品質なビデオとテキストのペアを含むデータセット
*   **解像度:** ビデオの解像度は480x720にスケールされ、中央がクロップされます。
*   **トレーニングの詳細:**
    *   AdamWオプティマイザーを使用
    *   学習率のwarm-upとcosine decay戦略を使用
    *   Model Exponential Moving Average (EMA)を使用

論文中にはGPUの種類や台数、トレーニング時間などの詳細な情報は明示されていません。詳細なコストについては、今後の研究で明らかにされることが期待されます。

## 7. 参考文献のうち、特に参照すべきもの

*   **Huang et al., VBench: Comprehensive benchmark suite for video generative models. CVPR 2024:** テキストからビデオ生成モデルの評価に広く使用されているVBenchベンチマークについて理解するために重要です。
*   **Ho et al., Denoising diffusion probabilistic models. NeurIPS 2020:** 拡散モデルの基礎となる理論について理解するために重要です。
*   **Touvron et al., LLaMA: Open and Efficient Foundation Language Models, February 2023:** 言語モデルのアーキテクチャに関する重要な情報源です。
*   **Zhang et al., Adding conditional control to text-to-image diffusion models. ICCV 2023:** ControlNetのアーキテクチャについて理解するために重要です。
*   **Kondratyuk et al., VideoPoet: A large language model for zero-shot video generation. ICML 2024:** LLMを用いたビデオ生成に関する先行研究として重要です。
*   **Yu et al., Language model beats diffusion - tokenizer is key to visual generation. ICLR 2024:** トークナイザーの重要性について述べており、本研究の動機付けにもなっています。

## 8. この論文を140字以内のツイートで要約すると？

テキストからビデオ生成の新手法LanDiff✨LLMと拡散モデルの良いとこ取りで高品質＆長尺ビデオを実現！独自のトークナイザーで圧縮率も大幅UP🚀VBenchでSOTA達成🎉 #T2V #VideoGeneration #AI


---


# How to Steer LLM Latents for Hallucination Detection?

[View Paper](http://arxiv.org/abs/2503.01917v1)

## 1. 既存研究では何ができなかったのか

既存研究は、大規模言語モデル（LLM）の幻覚検出において、以下の点で課題を抱えていました。

*   **埋め込み空間の分離の不備:** 既存の手法では、LLMの潜在空間を活用して幻覚を検出していましたが、LLMの埋め込みは言語的な一貫性を最適化するように学習されており、事実の正確さよりも流暢さや構文的な正しさが優先されていました。そのため、真実と幻覚の内容を明確に分離することが困難でした。既存のLLMの埋め込みは、幻覚検出タスクに最適化されていませんでした。
*   **計算コスト:** LLMをファインチューニングして幻覚を抑制する手法は、計算コストが高く、モデルのパラメータを変更する必要がありました。
*   **ラベル付きデータの不足:** LLM生成に対する真実性の注釈付きの大規模なデータセットは、コストがかかり、時間もかかるため、十分に存在しませんでした。
*   **ドメインシフト:** 異なるデータセットに適用した場合の性能が安定しませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、これらの課題を解決するために、以下の様なアプローチを取りました。

*   **Truthfulness Separator Vector (TSV) の導入:** 軽量で柔軟なsteering vectorであるTSVを導入し、推論時にLLMの表現空間を再形成することで、モデルのパラメータを変更せずに、真実と幻覚の出力をより明確に分離できるようにしました。
*   **2段階の学習フレームワーク:**
    *   **初期段階:** 少量のラベル付きデータ（exemplar set）を使用して、TSVを学習し、コンパクトで分離されたクラスターを形成しました。
    *   **拡張段階:** 大量のラベルなしLLM生成データを用いて、最適な輸送（Optimal Transport）に基づく擬似ラベリングアルゴリズムと信頼度ベースのフィルタリングプロセスを組み合わせて学習データを拡張しました。これにより、手動でのラベル付けへの依存を減らしつつ、学習データの多様性を向上させました。
*   **von Mises-Fisher (vMF) 分布の利用:** 最終層のlast-token embeddingsを単位ノルムを持つ超球面分布でモデル化し、真実と幻覚データを明確なクラスターとして形成しました。
*   **steアリング強度 (steering strength) の調整:** 適切な強度のsteeringを適用することで、表現空間を効果的に分離し、パフォーマンスを最適化しました。
*   **信頼性のある擬似ラベルの選択:** モデルの予測の不確実性を測定し、最も信頼性の高い擬似ラベルのサンプルのみを学習プロセスに含めることで、ノイズの影響を軽減しました。
*   **学習されたクラスプロトタイプの利用:** 推論時に、学習されたクラスプロトタイプを利用して真実性スコアを計算し、幻覚検出を行いました。

## 3. 結果、何が達成できたのか

本研究の結果、以下の点が達成されました。

*   **最先端の性能:** TSVは、最先端の幻覚検出精度を達成しました。特に、TruthfulQAデータセットにおいて、既存手法を大幅に上回る性能を示しました。
*   **ラベル効率:** TSVは、ごくわずかなラベル付きデータ（32個のサンプル）で、完全に教師ありの学習に匹敵する性能を達成しました。
*   **強力な汎化性能:** TSVは、未知のデータセットに対しても高い汎化能力を発揮し、ドメインシフトに対するロバスト性を示しました。
*   **計算効率:** TSVは、推論時の計算コストが低く、実用的な解を提供します。
*   **モジュール性:** TSVは、LLMのパラメータを変更せずに適用できるため、既存のLLMにプラグアンドプレイで組み込むことができます。

## 4. Limitationや問題点は何か

本研究には、以下の制限事項と問題点があります。

*   **ハイパーパラメータの調整:** TSVの性能は、steering strength（λ）、vMF分布の集中パラメータ（κ）、Sinkhornアルゴリズムの正則化パラメータ（ε）などのハイパーパラメータに依存します。これらのパラメータの最適な値は、データセットやモデルによって異なる可能性があり、適切な調整が必要となります。
*   **最適なsteering locationの特定:** TSVを適用する最適なレイヤー位置は、モデルアーキテクチャに依存します。論文中ではearly-middle layersが効果的であると示唆されていますが、すべてのLLMに当てはまるとは限りません。
*   **ドメインシフトに対する完全なロバスト性:** TSVは、ドメインシフトに対して一定のロバスト性を示しましたが、完全にドメインシフトの影響を受けないわけではありません。大幅なドメインシフトが発生した場合、TSVの性能が低下する可能性があります。
*   **評価指標:** BLEURTスコアとゴールドスタンダードの回答の閾値を使用して真実性を判断していますが、この閾値の選択が結果に影響を与える可能性があります。
*   **学習データの偏り:** 擬似ラベル生成に使用するLLM生成データに偏りが存在する場合、TSVの性能に悪影響を及ぼす可能性があります。
*   **negativeな例の欠如**: 完全に教師ありの手法と比較すると、TSVはnegativeな例（幻覚データ）のラベル付けされた例に依存している可能性があります。この依存により、モデルが幻覚の微妙な違いを十分に把握できない場合があります。

## 5. 技術的な詳細について

TSVは、LLMの内部表現空間を操作して、真実な出力と幻覚出力の分離を強化する手法です。以下に、技術的な詳細を説明します。

1.  **問題設定:**
    *   入力プロンプト`x_prompt`が与えられたとき、LLMが生成したテキスト`x_generated`が真実かどうかを判定する二値分類問題を扱います。
    *   真実なテキストの分布を`P_true`、幻覚テキストの分布を`P_hallucination`とします。
    *   ラベルなしデータセット`D_U`と、少量のラベル付きexemplar set `D_E`を利用します。

2.  **Truthfulness Separator Vector (TSV):**
    *   学習可能なベクトル`v`を定義します。`v`は、LLMのパラメータではなく、外部から操作するベクトルです。
    *   推論時に、LLMの中間層の潜在状態`h^(l)`に`λv`を加えます。
        ```python
        h_l = h_l + lambda_ * v
        ```
        ここで、`λ`はsteering strengthを制御するハイパーパラメータです。
    *   TSVを適用した後の最終層のlast-token embeddingを`Φ_final(h^(l) + λv)`とします。

3.  **初期学習:**
    *   exemplar set `D_E`を用いて、TSV `v`を学習します。
    *   目的関数は、exemplar setに対する負の対数尤度を最小化することです。
        ```python
        def loss(v, D_E):
          loss_sum = 0
          for (h_i, c_i) in D_E:
            r_i_v = normalize(phi_final(h_i + lambda_ * v)) # normalizeはベクトルを単位ベクトルに変換する関数
            p_c_given_r_i_v = von_mises_fisher(c_i, r_i_v, mu, kappa) # von Mises-Fisher分布に基づく確率計算
            loss_sum += -log(p_c_given_r_i_v)
          return loss_sum / len(D_E)
        ```
    *   各クラス（真実、幻覚）のcentroid `mu_c`を、指数移動平均で更新します。
        ```python
        mu_c = normalize(alpha * mu_c + (1 - alpha) * r_bar_v)
        r_bar_v = sum(q(c, r_i_v) * r_i_v) / sum(q(c, r_i_v)) # クラスcのembeddingsの平均
        ```
        ここで、`α`は平滑化パラメータ、`q(c, r_i_v)`はtarget label分布です。

4.  **拡張学習:**
    *   ラベルなしデータセット`D_U`を用いて、学習データを拡張します。
    *   Optimal Transport (OT) に基づく擬似ラベリングアルゴリズムを用いて、`D_U`の各サンプルに擬似ラベルを割り当てます。
        *   目的関数:
        ```python
        def ot_loss(Q, P, epsilon):
          loss_sum = 0
          for m in range(M): # Mはラベルなしサンプル数
            for c in range(2): # cはクラス（真実、幻覚）
              loss_sum += -Q[m, c] * log(P[m, c])
          return loss_sum - epsilon * H(Q) # H(Q)はエントロピー正則化項
        ```
        制約条件:
            *   `Q.sum(axis=1) == 1/M` (各サンプルは必ずいずれかのクラスに割り当てられる)
            *   `Q.sum(axis=0) == w` (各クラスに割り当てられるサンプル数の割合がexemplar setのクラス分布`w`に一致する)
            *   `Q`は、サンプル`m`がクラス`c`に割り当てられる確率を表す行列。
            *   `P`は、初期学習後のモデルが出力する各サンプルのクラス確率を表す行列。
    *   Sinkhornアルゴリズムを用いて、効率的に`Q`を計算します。
        ```python
        Q = diag(alpha) @ P**(1/epsilon) @ diag(beta) # Sinkhornアルゴリズムの解
        ```

    *   不確実性に基づいて、信頼性の高い擬似ラベルサンプルのみを選択します。
    ```python
    Omega = [-sum(q(c, r_i_v) * log(p(c, r_i_v)) for c in range(2)) for i in range(len(D_U))] # 各サンプルの不確実性を計算
    D_S = top_k_samples(D_U, Omega) # 不確実性の低い上位k個のサンプルを選択
    ```
    *   選択されたサンプルをexemplar setに追加し、TSVを再学習します。

5.  **推論:**
    *   学習されたクラスプロトタイプ`mu_truthful`と`mu_hallucinated`を用いて、テスト入力の真実性スコアを計算します。
        ```python
        def truthfulness_score(x_test, v):
          r_test_v = normalize(phi_final(h_test + lambda_ * v))
          score = exp(kappa * mu_truthful.T @ r_test_v) / (exp(kappa * mu_truthful.T @ r_test_v) + exp(kappa * mu_hallucinated.T @ r_test_v))
          return score
        ```
    *   真実性スコアに基づいて、幻覚を検出します。

## 6. コストや物理的な詳細について

論文中に記載されているコストおよび物理的な詳細は以下の通りです。

*   **モデル:**
    *   LLaMA-3.1-8b & 70b, Qwen-2.5-7b & 14b
    *   実験によっては、LLaMA-2-chat-7bも使用
*   **データセット:**
    *   TruthfulQA (817 QA pairs)
    *   TriviaQA
    *   SciQ (1,000 QA pairs)
    *   NQ Open (3,610 QA pairs)
*   **学習の詳細:**
    *   AdamW optimizer
    *   Learning rate: 5e-03
    *   Batch size: 128
    *   Epochs: exemplar setのみで20 epochs、拡張後にさらに20 epochs
*   **その他:**
    *   Beam search with 5 beams
    *   Multinomial samplingで10サンプル生成 (temperature 0.5)

論文で追跡された実行に基づくと、推定される合計トレーニング時間と推論時間は非常に少なく、LLaMA-3.1-8bおよびQwen-2.5-7bでは約0.1 GPU時間、Qwen-2.5-14bでは0.2 GPU時間、LLaMA-3.1-70bでは1 GPU時間です。

## 7. 参考文献のうち、特に参照すべきもの

本研究を理解する上で、特に参照すべき参考文献は以下の通りです。

*   **TruthfulQA: Measuring how models mimic human falsehoods.**  LLMの真実性を評価するためのデータセット。
*   **Sinkhorn distances: Lightspeed computation of optimal transport.**  Optimal Transport の計算効率を向上させる Sinkhorn アルゴリズムに関する論文。
*   **Llama 2: Open foundation and fine-tuned chat models.** 使用されたLLMモデルに関する論文。
*   **Representation engineering: A top-down approach to ai transparency.**  LLMの内部表現を操作するアプローチに関する論文。
*   **Haloscope: Harnessing unlabeled llm generations for hallucination detection.** 手法を比較している既存研究。

## 8. この論文を140字以内のツイートで要約すると？

LLMの幻覚検出にTSVを提案！少量のラベルデータと大量の非ラベルデータで、モデルを一切変更せずに幻覚を劇的に抑制。ドメインシフトにも強く、実用的な安全性向上に貢献 #LLM #AI #HallucinationDetection


---


# Token-Efficient Long Video Understanding for Multimodal LLMs

[View Paper](http://arxiv.org/abs/2503.04130v1)

## 1. 既存研究では何ができなかったのか

既存のVideo-LLMは、動画をフレームのシーケンスとして扱い、各フレームを独立して処理していました。このアプローチには以下の課題がありました。

*   **明示的な時間的モデリングの欠如:** フレーム間の時間的な関係性を捉えることができず、動画内の動的なパターンを理解することが困難でした。LLMが静止画像のシーケンスから時間的な関係性を推論する必要があり、計算負荷が増大しました。
*   **長尺動画への対応の難しさ:** 計算コストが高く、長尺動画を効率的に処理することが困難でした。フレームをサブサンプリングすることで計算量を削減する手法が用いられていましたが、重要な情報を失う可能性がありました。
*   **フレーム間の冗長性の無視:** 連続するフレームには重複する情報が含まれているにもかかわらず、既存手法では効果的に圧縮することができませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、これらの課題を解決するために、STORM (SpatioTemporal Token Reduction for Multimodal LLMs)という新しいアーキテクチャを提案しました。主なアプローチは以下の通りです。

*   **Mamba State Space Modelを用いた時間エンコーダの導入:** 画像エンコーダとLLMの間に、Mamba State Space Model (SSM) を活用した専用の時間エンコーダを導入しました。これにより、フレーム間の時間情報を画像トークンに統合し、動画シーケンス全体にわたるフレーム間の動的な関係性を保持することが可能になりました。
*   **トークン削減戦略の導入:** 効果的なトークン削減戦略として、テスト時のサンプリングと、学習ベースの空間および時間プーリングを導入しました。これにより、重要な時間情報を犠牲にすることなく、LLMの計算負荷を大幅に削減しました。
*   **空間および時間方向のトークン圧縮:** 時間的プーリングは時間方向のトークン数を減らし、空間的平均プーリングはフレームごとのトークン数を減らします。これらの圧縮戦略は、冗長性を最小限に抑えながら、重要な情報を保持するように学習中に最適化されます。
*   **テスト時の時間トークンサンプリング:** 学習済みのトークンから、テスト時にフレームを間引くことなくトークンをサンプリングすることで計算量を削減します。Mamba層で時間情報がエンコードされているため、重要な時間情報を失うことなくトークン数を削減できます。

## 3. 結果、何が達成できたのか

STORMは、長尺動画理解タスクにおいて、既存の最先端手法を大幅に上回る性能を達成しました。

*   **性能向上:** MLVUおよびLongVideoBenchにおいて、5%以上の性能向上が確認されました。MVBenchで71.3%、MLVUで72.5%、LongVideoBenchで59.5%、VideoMMEで63.4%の精度を達成しています。
*   **計算コストの削減:** 計算コストを最大8倍削減しました。
*   **推論遅延の削減:** デコード遅延を2.4〜2.9倍削減しました（入力フレーム数が固定の場合）。
*   **効率的な長尺動画理解:** 複雑な長尺動画シーンに関する自由形式のクエリを効率的に処理できるようになりました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

本文で明示的に言及されている制限事項は少ないですが、以下の点が考えられます。

*   **データセットへの依存性:** モデルの性能は、学習に使用するデータセットの品質と多様性に大きく依存します。もし利用したデータセットの特性が、実世界の動画の分布と大きく異なる場合、モデルの汎化性能が低下する可能性があります。
*   **タスク固有の性能差:** MLVUやLongVideoBenchに比べて、MVBenchやVideoMMEでのテスト時圧縮の効果が小さいことが述べられています。タスクの種類によっては、圧縮による情報損失が影響しやすい可能性があります。
*   **Mambaの計算コスト:** Mamba層は線形計算量ですが、従来のTransformerと比較してオーバーヘッドが存在する可能性があります。特に、GPUメモリが限られている場合、バッチサイズを小さくする必要があるかもしれません。
*   **汎用性:** 特定のアーキテクチャ (VILA) をベースにしているため、他のアーキテクチャへの適用には調整が必要となる可能性があります。

個人的に考える制限事項や問題点は以下の通りです。

*   **リアルタイム処理:** 長尺動画を処理するため、リアルタイムでの処理には向いていない可能性があります。
*   **長尺動画の定義:** 長尺動画の定義が曖昧であり、どの程度の長さの動画まで効果を発揮するか不明確です。
*   **評価指標:** 動画理解の評価指標はまだ発展途上であり、モデルの真の理解度を測るには不十分な場合があります。
*   **学習の安定性:** SSMは学習が難しい場合があり、パラメータの調整が難しい可能性があります。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

STORMアーキテクチャは、以下の主要なコンポーネントで構成されています。

1.  **Vision Encoder:** 入力動画の各フレームをトークン化します。論文ではSigLIPが用いられていますが、CLIPなどの他のエンコーダも利用可能です。

    ```python
    # 疑似コード: Vision Encoder
    def vision_encoder(frame):
        tokens = SigLIP(frame) # 例： SigLIPエンコーダを使用
        return tokens
    ```

2.  **Temporal Projector (Mamba Layer):** Vision Encoderからのトークンを受け取り、時間情報を統合します。Mamba SSMを使用し、長期的な依存関係をモデル化します。Mambaレイヤーは、空間的スキャンと時間的スキャンを同時に実行し、包括的な時空間情報をキャプチャします。

    ```python
    # 疑似コード: Temporal Projector (Mamba)
    def temporal_projector(tokens):
        # tokens: (T, N, D) - T: 時間, N: トークン数, D: 次元
        x = linear_layer(tokens)
        for _ in range(num_mamba_layers):
            x = x + MambaMixer(layer_norm(x))
        return x
    ```

    Mambaレイヤーの核となる状態空間モデル (SSM) は、以下の式で表されます。

    ```
    ht = At * ht-1 + Bt * xt
    yt = Ct * ht
    ```

    ここで、`xt` は入力、`yt` は出力、`ht` は隠れ状態、`At`, `Bt`, `Ct` は学習可能なパラメータです。Mambaでは、これらのパラメータが入力 `xt` に依存するように動的に調整されます。

3.  **Token Reduction:** LLMへの入力トークン数を削減するためのモジュールです。

    *   **Temporal Pooling:** 時間方向に平均プーリングを適用します。

        ```python
        # 疑似コード: Temporal Pooling
        def temporal_pooling(tokens, pool_size):
            T, N, D = tokens.shape
            pooled_tokens = []
            for i in range(0, T - pool_size + 1, pool_size):
                pooled_token = torch.mean(tokens[i:i+pool_size], dim=0)
                pooled_tokens.append(pooled_token)
            return torch.stack(pooled_tokens)
        ```

    *   **Spatial Pooling:** 空間方向に平均プーリングを適用します。

        ```python
        # 疑似コード: Spatial Pooling
        def spatial_pooling(tokens, pool_size):
            T, N, D = tokens.shape
            #  N = H * W を仮定 (H: 高さ, W: 幅)
            H = int(N**0.5) # 高さの計算
            W = H # 幅も高さと同じと仮定
            tokens = tokens.reshape(T, H, W, D)
            pooled_tokens = torch.nn.functional.avg_pool2d(
                tokens.permute(0, 3, 1, 2),  # (T, D, H, W) に変換
                kernel_size=pool_size,
                stride=pool_size
            )
            # (T, D, H', W') -> (T, N', D) に変換
            N_prime = (H // pool_size) * (W // pool_size)
            return pooled_tokens.permute(0, 2, 3, 1).reshape(T, N_prime, D) # (T,H',W',D)
        ```

    *   **Temporal Sampling (Test-time):** テスト時に時間方向にトークンをサブサンプリングします。

        ```python
        # 疑似コード: Temporal Sampling
        def temporal_sampling(tokens, sample_rate):
            T, N, D = tokens.shape
            sampled_tokens = tokens[::sample_rate]
            return sampled_tokens
        ```

4.  **Large Language Model (LLM):** 削減されたトークンを受け取り、動画の内容を理解し、質問に答えます。

    ```python
    # 疑似コード: LLM
    def large_language_model(tokens, prompt):
        #  tokens: (T', N', D)
        # ここでは、LLMが入力トークンとプロンプトを使用して応答を生成すると仮定
        response = LLM(tokens, prompt)
        return response
    ```

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

論文に記載されている情報に基づくと、以下の詳細が分かっています。

*   **GPU:** NVIDIA DGX A100-80G が使用されました。
*   **トレーニング時間:** Mambaモジュールを追加した場合、トレーニング時間がわずか1時間程度増加した (VILA: 19.1時間, STORM: 20.1時間) と記述されています。
*   **データセット:**
    *   LLaVA-CC3M-Pretrain-595K: Alignment Stageで使用
    *   M4-Instruct-Video など、複数の高品質な画像・動画データセットを組み合わせた混合データセット (12.5M samples): Supervised Fine-Tuning (SFT) Stageで使用
    *   LLaVA-Video (Long video fine-tuning stage): SFTデータから選択した動画（フレーム数128以上）を使用
*   **モデルサイズ:** 具体的なパラメータ数は明記されていませんが、VILAをベースにしており、LLMとしてはLlama 3が使用されているため、数十億パラメータ規模であると考えられます。
*   **入力サイズ:** SFT stageでは各動画入力に対して32フレームを使用。学習時のトークン圧縮（時間的プーリング）モデルでは、32フレームを8フレームに圧縮。空間プーリングモデルでは、画像あたり256トークンを64トークンに圧縮。Long video fine-tuning stageでは、各動画入力に128フレームを使用。

## 7. 参考文献のうち、特に参照すべきもの

*   **Mamba: Linear-time sequence modeling with selective state spaces:** STORMの核となるMamba SSMの詳細が記述されています。
*   **Vila: On pre-training for visual language models:** ベースラインアーキテクチャであるVILAの詳細が記述されています。

## 8. この論文を140字以内のツイートで要約すると？

長尺動画LLMのSTORM⚡️！Mambaで時空間情報を効率的に捉え、トークン圧縮で計算コスト大幅減。MLVU, LongVideoBenchで性能SOTA達成🎉。長尺動画理解の新潮流！ #VideoLLM #Mamba #AI


---


# Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks

[View Paper](http://arxiv.org/abs/2503.04378v1)

## 1. 既存研究では何ができなかったのか

既存研究における Inference-Time Scaling の多くの手法は、タスクが検証可能な答えを持つことを前提としていました。そのため、数学、コーディング、論理的推論といった領域に限定され、一般的なオープンエンドなタスクには適用できませんでした。また、人間からのフィードバックを活用した Reinforcement Learning from Human Feedback (RLHF) は、応答の有用性に関する選好情報や、事実の正確性、創造性、複雑性、一貫性といった固定された次元に基づくものであり、応答に対する包括的なフィードバックを捉えるには不十分でした。さらに、応答前に思考するようモデルを訓練するアプローチも、検証可能な答えを持つタスクに限定され、一般的なオープンエンドな問題には適用できませんでした。既存のLLMに自己フィードバックと編集を促すだけでは、効果的なフィードバックが得られず、改善が最小限にとどまる可能性がありました。LLMが自己修正できるという報告もありますが、一般的なタスクでフィードバックに基づいて応答を編集できるかどうかは不明確でした。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、人間の学習プロセスに着想を得て、オープンエンドな一般的なタスクに対する Inference-Time Scaling を実現するために、専用の Feedback モデルと Edit モデルを導入しました。具体的なアプローチは以下の通りです。

1.  **データ収集:** 7000人以上の多様なアノテーターから、一般的なオープンエンドタスクに関するデータを収集しました。アノテーターは、モデルの応答の有用性に関するテキストフィードバックを提供し、そのフィードバックに基づいて応答を編集しました。データは、General, STEM, Coding, Multilingual のカテゴリに分類されました。
2.  **モデルの訓練:** 収集したデータを用いて、Feedback モデルと Edit モデルを訓練しました。
    *   **Feedback Model:** ユーザープロンプトとモデル応答を与えられたときに、人間が提供するフィードバックを模倣するように訓練されました。
    *   **Edit Model:** ユーザープロンプト、モデル応答、およびフィードバックのセットを与えられたときに、編集された応答を生成するように訓練されました。
    *   **Edit Preference Model:** 良い編集と不十分な編集を区別するように訓練されました。不十分な編集には、フィードバックに基づいていない編集や、編集されていない応答が含まれます。
3.  **Inference-Time Scaling:** 推論時に、初期応答を生成するモデル、フィードバックを提供する Feedback モデル、および応答を編集する Edit モデルを連携させました。さらに、初期応答の数、フィードバックの数、および編集された応答の数をスケーリングすることで、パフォーマンスを向上させました。

## 3. 結果、何が達成できたのか

このアプローチにより、以下の成果を達成しました。

*   **Arena Hard での SoTA パフォーマンス:** Llama 3 ファミリーの 70B モデルに基づいた最適化されたセットアップは、Arena Hard ベンチマークで 92.7 を達成し、OpenAI o1-preview-2024-09-12 (90.4) および DeepSeek R1 (92.3) を上回りました。
*   **効果的な Inference-Time Scaling:** 初期応答の数、フィードバックの数、および編集された応答の数をスケーリングすることで、パフォーマンスを効果的に向上させることができました。
*   **汎用的なタスクへの適用:** 提案手法は、検証可能な答えを持つタスクに限定されず、一般的なオープンエンドなタスクにも適用可能です。
*   **蒸留による性能向上:** Feedback-Editシステムによって生成されたデータを使用してモデルをトレーニングすることで、推論時にスケーリングシステムを使用せずに、モデルの応答を改善できることを示しました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

### 本文で言及されているもの

*   **データ収集の限界:** ShareGPT や WildChat から収集されたプロンプトは、収集時期 (2023年4月、2024年4月) の時点でユーザーが送信していたクエリに限定されており、現在の LLM の能力を最大限に活用した複雑なクエリを十分に反映していない可能性があります。また、応答が 2000 ワードを超えるプロンプトや、入力が 4000 ワードを超えるプロンプトは、アノテーターの負担を考慮して除外されました。さらに、2024年7月以降の知識を必要とするプロンプトも除外されました。
*   **サンプリング方法の改善余地:** 複数のフィードバックをサンプリングし、建設的な批判のキーワードの数に基づいてフィードバックをランク付けする方法は、計算効率の点で最適ではありません。Constrained decoding などの技術を使用して、より効率的なサンプリングが可能です。
*   **Inference-Time Scaling 手法:** Feedback-Edit システムが唯一の有用な Inference-Time Scaling 手法であると解釈される可能性があること。他の手法も異なる設定で有用であり、互いに補完し合う可能性があります。
*   **Reward Modelの限界:** Llama-3.1-Nemotron-70B-Reward モデルが常に最適な応答を選択できるとは限らないこと。

### その他

*   **アノテーションの質:** アノテーターの質によって、フィードバックと編集の質が左右される可能性があります。特に、複雑なタスクでは、専門知識を持つアノテーターの確保が重要です。
*   **計算コスト:** Inference-Time Scaling は、計算コストが増加する可能性があります。特に、初期応答の数、フィードバックの数、および編集された応答の数を増やすと、コストが大幅に増加する可能性があります。
*   **言語の偏り:** データセットが特定の言語に偏っている可能性があります。特に、Coding タスクでは Python や Javascript に、Multilingual タスクでは中国語に偏っている傾向があります。
*   **倫理的な懸念:** モデルが生成するフィードバックや編集された応答に、偏見や有害なコンテンツが含まれる可能性があります。データ収集時にフィルタリングを行っていますが、完全に排除することは困難です。
*   **フィードバックと編集の関連性:** 編集者が実際にどのフィードバックを使用したかを明示的に収集していないため、変更サマリーとフィードバックの対応関係に基づいて間接的に判断しています。このため、編集モデルがすべてのフィードバックを適切に学習しているとは限りません。
*   **環境への影響:** 大規模な言語モデルの学習と推論は、大量のエネルギーを消費し、環境に負荷を与える可能性があります。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

この論文では、Feedback モデルと Edit モデルを組み合わせることで、オープンエンドなタスクに対する Inference-Time Scaling を実現しています。ここでは、技術的な詳細について説明します。

### データセット

データセットは、General, STEM, Coding, Multilingual のカテゴリに分類されています。Coding と Multilingual のプロンプトは ShareGPT から、General と STEM のプロンプトは WildChat から収集されました。データセットは、以下の3つのタスクのために使用されました。

*   **Feedback Demonstration:** ユーザープロンプト、モデル応答、フィードバックのペアで構成されます。モデルは、与えられたプロンプトと応答に対して、人間が提供するフィードバックを模倣するように訓練されます。
*   **Edit Demonstration:** ユーザープロンプト、モデル応答、フィードバック、編集された応答のタプルで構成されます。モデルは、与えられたプロンプト、応答、フィードバックに基づいて、編集された応答を生成するように訓練されます。
*   **Edit Preference:** ユーザープロンプト、モデル応答、フィードバック、良い編集、不十分な編集のタプルで構成されます。モデルは、良い編集と不十分な編集を区別するように訓練されます。不十分な編集には、フィードバックに基づいていない編集や、編集されていない応答が含まれます。

### モデルアーキテクチャと学習

*   **ベースモデル:** Llama 3 ファミリーの 70B モデルを使用しています。具体的には、`Llama-3.1-Nemotron-70B-Instruct`, `Llama-3.3-70B-Instruct` が使用されました。
*   **学習方法:**
    *   **Supervised Fine-Tuning (SFT):** Feedback Demonstration と Edit Demonstration データセットを使用して、Feedback モデルと Edit モデルをそれぞれ SFT で学習します。
    *   **Reward Modeling (RM):** Edit Preference データセットを使用して、Edit RM を学習します。Bradley-Terry (BT) スタイルの Reward Model を使用し、過学習を防ぐためにエポック数を制限しています。
    *   **Reinforcement Learning (RL):** Edit SFT モデルを Edit RM でガイドして RL で学習します。
*   **ハイパーパラメータ:**
    *   最大シーケンス長: 4096 トークン
    *   最適化アルゴリズム: AdamW
    *   学習率: SFT (Feedback, Edit) で `5e-5`, RM, Distillation で `1e-5`, RL で `5e-6`。
    *   バッチサイズ: 128 (グローバル)
    *   ロールアウト数 (RL): 4
    *   生成トークン数 (RL): 2048
    *   温度 (RL): 1.0
    *   Top P (RL): 0.9
*   **蒸留 (Distillation):** Feedback-Edit システムによって生成されたデータを使用して、教師あり学習でモデルを訓練します。

### 推論時のパイプライン

1.  **初期応答生成:** 指示に従うモデル (`Llama-3.1-Nemotron-70B-Instruct` など) を使用して初期応答を生成します。 Best-of-N サンプリングを使用し、複数の応答を生成し、Reward Model で最適なものを選択します。
    ```python
    def generate_initial_responses(prompt, model, n=1):
        responses = []
        for _ in range(n):
            response = model.generate(prompt, temperature=0.7, top_p=0.9)
            responses.append(response)
        return responses

    def select_best_response(responses, reward_model):
        best_response = None
        best_reward = -float('inf')
        for response in responses:
            reward = reward_model.score(response)
            if reward > best_reward:
                best_reward = reward
                best_response = response
        return best_response
    ```
2.  **フィードバック生成:** Feedback モデルを使用して、初期応答に対するフィードバックを生成します。複数のフィードバックを生成し、建設的な批判のキーワードの数に基づいてランク付けします。
    ```python
    def generate_feedback(prompt, response, feedback_model, n=10):
        feedback_list = []
        for _ in range(n):
            feedback = feedback_model.generate(prompt + " " + response, temperature=0.7, top_p=0.9)
            feedback_list.append(feedback)
        return feedback_list

    def rank_feedback(feedback_list, constructive_keywords):
        def score_feedback(feedback):
            score = 0
            for keyword in constructive_keywords:
                if keyword in feedback:
                    score += 1
            return score
        
        ranked_feedback = sorted(feedback_list, key=score_feedback, reverse=True)
        return ranked_feedback
    ```
3.  **応答編集:** Edit モデルを使用して、フィードバックに基づいて初期応答を編集します。複数の編集された応答を生成し、Reward Model で最適なものを選択します。
    ```python
    def generate_edits(prompt, response, feedback_list, edit_model, n=1):
        edited_responses = []
        for feedback in feedback_list[:n]:  # 使用するフィードバックは最大 n 個
            edited_response = edit_model.generate(prompt + " " + response + " " + feedback, temperature=0.7, top_p=0.9)
            edited_responses.append(edited_response)
        return edited_responses

    def select_best_edit(edited_responses, reward_model):
        best_edit = None
        best_reward = -float('inf')
        for edit in edited_responses:
            reward = reward_model.score(edit)
            if reward > best_reward:
                best_reward = reward
                best_edit = edit
        return best_edit
    ```
4.  **スケーリング:** 初期応答の数、フィードバックの数、および編集された応答の数をスケーリングすることで、パフォーマンスを向上させます。

### その他

*   Nemo-Aligner フレームワークを使用
*   推論には 0.6.6.post1 コンテナを使用

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

*   **データセット:**
    *   7000 人以上のアノテーター
    *   さまざまなオープンエンドの一般的なタスク
    *   General, STEM, Coding, Multilingual のカテゴリ
    *   Coding と Multilingual のプロンプトは ShareGPT から、General と STEM のプロンプトは WildChat から収集
*   **モデルサイズ:** 70B パラメータ
*   **GPU:** 8x H100-80GB SXM GPUs
*   **学習時間:** 詳細な学習時間は不明ですが、H100-eqv. node-hours で測定されています。SFT, RM, Distillation 実験では学習率の探索が行われました。
*   **フレームワーク:** Nemo-Aligner
*   **詳細な設定:**
    *   max sequence length: 4096
    *   global batch size: 128
    *   AdamW optimizer

具体的な学習コスト（時間や GPU 使用量）は論文には明記されていませんが、70B モデルの学習には相当な計算資源が必要となることが予想されます。

## 7. 参考文献のうち、特に参照すべきもの

*   **Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning:** DeepSeek R1 との比較について言及されているため、参照すべきです。
*   **Training language models to follow instructions with human feedback:** RLHF の初期の研究であり、本研究との比較の文脈で重要です。
*   **HelpSteer2: Open-source dataset for training top-performing reward models:** データセットの作成において参考にしているため、関連性が高いです。
*   **Chatbot arena: An open platform for evaluating llms by human preference:** Arena Hard ベンチマークの概要を知る上で重要です。
*   **https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct, https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct:** 使用されたモデルの詳細について確認できます。

## 8. この論文を140字以内のツイートで要約すると？

FeedbackとEditモデルでLLMをInference-Time Scaling！人間の学習に着想、汎用タスクでOpenAIやDeepSeek超えの性能を達成。初期応答、フィードバック、編集のスケーリングが鍵。 #LLM #AI #InferenceTimeScaling


---

はい、承知いたしました。以下に、ご指定のフォーマットで回答します。


# LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation

[View Paper](http://arxiv.org/abs/2503.02972v2)

## 1. 既存研究では何ができなかったのか

既存研究では、大規模言語モデル（LLM）の推論能力の評価において、評価ベンチマークのデータへの露出による過大評価を防ぐことが困難でした。具体的には以下の点が課題でした。

*   **データ汚染:** LLMの学習データに評価データセットが部分的に、あるいは完全に含まれている場合、モデルは推論ではなく、単なる記憶によって高い性能を発揮してしまう可能性がありました。
*   **ベンチマークの妥当性:** 既存のベンチマークでは、高い性能を発揮するために推論が必ずしも必要条件とならず、モデルが記憶に頼って問題を解くことが可能でした。
*   **言語リソースの偏り:** 既存のベンチマークでは、高リソース言語に偏っている場合があり、LLMが言語知識を利用して推論を回避できる可能性がありました。
*   **部分的な評価:** 正誤判定が曖昧な問題に対する部分的な評価が不足しており、モデルが推論ではなく、単に文脈中の単語を繰り返すだけでも高いスコアを得られる可能性がありました。

## 2. どのようなアプローチでそれを解決しようとしたか

論文では、LLMの推論能力をより正確に評価するために、以下の3つの主要なアプローチを採用しました。

1.  **動的な難読化ベンチマークの生成:** 言語推論問題を、LLMの学習データに存在しないように、動的に難読化するフレームワークを開発しました。
2.  **正書法テンプレートの利用:** 実在の言語の書記体系を動的に難読化する正書法テンプレートを開発し、多数の問題バリエーションを生成しました。これらのバリエーションは、解法に必要な推論ステップを保持しつつ、特定の問題インスタンスがモデルの学習データに現れる可能性を低減しました。
3.  **LINGOLY-TOOベンチマークの作成:** 開発したフレームワークを適用し、言語推論のための挑戦的なベンチマークであるLINGOLY-TOOを開発しました。このベンチマークは、既存のLINGOLYを拡張したもので、UK Linguistics Olympiad (UKLO) の問題を使用しています。

**技術的なアプローチ:**

*   **正書法テンプレート:** 各問題に対して、問題の解決可能性に影響を与えないように、発音や音韻的な区別を保持するルールセットを作成しました。
*   **アノテーション:** 問題から言語名、言語ファミリー、地域情報などのメタデータを手動で削除し、特殊なタグでアノテーションしました。
*   **難読化:** 各問題の正書法テンプレートに基づいて、問題内の文字をランダムに置換し、問題のバリエーションを生成しました。

**疑似コード (難読化処理):**

```python
def obfuscate_problem(problem, orthographic_template):
  """
  問題を指定された正書法テンプレートで難読化する。

  Args:
    problem: 難読化する問題テキスト（文字列）。
    orthographic_template: 正書法テンプレート（辞書など）。

  Returns:
    難読化された問題テキスト（文字列）。
  """
  obfuscated_problem = problem
  for original_grapheme, replacement_graphemes in orthographic_template.items():
    # replacement_graphemes は、置換候補のリスト
    replacement_grapheme = random.choice(replacement_graphemes)
    obfuscated_problem = obfuscated_problem.replace(original_grapheme, replacement_grapheme)
  return obfuscated_problem
```

## 3. 結果、何が達成できたのか

実験結果から、以下のことが明らかになりました。

*   **モデルの苦戦:** Claude 3.7 Sonnet, o1-preview, DeepSeek R1を含む最先端のLLMは、高度な推論に苦戦しました。
*   **精度のばらつき:** LLMは、同じ問題の順列（permutation）間で精度に顕著なばらつきを示し、元の正書法で表示された問題の方が平均的に良い結果を出しました。
*   **事前データ露出の影響:** LLMの応答生成の不透明性を明らかにし、事前データ露出が最先端モデルの推論能力の過大評価に寄与している証拠を提供しました。
*   **難読化の有効性:** 難読化によって、モデルが問題を記憶によって解決することを防ぎ、推論能力をより正確に評価できることを示しました。

## 4. Limitationや問題点は何か

### 本文で言及されているもの

*   **正書法テンプレートの制約:** 正書法テンプレートの作成には専門知識が必要であり、自動化が難しい可能性があります。
*   **難読化による問題の難易度変化:** 難読化によって、問題の難易度がわずかに変化する可能性があり、モデルの性能低下が純粋な推論能力の低下とは限らない場合があります。
*   **評価指標の限界:** 正答率（exact match）は厳密すぎるため、部分的な正解を評価できない場合があります。
*   **計算コスト:** 大量の問題バリエーションを生成し、多数のLLMで評価するには、計算コストがかかります。

### その他

*   **言語の選択:** UKLOの問題に偏っているため、特定の言語や言語構造に特化した推論能力を評価できない可能性があります。
*   **参加者の言語知識:** RCTでの参加者の言語知識が完全に統制されていないため、難読化の効果を正確に評価できない可能性があります。
*   **ベンチマークの複雑さ:** LINGOLY-TOOは非常に複雑であり、LLMが推論に失敗した場合、その原因を特定することが難しい場合があります。
*   **実世界の応用との乖離:** LINGOLY-TOOは人工的な問題設定であり、実世界のタスクにおけるLLMの推論能力を反映していない可能性があります。
*   **倫理的な考慮:** 低リソース言語のデータを扱うため、言語コミュニティへの配慮が必要です。
*   **生成される問題の品質:** 難読化された問題の品質が担保されているかの検証が難しい場合があります。
*   **評価の偏り:** zero-shotでの評価なので、 few-shotなどの設定で結果が異なる可能性があります。

## 5. 技術的な詳細について

*   **正書法テンプレート:** 各問題に対して、文字列を grapheme として扱うかどうかの判断は専門家の知識に基づいており、曖昧さが残る場合があります。例: `ch` を1つの grapheme として扱うか、2つの grapheme として扱うか。
*   **難読化アルゴリズム:** 難読化アルゴリズムは、annotation されたテキストを左から右へスキャンし、最長一致する grapheme を置換します。この greedy なアプローチは、置換の順序によって結果が異なる可能性があります。
*   **評価方法:** 正答率（exact match）は、完全に正しい答えのみを正解とするため、モデルの微妙な違いを捉えることができません。Levenshtein距離などの編集距離を考慮した評価指標を使用することも考えられます。
*   **実験設定:** 実験では、prompt と system message を固定していますが、LLM によって最適な prompt が異なる可能性があります。

**疑似コード (難読化における grapheme 置換):**

```python
def replace_graphemes(text, grapheme_map):
    """
    テキスト中の grapheme を、指定されたマップに基づいて置換する。

    Args:
        text: 置換対象のテキスト (文字列)。
        grapheme_map: grapheme と置換文字列のマッピング (辞書)。

    Returns:
        置換後のテキスト (文字列)。
    """
    sorted_graphemes = sorted(grapheme_map.keys(), key=len, reverse=True) # 長い順にソート
    for grapheme in sorted_graphemes:
        if grapheme in text:
            text = text.replace(grapheme, grapheme_map[grapheme])
    return text
```

## 6. コストや物理的な詳細について

*   **モデル:** 実験では、Claude 3.7 Sonnet, Claude 3.5 Sonnet, Gemini 1.5 Pro, GPT-4.5, GPT-4o, o1-preview, o3-mini, DeepSeek R1, Phi-4, Llama 3.3 70B, Aya 23 35BなどのLLMを使用しました。
*   **ハードウェア:** オープンソースモデルの実験には、A100およびH100 GPUを使用しました。
*   **データセット:** UKLO過去問から標準化された82の問題を再利用・改変し、各問題に対して複数の難読化バリエーションを生成しました。小規模ベンチマークでは各問題あたり5つの難読化、大規模ベンチマークでは7つの難読化を生成しました。
*   **Fine-tuning:** LLama 3.2 1B と LLama 3.2 3B の fine-tuning を1 epoch 行いました。AdamW optimizer を使用し、learning rate は3Bモデルで `5e-5` に設定しました。
*   **詳細なコスト:** GPU時間、データセットサイズ、モデルサイズなどの具体的な数値は記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

*   **Bean, A. M., Hellsten, S., Mayne, H., Magomere, J., Chi, E. A., Chi, R. A., Hale, S. A., and Kirk, H. R. LINGOLY: A benchmark of olympiad-level linguistic reasoning puzzles in low resource and extinct languages.**：既存のLINGOLYベンチマークに関する論文であり、本研究の基盤となっています。
*   **Jacovi, A., Caciularu, A., Goldman, O., and Goldberg, Y. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks.**：データ汚染の問題に関する論文であり、本研究のモチベーションとなっています。
*   **Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models.**：Chain-of-thought prompting に関する論文であり、推論能力向上のための技術として言及されています。

## 8. この論文を140字以内のツイートで要約すると？

LLMの推論能力を正確に測るため、LINGOLY-TOOを開発！言語の難読化で記憶による不正解を防止。最先端モデルも苦戦。事前学習データの影響大！ #LLM #推論 #言語処理


---


# On the Acquisition of Shared Grammatical Representations in Bilingual Language Models

[View Paper](http://arxiv.org/abs/2503.03962v1)

## 1. 既存研究では何ができなかったのか

既存研究（特にcrosslingual transferに関する研究）では、multilingual language models（多言語言語モデル）がどのように多言語能力を獲得するかのメカニズムが十分に解明されていませんでした。具体的には、以下の点が不明確でした。

*   **第二言語の学習が第一言語で学習したモデルにどのような影響を与えるか:** モノリンガル言語モデルが第二言語で学習を開始した際に、内部表現がどのように変化するのか。
*   **言語データ量と学習順序の影響:** 各言語のデータ量や学習順序が、多言語表現の学習にどのように影響を与えるのか。
*   **構造プライミング現象の解釈:** 既存のcrosslingual構造プライミング研究結果を、言語データ量や学習順序を考慮した場合にどのように解釈すべきか。
*   **言語間の類型的な違いの影響:** 言語間の類似性が低い場合、crosslingual transfer learningや共有表現の学習にどのような限界があるのか。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、以下の実験的アプローチを用いて、上記の課題を解決しようと試みました。

1.  **小規模なバイリンガルモデルの構築:** データ量と学習順序を制御可能な小規模なバイリンガル言語モデルを構築し、学習状況を詳細に分析しました。
2.  **構造プライミングの利用:** 人間の文法表現の研究で用いられる構造プライミングという手法を応用し、モデルが獲得した文法表現に注目しました。構造プライミングとは、ある文構造を聞いた後に、同じ文構造の文を生成しやすくなる現象です。
3.  **言語データ量と学習順序の制御:** 各言語のデータ量や学習順序を変化させ、構造プライミング効果に与える影響を調べました。
4.  **言語ペアの選択:** 言語間の類似性が異なる言語ペアを選択し、crosslingual transfer learningの限界を検証しました。

疑似コードで表現すると、以下のようになります。

```python
# 1. モデルの初期化
model = MonolingualLanguageModel()

# 2. 言語データと学習順序の設定
languages = ["LanguageA", "LanguageB"]
data_amounts = {"LanguageA": amount_A, "LanguageB": amount_B}
training_order = ["LanguageA", "LanguageB"] # or ["LanguageB", "LanguageA"]

# 3. バイリンガル学習
for language in training_order:
  data = load_data(language, data_amounts[language])
  model.train(data, language)

# 4. 構造プライミング実験
prime_sentence = "Sentence in LanguageA with specific structure"
probe_sentence = "Sentence in LanguageB that can use the same structure"

prime_output = model.generate(prime_sentence) # モデルにprime_sentenceを入力
probe_output = model.generate(probe_sentence) # モデルにprobe_sentenceを入力

# 5. 構造プライミング効果の測定
priming_effect = calculate_similarity(prime_output, probe_output) # 生成された文の類似度を計算
```

## 3. 結果、何が達成できたのか

この研究により、以下の点が明らかになりました。

*   **非対称性の発見:** 言語ペアと方向によって、crosslingual構造プライミング効果に非対称性が存在することがわかりました。これは、言語間の影響が一方向的であることを示唆しています。
*   **データ量と学習順序の影響:** 学習データ量や学習順序が、構造プライミング効果に影響を与えることが示されました。
*   **言語間の類似性の影響:** 言語間の類似性が低いほど、構造プライミング効果が弱まることが示されました。これは、crosslingual transfer learningの限界を示唆しています。

これらの結果は、multilingual language modelsがどのように多言語表現を獲得するかの理解を深め、人間の構造プライミング効果に関する仮説を形成するのに役立ちます。

## 4. Limitationや問題点は何か

この研究には、いくつかの Limitation や問題点が存在します。

*   **小規模モデルの利用:** 実験に使用したモデルが小規模であるため、大規模な言語モデルに一般化できるかどうかは不明です。
*   **特定の言語ペア:** 実験に使用した言語ペアが限られているため、すべての言語ペアに適用できるかどうかは不明です。
*   **構造プライミングの限定的な適用:** 構造プライミングという特定の現象に焦点を当てているため、言語モデル全体の多言語能力を評価するには不十分です。
*   **abstractに書かれている以外のノイズ除去がされていない:** テキストがHTMLから抽出されたものであるため、完全な形で提示されていない可能性がある。

私が考える問題点としては、以下の点が挙げられます。

*   **評価方法の妥当性:** 構造プライミング効果の測定方法が、言語モデルの文法獲得能力を正確に反映しているかどうかは不明です。
*   **implicit bias:** モデルのアーキテクチャや学習方法に起因するバイアスが、実験結果に影響を与えている可能性があります。

## 5. 技術的な詳細について

この研究では、transformersベースの言語モデルが使用されていると考えられます。モデルのアーキテクチャとしては、encoder-decoderモデルまたはdecoder-onlyモデルが考えられます。

学習プロセスは、以下の手順で行われたと推測されます。

1.  **トークナイザの学習:** 各言語のコーパスを用いて、サブワードトークナイザ（例: Byte-Pair Encoding (BPE)）を学習します。
2.  **モデルの初期化:** ランダムな重みでモデルを初期化します。
3.  **教師あり学習:** 各言語のテキストデータを用いて、言語モデリングタスク（次の単語を予測するタスク）でモデルを学習します。損失関数としては、cross-entropy損失が用いられると考えられます。
4.  **ハイパーパラメータの調整:** 検証データを用いて、学習率、バッチサイズ、エポック数などのハイパーパラメータを調整します。

構造プライミング効果の測定には、以下の手法が用いられたと考えられます。

1.  **プライミング文の入力:** モデルにプライミング文を入力し、隠れ層の活性化ベクトルを取得します。
2.  **プローブ文の入力:** モデルにプローブ文を入力し、隠れ層の活性化ベクトルを取得します。
3.  **類似度の計算:** プライミング文とプローブ文の活性化ベクトルの類似度を計算します。類似度としては、コサイン類似度などが用いられると考えられます。
4.  **プライミング効果の評価:** プライミング文を入力した場合と入力しない場合で、プローブ文の生成確率または活性化ベクトルの類似度が有意に異なるかどうかを統計的に検定します。

## 6. コストや物理的な詳細について

論文に具体的な記述がないため、コストや物理的な詳細を正確に推定することはできません。しかし、小規模なモデルを使用していることから、以下の推測ができます。

*   **GPU:** NVIDIA GeForce RTX 3090などの高性能GPU数枚
*   **学習時間:** 数時間から数日程度
*   **データセット:** 数百万文程度のテキストデータ
*   **モデルサイズ:** 数千万から数億パラメータ程度

大規模な言語モデルに比べれば、計算資源や時間的なコストは大幅に低いと考えられます。

## 7. 参考文献のうち、特に参照すべきもの

abstractに"We first replicate previous crosslingual structural priming results"とあるので、crosslingual structural primingに関する先行研究を調べるべきです。具体的には、この論文の結果と比較されている先行研究を調査することで、この論文の貢献をより深く理解できます。

## 8. この論文を140字以内のツイートで要約すると？

バイリンガル言語モデルの学習メカニズムを構造プライミングで解明。データ量や順序で効果が非対称に。言語間の類似性が低いと効果減。多言語学習の限界を示唆。 #自然言語処理 #多言語モデル #構造プライミング


---


# Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems

[View Paper](http://arxiv.org/abs/2503.01375v1)

## 1. 既存研究では何ができなかったのか

既存研究におけるBayesian Inverse Problem (BIP) の解決には、主に以下の課題がありました。

*   **計算コストの高さ**: MCMC のような古典的な手法は、posterior distribution からのサンプリングに多くのforward modelの評価を必要とし、計算時間が膨大になります。Variational inferenceも高次元設定ではposterior distributionの近似が難しく、計算コストが課題でした。
*   **観測数の固定**: 多くの既存手法は、観測点の数が固定されていることを前提としていました。現実の問題では観測点の数が変動することが多く、柔軟性に欠けます。例えば、医学におけるインバース問題では、観測点の数が変化することがあります。
*   **事後分布の明示的な生成**: 既存の生成モデルを用いたアプローチでは、多くの場合、事後分布を明示的に生成しませんでした。そのため、反復的な処理が必要となり、効率的なサンプリングが困難でした。
*   **個別のloss関数の設計**: physics-informed neural networks (PINN) を用いる場合、PDEごとにカスタムloss関数を設計する必要があり、汎用性に欠けました。
*   **大規模な物理システムへの適用**: Normalizing Flow を variational inference に組み込む手法は、理論的な保証はあるものの、モデルの複雑さが増すと計算コストが大きくなり、大規模な物理システムへの適用が困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、Conditional Flow Matching (CFM) と Transformer アーキテクチャを組み合わせることで、上記の課題を解決しようとしました。具体的なアプローチは以下の通りです。

*   **Conditional Flow Matching (CFM) の利用**: CFMは、確率分布間の連続的な変換を学習する生成モデルであり、forward model の評価回数を削減し、効率的なサンプリングを実現します。
*   **Transformer アーキテクチャの導入**: Transformer は、可変長の入力に対応できるため、観測点の数が変動する場合でも柔軟に対応できます。また、Attention機構により、入力データ間の依存関係を捉え、高精度な推論を可能にします。
*   **可変長入力への対応**: 観測点の数が異なるデータセットをバッチ処理する際に、2つの異なる戦略を提案しています。
    *   各バッチの勾配を計算し、固定数のバッチ後に更新する
    *   異なる数のポイントを持つバッチ間で勾配を蓄積し、単一の最適化ステップを実行する
*   **Bayesian Inverse Problem の定式化**: Bayesian Inverse Problem を、サンプルから条件付き確率分布を学習する問題として定式化し、CFM を適用しやすい形にしました。具体的には、forward model と prior distribution から容易にサンプルを生成できるようにしました。
*   **損失関数の最適化**: Velocity field を学習するための損失関数を定義し、確率分布間の最適な輸送経路を学習します。損失関数は、以下のPython風の疑似コードで表されます。

```python
def loss(v_theta, u_t):
  """
  Calculate the loss between the predicted velocity field (v_theta)
  and the ideal velocity field (u_t).
  """
  return np.mean((v_theta - u_t)**2)  # Mean Squared Error
```

## 3. 結果、何が達成できたのか

本研究の結果、以下の点が達成されました。

*   **効率的なサンプリング**: CFM と Transformer の組み合わせにより、MCMC などの古典的な手法と比較して、大幅に高速なサンプリングを実現しました。SEIRモデルに対して、CFM transformerは約0.22秒で推論可能です。
*   **可変長入力への対応**: Transformer アーキテクチャにより、観測点の数が変動する場合でも、柔軟に対応できることを示しました。
*   **高精度な推論**: 複数の数値実験において、提案手法が高精度なパラメータ推定を実現することを示しました。
*   **非自明なパスの学習**: Flow Matching が、prior distribution から target distribution への非自明なパスを効率的に学習できることを示しました。実験結果から、パスがほぼ直線的であり、最適な輸送 (optimal transport) が実現されていることが示唆されました。

## 4. Limitationや問題点は何か

本研究には、以下の Limitation および問題点が考えられます。

*   **高次元問題へのスケーリング**: 本研究では、比較的小規模な問題を取り扱っています。高次元の Bayesian Inverse Problem への適用には、アーキテクチャの変更やデータセットの追加スケーリングが必要となる可能性があります。
*   **対数尤度の評価**: Bayesian optimal experiment design には、対数尤度の評価が必要となりますが、CFM が学習する分布からのサンプリングだけでなく、対数尤度の評価の複雑さと精度を検討する必要があります。
*   **forward model によるガイダンス**: forward model を考慮した CFM の目的関数の導入、および、推定されたパラメータが観測データに適合するかどうかをチェックし、可能なエラーを修正する推論手順の改善が必要です。
*   **学習された分布の特性**: 学習された分布の実際の特性の研究が不十分です。特に、モデルパラメータが観測データによって一意に定義される場合、velocity field が効果的に回帰問題を解いている可能性があります。
*   **CFMにおけるノイズの重要性**: CFMにおけるノイズに関するランダム化の有用性と緊急性については、さらなる研究が必要です。
*   **MCMC との分布の差異**: MCMC と CFM によって得られる分布に差異が見られる場合があり、詳細な検討が必要です。
*   **計算コスト**: 論文中では、MCMC と比較して計算コストが低いと述べられていますが、Transformer モデルのトレーニングには計算資源が必要です。

## 5. 技術的な詳細について

本研究では、Conditional Flow Matching (CFM) と Transformer アーキテクチャを組み合わせた新しい手法を提案しています。以下に、技術的な詳細を解説します。

*   **Conditional Flow Matching (CFM)**: CFM は、2つの確率分布間の連続的な変換を学習する生成モデルです。本研究では、prior distribution (p0) から target distribution (p1) への変換を学習します。
    *   **学習**: CFM は、velocity field (v_θ(t, x, e, d)) を学習することで、確率分布間の連続的なパスを定義します。velocity field は、時刻 t、状態 x、実験条件 e、観測データ d を入力とし、状態の変化を表すベクトルを出力します。
    *   **損失関数**: velocity field を学習するための損失関数は、以下のようになります。

        ```python
        def loss(v_theta, u_t):
            """
            Calculate the loss between the predicted velocity field (v_theta)
            and the ideal velocity field (u_t).
            """
            return np.mean((v_theta - u_t)**2)  # Mean Squared Error
        ```

        ここで、v_theta は学習された velocity field の出力、u_t は理想的な velocity field を表します。理想的な velocity field は、prior distribution から target distribution への最短経路 (optimal transport) を実現するものです。
    *   **サンプリング**: 学習された velocity field を用いて、ODE solver により target distribution からのサンプリングを行います。具体的には、以下の ODE を解きます。

        ```
        dx/dt = v_θ(t, x, e, d)
        ```

        初期状態 x(t=0) は prior distribution からサンプリングされます。t=1 における状態 x(t=1) が、target distribution からのサンプルとなります。
*   **Transformer アーキテクチャ**: 本研究では、velocity field をパラメータ化するために、Transformer アーキテクチャを採用しています。Transformer は、可変長の入力に対応でき、Attention 機構により、入力データ間の依存関係を捉えることができます。
    *   **入力**: Transformer への入力は、時刻 t、状態 x、実験条件 e、観測データ d です。
    *   **Embedding**: 入力データは、Linear Projection により embedding 空間に変換されます。時刻 t は、Timestep Embedder によりエンコードされます。
    *   **Attention**: Transformer の Self-Attention 機構により、入力データ間の依存関係を学習します。Rotary Position Embeddings (RoPE) を用いることで、相対的な位置情報を学習し、学習時よりも長いシーケンスへの汎化を可能にします。
    *   **出力**: Transformer の出力は、velocity field v_θ(t, x, e, d) です。
*   **可変長入力への対応**:
    *   異なる数の観測点を持つバッチに対して、勾配を計算し、固定数のバッチ後に更新する
    *   異なる数の観測点を持つバッチ間で勾配を蓄積し、単一の最適化ステップを実行する

## 6. コストや物理的な詳細について

論文中に具体的なコストや物理的な詳細（トレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど）に関する記述はありません。したがって、推定に基づく情報となります。

*   **データセット**: SEIRモデルや Darcy Flow などの数値実験で使用されるデータセットは、forward model と prior distribution から生成されます。データセットのサイズは、実験設定によって異なります。
*   **モデルサイズ**: Transformer モデルのパラメータ数は、アーキテクチャによって異なります。本研究で使用されている Transformer モデルの具体的なパラメータ数は不明です。
*   **トレーニング**: Transformer モデルのトレーニングには、GPU を用いた並列計算が有効です。トレーニング時間およびGPUの数は、モデルサイズ、データセットサイズ、および計算資源によって大きく異なります。
*   **推論**: CFM Transformer を用いた推論は、高速に実行できます。SEIRモデルに対しては約0.22秒、Permeability inversion problem に対しては、アルゴリズム 1 を使用した CPU 上で 1 推論あたり 1.08 秒という結果が出ています。

これらの情報は、論文中に明記されていないため、あくまで一般的な推定です。

## 7. 参考文献のうち、特に参照すべきもの

*   **Lipman et al., 2023. Flow matching for generative modeling**: CFM の基本的な概念を理解するために重要です。
*   **Vaswani et al., 2017. Attention is all you need**: Transformer アーキテクチャの詳細について理解するために重要です。
*   **Raissi et al., 2019. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations**: 既存の Deep Learning 手法との比較の文脈で有用です。

## 8. この論文を140字以内のツイートで要約すると？

CFMとTransformerでベイズインバース問題を効率的に解く！可変長観測に対応し、MCMCより高速。医療や流体解析等への応用も！ #CFM #Transformer #ベイズ推定


---


# Understanding and Predicting Derailment in Toxic Conversations on GitHub

[View Paper](http://arxiv.org/abs/2503.02191v1)

## 1. 既存研究では何ができなかったのか

既存研究の限界については、本論文のAbstractに直接的な記述がないため、明示的に言及されていません。ただし、以下の点から既存研究の課題を推測できます。

*   **毒性会話の発生予防の難しさ:** 毒性のある言語や否定的なやり取りは、貢献者の参加や定着を妨げ、新規参入者を遠ざける可能性があります。既存研究では、毒性がエスカレートする前に会話の脱線を検知し、予防的な対策を講じることの難しさが課題として存在すると考えられます。
*   **脱線点の特定と分析の不足:** 毒性会話における脱線点が具体的にどのような特徴を持つのか、既存研究では十分に解明されていなかった可能性があります。
*   **会話の進化の把握:** 会話がどのように展開し、脱線につながる兆候を早期に捉えるための効果的な手法が確立されていなかった可能性があります。

これらの課題を踏まえ、本研究はGitHubにおける毒性会話の脱線を理解し、予測することを目指しています。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、以下のステップで毒性会話の脱線予測に取り組んでいます。

1.  **データセットの構築:** GitHubから202件の毒性会話と696件の非毒性会話からなるデータセットを構築しました。毒性会話には、脱線点がアノテーションされています。
2.  **特徴の分析:** データセットを用いて、毒性会話と脱線点のユニークな特徴を特定しました。具体的には、二人称代名詞、否定語、Bitter FrustrationやImpatienceといった口調などの言語的特徴、プロジェクト貢献者と外部参加者間の会話のダイナミクスのパターンなどを分析しました。
3.  **会話軌跡要約技術の開発:** 大規模言語モデル（LLM）を活用し、会話の進化を捉え、脱線の兆候を早期に特定するための会話軌跡要約技術を開発しました。
4.  **脱線予測モデルの構築と評価:** LLMを用いたプロンプトエンジニアリングにより、GitHubの会話の要約を提供することで、会話の脱線を予測するモデルを構築し、その性能を評価しました。ベースラインモデルと比較し、LLMを用いたアプローチの有効性を検証しました。

## 3. 結果、何が達成できたのか

本研究によって、以下の成果が達成されました。

*   **GitHubにおける毒性会話の脱線予測:** LLMを活用した会話軌跡要約技術により、会話の脱線を69%のF1スコアで予測できることを示しました。これは、ベースラインモデルを大幅に上回る性能です。
*   **毒性会話と脱線点の特性の解明:** 毒性会話と脱線点に特有の言語的特徴や会話ダイナミクスのパターンを特定しました。これは、毒性会話の理解を深める上で重要な知見です。
*   **予防的モデレーション戦略の提案:** 毒性会話がエスカレートする前に、自動的に検出し、対処するための予防的モデレーションアプローチを提案しました。

## 4. Limitationや問題点は何か

本研究の限界と問題点として、以下の点が挙げられます。

*   **データセットの偏り:** GitHubから収集されたデータセットは、特定のプロジェクトやコミュニティに偏っている可能性があります。そのため、他のプラットフォームやコミュニティにおける毒性会話にも同様の結果が適用できるとは限りません。
*   **アノテーションの主観性:** 毒性会話の判断や脱線点のアノテーションは、主観的な判断に依存する部分があります。アノテーションの客観性を高めるための工夫（複数人でのアノテーションなど）が必要となる可能性があります。
*   **LLMの限界:** LLMは、学習データに偏りがある場合や、文脈を理解することが難しい場合に、不適切な判断をする可能性があります。特に、毒性会話の判断は、微妙なニュアンスや隠喩を含む場合があり、LLMの判断が難しい場合があります。
*   **言語依存性:** 今回の分析は英語の会話に限定されている可能性があり、他の言語での毒性会話にそのまま適用できるとは限りません。多言語対応のためには、追加のデータ収集とモデルの調整が必要となるでしょう。
*   **実用化における課題:** 提案されたモデレーションアプローチを実用化するためには、誤検出率の低減や、ユーザーへの適切な通知方法など、多くの課題を解決する必要があります。

## 5. 技術的な詳細について

本研究における技術的な詳細について解説します。

*   **データセット:** GitHubの会話データから構築されています。
    *   毒性会話: 202件 (脱線点のアノテーションあり)
    *   非毒性会話: 696件
*   **特徴量エンジニアリング:**
    *   言語的特徴: 二人称代名詞、否定語、感情分析（Bitter Frustration、Impatienceなど）
    *   会話ダイナミクス: プロジェクト貢献者と外部参加者のインタラクションパターン
*   **モデル:** 大規模言語モデル (LLM) を活用
    *   具体的なLLMの種類は不明 (Abstractに記載なし)
*   **アプローチ:** 会話軌跡要約技術
    *   LLMにGitHubの会話を要約させるプロンプトを設計
    *   要約結果を用いて、会話の脱線を予測
*   **疑似コード:** 会話軌跡要約と脱線予測のプロセスを疑似コードで表現すると以下のようになります。

```python
def predict_derailment(conversation_history, llm_model):
  """
  会話履歴から脱線を予測する。

  Args:
    conversation_history: 会話履歴のリスト。各要素は会話のターンを表す文字列。
    llm_model: 大規模言語モデルのインスタンス。

  Returns:
    脱線の確率 (0から1の範囲の数値)。
  """

  # 1. 会話履歴をLLMに入力し、要約を生成するプロンプトを作成
  prompt = f"以下のGitHubの会話を要約してください:\n{conversation_history}\n要約:"

  # 2. LLMを用いて会話の要約を生成
  conversation_summary = llm_model.generate_text(prompt)

  # 3. 要約に基づいて、脱線の可能性を評価するプロンプトを作成
  derailment_evaluation_prompt = f"以下の会話の要約に基づいて、この会話が脱線する可能性はどれくらいですか？ (0から1で評価):\n{conversation_summary}\n脱線の可能性 (0-1):"

  # 4. LLMを用いて脱線の可能性を評価
  derailment_probability = float(llm_model.generate_text(derailment_evaluation_prompt))

  return derailment_probability
```

*   **評価指標:** F1スコア (69%)

## 6. コストや物理的な詳細について

論文中にコストや物理的な詳細に関する記述は存在しません。トレーニングに使用したGPUの数や時間、データセットの具体的なサイズ、モデルのサイズなどは不明です。

## 7. 参考文献のうち、特に参照すべきもの

論文自体がまだプレプリントであり、参考文献リストがないため、現時点では特定できません。ただし、大規模言語モデル(LLM)を用いたテキスト要約、毒性検知、会話分析に関する既存研究は、この論文の内容を理解する上で役立つと考えられます。

## 8. この論文を140字以内のツイートで要約すると？

GitHubの毒性会話、脱線予測にLLM活用！会話履歴を要約し、F1スコア69%で脱線を検知。発言の偏りや主観性など課題も。毒性対策の糸口となるか？ #自然言語処理 #毒性検知 #LLM


---


# IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval

[View Paper](http://arxiv.org/abs/2503.04644v1)

## 1. 既存研究では何ができなかったのか

既存の instruction-following 情報検索 (IR) ベンチマークには、以下の問題点がありました。

*   **複雑な instruction の欠如:** 既存のベンチマークは、多くの場合、単純な文による instruction を使用しており、実際の専門分野における複雑なユーザー要求を十分に反映していませんでした。
*   **専門分野の評価不足:** 既存の研究は、一般的なドメインに焦点を当てており、金融、法律、医療、科学文献などの専門分野における instruction-following retrieval の評価が不足していました。
*   **明示的な複雑性レベルの欠如:** 複雑な instruction を含んだ研究でも、instruction の複雑性レベルを明示的に定義していなかったため、retriever の性能を詳細に評価することができませんでした。
*   **複数 relevant passage の考慮不足:** 一部のベンチマークでは、各クエリに対して単一の relevant passage のみを考慮しており、複数の関連 passage が存在する現実世界のシナリオを反映していませんでした。
*   **instruction-specific な評価方法の欠如:** 従来の評価指標では、モデルが instruction に従う能力を十分に評価できませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の課題を解決するために、以下の要素を含む新しいベンチマーク IFIR を提案しました。

*   **専門分野の instruction-following IR ベンチマーク:** 金融、法律、医療、科学文献の4つの専門分野をカバーする、包括的なベンチマークを構築しました。
*   **複雑性レベルの定義:** 各ドメインにおいて、instruction の複雑さを3つのレベルに分類し、より粒度の細かい評価を可能にしました。
*   **大規模なデータセット:** 2,426 件の instruction-following クエリを含むデータセットを構築し、各クエリに対して平均 6.14 件の ground-truth passage を用意しました。
*   **高品質なデータ:** ドメイン専門家による検証を行い、instruction の文脈的関連性と現実世界の課題との整合性を確保しました。
*   **新しい評価指標:** LLM ベースの新しい評価指標 `InstFol` を導入し、retriever が instruction に従う能力をより正確に評価できるようにしました。`InstFol` は、instruction を含むクエリと含まないクエリに対する検索結果を LLM で評価し、その改善度を測ることで instruction-following 能力を評価します。

    ```python
    def calculate_instfol_at_k(S_inst, S_q, alpha):
        """
        InstFol@K スコアを計算する

        Args:
            S_inst: instruction を含むクエリで検索された passage の LLM 評価スコア
            S_q: instruction を含まないクエリで検索された passage の LLM 評価スコア
            alpha: 正規化関数 (1 / (3 - S_q))

        Returns:
            InstFol@K スコア
        """
        instfol_score = (S_inst - S_q) * alpha
        return instfol_score
    ```

## 3. 結果、何が達成できたのか

本研究によって、以下の成果が得られました。

*   **IFIR ベンチマークの提供:** 専門分野における instruction-following IR の評価を可能にする、高品質なベンチマーク IFIR を公開しました。
*   **既存モデルの課題の明確化:** 15 の最先端 retriever を用いた実験により、現在のモデルが複雑なドメイン固有の instruction を効果的に理解し、処理することに苦労していることを明らかにしました。
*   **LLM ベース retriever の可能性の示唆:** LLM ベースの retriever が、より複雑な検索タスクを管理する上で、より堅牢な性能を示すことを示しました。
*   **新しい評価指標の有効性:** 提案した `InstFol` メトリックが、人間の専門家による評価と高い相関を示すことを確認し、instruction-following 能力の評価に有効であることを示しました。
*   **今後の開発に向けた洞察の提供:** 実験結果の詳細な分析を通じて、今後の retriever 開発の方向性を示すための貴重な洞察を提供しました。例えば、lexical search が特定のドメインにおいて複雑な instruction の補助ツールとして機能する可能性や、instruction に特化した学習方法の改善の必要性などが示唆されました。

## 4. Limitationや問題点は何か

本研究には、以下の limitation および問題点が存在します。

*   **データセットの規模:** データセットに含まれるクエリの数が限られていること、およびモデルの訓練に使用できるトレーニングデータセットが提供されていないこと。
*   **ドメイン固有モデルとの比較:** BioBERT などのドメイン固有 retriever と、汎用 retriever との性能比較を行っていないこと。ドメイン知識が重要な医療や法律などの分野では、ドメイン固有のモデルがより良い結果を達成する可能性があります。
*   **関連性判断のギャップ:** シードデータセットにおける関連性判断のギャップが存在する可能性。以前にアノテーションされた passage に基づいて関連 passage を選択しているため、関連する passage が無視される可能性があります。
*   **長い instruction への対応:** 長い instruction (1024 トークンを超える) を含むクエリが存在し、現在の retriever は通常、最大トークン長が 512 で訓練されているため、これらの長い instruction を完全に処理できません。
*   **専門知識の密度:** 特に科学文献や医療分野では、専門知識が高度に凝縮された instruction が存在し、一般的な訓練データでは専門分野で必要な専門知識をすべて網羅できない場合があります。
*   **高度にカスタマイズされた instruction:** 金融や医療分野では、ユーザーや医師が複数の優先順位の高い目標やニーズを持っている instruction が存在し、従来の retriever では認識されない可能性があります。

## 5. 技術的な詳細について

本研究では、以下の技術的な詳細が用いられています。

*   **データセット構築:**
    *   既存の専門分野 IR ベンチマークのクエリを拡張し、詳細な instruction を組み込むことで、IFIR データセットを構築しました。
    *   instruction の生成には GPT-4o を使用し、ドメイン専門家がレビューおよび改善を行いました。
    *   instruction の複雑さは、各ドメインで3つのレベルに分類されました。
*   **評価指標:**
    *   nDCG (normalized Discounted Cumulative Gain) を使用して、retriever の検索性能を評価しました。
    *   LLM ベースの新しい評価指標 `InstFol` を導入し、retriever が instruction に従う能力を評価しました。`InstFol` は、instruction の有無による検索結果の改善度を LLM で評価します。
*   **モデル:**
    *   BM25、ColBERT、Contriever、GTR などの、instruction-tuning されていない一般的な retriever を評価しました。
    *   Instructor-xl、E5-mistral-7b-instruct、GritLM-7B、Promptriever などの、instruction-tuning された retriever を評価しました。
    *   OpenAI の Text-Embedding-v3-Large および Text-Embedding-v3-Small などの、LLM ベースの retriever を評価しました。
*   **実装:**
    *   長い passage に対応するため、512 トークンのスライディングウィンドウと 128 トークンのオーバーラップを使用し、mean pooling を適用して embedding を生成しました。ただし、LLM ベースの retriever では、長いコンテキストウィンドウを利用できるため、mean pooling は使用しませんでした。
    *   ハードウェア制約のため、LLM ベースの retriever は FP16 モードで実行し、GPU メモリの使用量を削減しました。
    *   LangChain と Elasticsearch を使用して、passage の大規模な embedding と検索を効率化しました。

## 6. コストや物理的な詳細について

論文中に明示的な記述はありませんが、以下の点が推測できます。

*   **GPU:** LLM ベースの retriever を実行するために、複数の GPU が必要です。特に、大規模な LLM を扱う場合は、メモリ容量の大きな GPU が必要になります。
*   **学習データセット:** 明示的な記述はありませんが、instruction-tuning された retriever は、MEDI などの既存のデータセットでファインチューニングされていると考えられます。また、Promptriever は、インスタンスレベルの instruction でターゲットを絞った訓練を行っていることが述べられています。
*   **モデルサイズ:** 評価対象のモデルサイズは、110M から 7B パラメータと記載されています。
*   **計算コスト:** LLM ベースの評価指標 `InstFol` を使用するため、LLM API の使用コストが発生します。特に、データセット規模が大きい場合は、コストが大きくなる可能性があります。
* **その他** データセット構築においては、ドメインエキスパートへのアノテーション費用が発生します。

## 7. 参考文献のうち、特に参照すべきもの

*   **Ouyang et al., 2022:** Training language models to follow instructions with human feedback
    *   instruction-following の研究における重要な論文であり、本研究のモチベーションとなっています。
*   **Thakur et al., 2021:** BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models
    *   既存の IR ベンチマークの代表例であり、本研究で提案する IFIR との比較対象となります。
*   **Liu et al., 2023:** G-eval: Nlg evaluation using gpt-4 with better human alignment
    *   LLM を用いた評価手法に関する研究であり、本研究で提案する LLM ベースの評価指標 `InstFol` の基礎となっています。
*   **Su et al., 2023a, 2023b:** One embedder, any task: Instruction-finetuned text embeddings
    *   instruction-tuning された text embedding に関する研究であり、本研究で評価対象としたモデルの一つである Instructor-xl の基礎となっています。
*   **Weller et al., 2024b:** Promptriever: Instruction-trained retrievers can be prompted like language models.
     *  Promptriever に関する研究であり、本研究で評価対象としたモデルの一つです。

## 8. この論文を140字以内のツイートで要約すると？

専門分野の #instructionFollowing 情報検索ベンチマーク #IFIR を発表！金融/法律/医療/科学文献を網羅。既存モデルは複雑な指示に苦戦。LLM基盤モデルが有望！LLM評価指標 #InstFol も提案。 #情報検索 #自然言語処理


---


# PokéChamp: an Expert-level Minimax Language Agent

[View Paper](http://arxiv.org/abs/2503.04094v1)

## 1. 既存研究では何ができなかったのか

既存研究では、以下の点が課題でした。

*   **タスク特化型学習の限界:** 従来の強化学習は、Chess, Go, Poker などのゲームにおいて、模倣学習や自己対戦を通じて優れた性能を発揮しましたが、タスク固有の訓練とエンジニアリングに多大な労力を要しました。
*   **言語エージェントの計画能力の限界:** 大規模言語モデル（LLM）は汎用的なエージェントとして有望ですが、テキストベースの言語エージェントは計画能力に限界があり、ハードコードされたヒューリスティックなボットに劣る場合や、基本的なゲームの仕組みを理解できない場合がありました。
*   **複雑なゲーム環境への適応:** ポケモンバトルは、1000種類以上のポケモン、それぞれ異なる能力、技、タイプ、ステータスを持ち、状態空間が膨大（10^354 オーダー）であるため、従来のAIシステムにとって大きな課題でした。部分的な観測可能性も存在し、対戦を通じて相手のチームの情報が徐々に明らかになるため、探索空間が広大になり、計算コストが膨大でした。
*   **ゲーム理論的戦略の欠如:** LLMは多様なデータセットで学習されていますが、ゲーム理論的戦略（例えば、相手の行動を予測し、それに対する最適な行動を選択する）を効果的に活用することが困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

PokéChampは、これらの課題を解決するために、以下の要素を取り入れた新しいフレームワークを導入しました。

*   **LLMによるMinimax探索の強化:** LLMの汎用的な能力を活用して、Minimax探索を強化しました。具体的には、Minimax探索の3つの主要モジュールをLLMで置き換えました。
    1.  **プレイヤーの行動サンプリング:** LLMに現在の状態に関する情報を提供し、実行可能な行動のサンプルを生成させ、探索木のサイズを削減しました。
    2.  **対戦相手のモデリング:** LLMに対戦相手の行動を予測させました。部分的な観測可能性に対処するため、LLMはプレイヤーの観測に基づいて隠れた状態を推測する必要がありました。
    3.  **価値関数の推定:** LLMを価値関数として使用し、探索木をゲーム終了まで展開する代わりに、一定の深さで探索を停止し、LLMにその時点での状態の価値を評価させました。
*   **追加のLLM学習は不要:** LLMの事前学習された知識と、ゲーム理論的な計画アルゴリズムとの統合に依存し、追加のLLMトレーニングは行いませんでした。
*   **世界モデルの開発:** ゲームの遷移を近似し、部分的な観測可能性と複雑なゲームの仕組みの課題に対処するための世界モデルを開発しました。この世界モデルは、コアなゲームの力学を数学的に計算するワンステップ先読みを取り入れ、対戦相手のチームの統計を推定するために実際のプレイヤーのゲームからの履歴データを利用しました。
*   **大規模なポケモンバトルデータセットの構築:** さまざまなスキルレベルとゲームモードにわたる300万件以上のゲームを含むポケモンバトルデータセットを構築し、対戦相手のモデリングと戦略開発のための豊富な情報源を提供しました。
*   **行動候補の追加:** LLMが生成した行動に加えて、ワンステップ先読みによる最適な行動と、既存のAbyssalボットからの最適な交代行動を候補に追加しました。

## 3. 結果、何が達成できたのか

PokéChampは、以下の点で優れた成果を達成しました。

*   **既存のボットに対する優れたパフォーマンス:** GPT-4oを搭載した場合、既存の最高のLLMベースのボットに対して76％、最強のルールベースのボットに対して84％の勝率を達成しました。
*   **オープンソースモデルでの優れたパフォーマンス:** 80億パラメータのLlama 3.1モデルを使用した場合でも、GPT-4oを搭載した既存の最高のLLMベースのボットであるPokéllmonを64％の勝率で一貫して上回りました。
*   **人間のプレイヤーに対する競争力:** ポケモンShowdownオンラインラダーで1300〜1500のEloを達成し、上位30％〜10％の人間のプレイヤーに匹敵するレベルに到達しました。
*   **大規模なデータセットとベンチマークの提供:** 300万件以上のゲーム（50万件以上の高Eloマッチを含む）を特徴とする最大規模の実プレイヤーポケモンバトルデータセットを構築しました。また、特定のバトルスキルを評価するためのバトルベンチマークとパズルを確立しました。
*   **ゲームエンジンのアップデート:** ローカルゲームエンジンの重要なアップデートを提供しました。

## 4. Limitationや問題点は何か

PokéChampには、以下の制限事項と問題点があります。

*   **対戦相手のモデリングの精度:**  対戦相手の行動を正確に予測することは依然として困難であり、特に相手の戦略が頻繁に変化する場合や、隠された情報が多い場合には課題が残ります。論文内でも、対戦相手の行動予測の精度は、プレイヤー側の予測精度と比較して低いことが示されています (Table 1)。
*   **計算リソースの制約:** リアルタイムのゲームプレイでは、計算時間に制約があるため、Minimax探索の深さと幅を制限する必要がありました。これにより、長期的な戦略や複雑な状況を完全に評価することが困難になる可能性があります。
*   **特定の戦略への弱点:**  論文内では、ステール戦術（耐久戦術）と過剰な交代戦略に対して苦戦することが言及されています。これは、探索の深さが限られていることと、これらの戦略を正確にモデル化することが困難であることに起因します。また、相手がPokéChampの戦略を理解している場合、その弱点（例えば、過剰な交代）を悪用する可能性があります。
*   **メタゲームの変化への対応:** LLMの事前学習データには固定されたカットオフ日があるため、現在のメタゲームに関する最新の統計情報が提供されても、LLMの意思決定がバイアスされる可能性があります。
*   **匿名性の必要性:** ライブデモでは、対戦相手がPokéChampと対戦していることを知っている場合、その弱点を利用した戦略を立てることができました。したがって、正確なパフォーマンス分析には匿名性が必要です。
*   **評価の偏り:**  オンラインラダーでの評価は、対戦相手の分布やメタゲームの変化に影響される可能性があり、真のパフォーマンスを正確に反映していない可能性があります。
*   **ルール外の行動への対応:** トレーニングデータに含まれていない行動（チート行為やバグの利用など）への対応は考慮されていません。

## 5. 技術的な詳細について

PokéChampは、Minimax探索アルゴリズムをLLMで強化したものです。以下に、技術的な詳細を解説します。

1.  **Minimax探索の基本:**
    *   現在の状態 (observation) を元に、可能な行動 (action) を列挙します。
    *   それぞれの行動に対して、対戦相手が最適な行動を取ると仮定し、その結果を評価します。
    *   自身の利益を最大化し、かつ対戦相手の利益を最小化する行動を選択します。
    *   この処理を探索深度 (search depth) が深くなるまで繰り返します。

    ```python
    def minimax(state, depth, maximizing_player):
        if depth == 0 or is_terminal(state):
            return evaluate(state) # 価値関数による評価

        if maximizing_player:
            max_eval = -float('inf')
            for action in get_possible_actions(state):
                new_state = apply_action(state, action)
                eval = minimax(new_state, depth - 1, False)
                max_eval = max(max_eval, eval)
            return max_eval
        else: # minimizing player
            min_eval = float('inf')
            for action in get_possible_actions(state):
                new_state = apply_action(state, action)
                eval = minimax(new_state, depth - 1, True)
                min_eval = min(min_eval, eval)
            return min_eval

    def get_best_action(state):
        best_action = None
        max_eval = -float('inf')
        for action in get_possible_actions(state):
            new_state = apply_action(state, action)
            eval = minimax(new_state, depth, False)
            if eval > max_eval:
                max_eval = eval
                best_action = action
        return best_action
    ```

2.  **LLMによるMinimax探索の拡張:**

    *   **行動サンプリング (Action Sampling):** LLMにプロンプトを入力し、実行可能な行動の候補を生成させます。プロンプトには、現在の状態、プレイヤーのチーム、アイテム、可視化された相手のポケモンなどの情報が含まれます。また、ワンステップ先読みを用いて、相手のポケモンを倒すために必要な最小ターン数を計算し、候補行動を絞り込みます。
    *   **対戦相手のモデリング (Opponent Modeling):** LLMに対戦相手の行動を予測させます。部分的な観測可能性に対処するために、過去のデータ分析とLLMによる予測を組み合わせます。対戦相手のチーム構成、過去の行動履歴などを考慮して、最も可能性の高い行動を予測します。
    *   **価値関数の推定 (Value Function Estimation):** 探索木のリーフノードにおいて、LLMに現在の状態の価値を評価させます。LLMは、現在の行動の有効性、残りのポケモンの数、勝利の確率などの基準に基づいてスコアを生成します。

    ```python
    def get_possible_actions(state):
        # LLMにプロンプトを入力し、行動の候補を生成
        llm_actions = generate_actions_with_llm(state)

        # ワンステップ先読みによる行動候補
        one_step_action = get_one_step_lookahead_action(state)

        # Abyssalボットからの行動候補
        abyssal_action = get_abyssal_bot_action(state)

        # 行動候補を統合
        actions = llm_actions + [one_step_action, abyssal_action]
        return actions

    def generate_actions_with_llm(state):
        prompt = create_prompt(state)
        actions = llm.generate(prompt) # LLMによる行動生成
        return actions

    def evaluate(state):
        prompt = create_evaluation_prompt(state)
        value = llm.generate(prompt) # LLMによる状態価値の評価
        return value
    ```

3.  **状態遷移のシミュレーション:**

    *   状態遷移関数 (P(s' | s, a, b)) を近似するために、ローカルのShowdownシミュレーターを使用します。
    *   ポケモンの技の命中率など、確率的な要素を考慮して、期待値を計算します。
    *   対戦相手の攻撃力 (A) と防御力 (D) の統計を推定するために、過去のデータとLLMの予測を組み合わせます。
        ```python
        #ダメージ計算の疑似コード
        def calculate_damage(level, power, attack, defense, other_mechanics):
            damage = (1/50 * ((2/5 * level + 2) * power * (attack / defense)) + 2) * other_mechanics
            return damage
        ```
4.  **その他:**
    *   詳細なダメージ計算式を用いて、技の効果を評価します。
    *   Pokémon ShowdownのAPIを使用して、対戦環境を構築します。

## 6. コストや物理的な詳細について

論文には、コストや物理的な詳細に関する具体的な数値は記載されていません。しかし、以下の点を考慮することができます。

*   **LLMの選択:** GPT-4o と Llama 3.1:8b が使用されています。GPT-4o は API 経由で使用した場合、トークン数に応じた費用が発生します。Llama 3.1:8b はオープンソースモデルであるため、モデル自体のライセンス費用は発生しませんが、推論を実行するための計算リソースが必要です。
*   **計算リソース:** LLM の推論には、GPU が必要です。特に、GPT-4o のような大規模モデルを使用する場合は、高性能な GPU が複数必要になる可能性があります。Llama 3.1:8b の場合、VRAM が 16GB 程度あれば推論可能ですが、より高速な推論のためには複数の GPU を使用することが望ましいです。
*   **データセット:** 300万件以上のポケモンバトルのデータセットが使用されています。このデータセットは、Pokémon Showdown プラットフォームから収集されたものと考えられます。データ収集には、スクレイピングや API の利用が必要になる場合があります。
*   **トレーニング:** LLM の追加トレーニングは行われていません。LLM の事前学習済みの知識を活用しています。
*   **オンライン評価:** Pokémon Showdown のオンラインラダーで評価を行っています。オンラインラダーの利用は無料ですが、評価には時間と計算リソースが必要です。

## 7. 参考文献のうち、特に参照すべきもの

以下の参考文献は、特に参照する価値があります。

*   **Silver et al. (2016). Mastering the game of go with deep neural networks and tree search.** AlphaGo の論文であり、深層学習と木探索を組み合わせたアプローチの基礎を理解する上で重要です。
*   **Silver et al. (2018). Mastering chess and shogi by self-play with a general reinforcement learning algorithm.** AlphaZero の論文であり、自己対戦による強化学習の有効性を示しています。
*   **Hu et al. (2024). LLM+P: Empowering Large Language Models with Optimal Search Planning.** 大規模言語モデルの計画能力を強化するための研究であり、PokéChamp のアプローチと関連があります。
*   **"How an A.I. is becoming the world’s best pokemon player, 2022."** ポケモン AI の既存研究について、概要を知る上で役立ちます。
*   **"Training AI to play pokemon with reinforcement learning, 2023."** 強化学習によるポケモン AI のトレーニングについて、概要を知る上で役立ちます。

## 8. この論文を140字以内のツイートで要約すると？

LLMでMinimax探索を強化した#PokeChamp が、ポケモンバトルで専門家レベルの性能を達成！既存ボットを圧倒し、人間プレイヤーとも互角に戦える！#LLM #Pokemon #AI


---


# HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization

[View Paper](http://arxiv.org/abs/2503.04598v1)

## 1. 既存研究では何ができなかったのか

既存のTransformerの学習における課題は、Layer Normalization(LayerNorm)の位置に起因する学習の安定性と最終的なモデル性能のトレードオフにあります。具体的には、以下の点が問題でした。

*   **Pre-Normの課題:** Pre-Normは、NormalizationをResidual Connectionの前に行うことで学習を安定化させやすく、勾配消失問題を軽減します。しかし、最終的なモデル性能がPost-Normに劣る傾向がありました。これは、Residual ConnectionとSub-Layerの出力の相互作用をNormalizationが考慮しないために、汎化性能が低下することが原因と考えられます。
*   **Post-Normの課題:** Post-Normは、Residual Connectionの後にNormalizationを行うことで、モデルの汎化性能を向上させることができます。しかし、特にDeepなモデルにおいては学習が不安定になることが多く、勾配爆発や勾配消失といった問題が発生しやすいです。

既存研究では、Pre-NormとPost-Normのどちらか一方を用いる、あるいは、Mix-LNのように層によって使い分けるなどの工夫はありましたが、各Transformerブロック内で両者の利点を統合するアプローチは十分に研究されていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、Pre-NormとPost-Normの利点を統合する新しいNormalization戦略である**HybridNorm**を提案しました。HybridNormは、各Transformerブロック内でNormalizationの位置を使い分けます。

*   **Attention機構内:** Query (Q)、Key (K)、Value (V) の各行列に対してNormalization (QKV Normalization)を適用します。これにより、Attention計算における情報の流れを安定化させます。
*   **Feed-Forward Network (FFN)内:** Residual Connectionの後にPost-Normを適用します。これにより、Deepな層においても効果的な学習を可能にし、モデルの表現力を高めます。

具体的には、Transformerブロックの計算は以下のようになります。

```python
def hybridnorm_transformer_block(x):
  # Multi-Head Attention with QKV Normalization
  q = linear(x, w_q) # Linear projection for Query
  k = linear(x, w_k) # Linear projection for Key
  v = linear(x, w_v) # Linear projection for Value
  q_norm = layer_norm(q) # Normalize Query
  k_norm = layer_norm(k) # Normalize Key
  v_norm = layer_norm(v) # Normalize Value

  attention_scores = softmax(matmul(q_norm, transpose(k_norm)) / sqrt(d_k)) # Scaled dot-product attention

  attention_output = matmul(attention_scores, v_norm)
  mha_output = linear(attention_output, w_o)

  # Residual connection
  y = mha_output + x

  # Feed-Forward Network with Post-Norm
  ffn_input = layer_norm(y)
  ffn_output = feed_forward_network(ffn_input)
  # Residual connection
  x = ffn_output + layer_norm(y) #Post Norm
  return x
```

さらに、最初のTransformerブロックに特別な処理を行う**HybridNorm*** というバリエーションも提案しています。具体的には、最初のブロックでは、MHAとFFNの両方に対してPre-Normを適用します。

## 3. 結果、何が達成できたのか

HybridNormを導入することで、以下の成果が得られました。

*   **学習の安定化:** QKV Normalizationにより、勾配爆発や勾配消失を抑制し、DeepなTransformerモデルの学習を安定化させることができました。
*   **高性能の実現:** Post-NormをFFNに適用することで、モデルの表現力を高め、Pre-Normよりも高い最終的なモデル性能を達成しました。
*   **既存手法を上回る性能:** 密なモデルとSparseなモデルの両方において、HybridNormがPre-NormとPost-Normの両方を一貫して上回り、様々なベンチマークで最先端の結果を達成しました。
*   **大規模言語モデル（LLM）への有効性:** 特にLLMの文脈において、HybridNormの利点が顕著に現れました。

これらの結果は、HybridNormがDeepなTransformerモデルの学習を改善するための、より安定かつ効果的な手法となる可能性を示しています。

## 4. Limitationや問題点は何か

論文で言及されているLimitationsと、追加で考えられるLimitationsは以下の通りです。

**論文で言及されているLimitations:**

*   **勾配爆発:** HybridNormはPostNormを使用しているため、勾配爆発が起こる可能性はあります。ただし、QKV-Normによってある程度緩和されています。
*   **初期化への依存性:** 実験結果より、Pre-Norm と HybridNorm は初期化手法に対して敏感であることが示唆されています。

**追加で考えられるLimitations:**

*   **ハイパーパラメータの調整:** HybridNormは、Pre-NormとPost-Normを組み合わせた新しいNormalization戦略であるため、学習率やWeight Decayなどのハイパーパラメータの調整がより重要になる可能性があります。
*   **計算コスト:** QKV Normalizationは、追加の計算コストを必要とする可能性があります。特に大規模なモデルにおいては、このコストが無視できない場合があります。
*   **理論的な解析:** HybridNormの有効性については、実験的な検証が中心であり、その動作原理に関する理論的な解析はまだ十分ではありません。
*   **特定のタスクへの偏り:** 実験はLLMタスクが中心であるため、他の種類のタスク(画像認識、音声認識など)に対するHybridNormの有効性は検証されていません。

## 5. 技術的な詳細について

HybridNormは、TransformerブロックにおけるLayer Normalizationの位置を最適化する手法です。技術的な詳細は以下の通りです。

1.  **QKV Normalization:** Multi-Head Attention (MHA)機構において、Query (Q), Key (K), Value (V) 行列をそれぞれLayer Normalizationします。これにより、Attention Mapの生成が安定化され、勾配消失/爆発を抑制します。Layer Normalizationは、各行列のサンプルごとの平均と分散を用いて、以下のように計算されます。

    ```python
    def layer_norm(x, eps=1e-5):
      mean = mean(x, axis=-1, keepdims=True) # Calculate mean across features
      variance = var(x, axis=-1, keepdims=True) # Calculate variance across features
      x_norm = (x - mean) / sqrt(variance + eps) # Normalize
      return x_norm
    ```
2.  **Post-Norm in FFN:** Feed-Forward Network (FFN)への入力に対して、Layer NormalizationをResidual Connectionの後に行います。これにより、モデルの表現能力を最大限に引き出すとともに、Deepな層における学習を効果的にします。

3.  **HybridNorm* (Optional):** 最初のTransformerブロックに特別な処理を行います。最初のブロックでは、MHAとFFNの両方に対してPre-Normを適用します。

## 6. コストや物理的な詳細について

論文には、以下のコストおよび物理的な詳細が記載されています。

*   **モデルサイズ:**
    *   Denseモデル: 550Mパラメータ、1.2Bパラメータ (1.27Bパラメータ)
    *   MoEモデル: 1.3Bパラメータ活性化 (6.9Bパラメータ中)
*   **データセット:** OLMoE Mix dataset ([https://huggingface.co/datasets/allenai/OLMoE-mix-0924](https://huggingface.co/datasets/allenai/OLMoE-mix-0924))
*   **ハイパーパラメータ:** 詳細はTable 1, Table 7参照. 特に、学習率はDense Modelで3e-4, MoEモデルで4e-4, optimizerにAdamWを使用,  warmup tokensが8,388,608,000 (Dense), 10,485,760,000 (MoE)など。
*   **その他:**
    *   Megatron initializationを使用
    *   コンテキスト長: 4096

トレーニングに使用したGPUの数や時間については、論文には明記されていません。
しかし、モデルサイズとデータセットの規模から判断すると、多数のGPUを用いて数日から数週間かけて学習を行ったと推測できます。

## 7. 参考文献のうち、特に参照すべきもの

*   **Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.** (Layer Normalizationの原論文)
*   **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need.** (Transformerの原論文)
*   **Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture.** (TransformerにおけるLayer Normalizationの重要性について議論)
*   **Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, E. P., Tafjord, O., Lambert, N., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden, D., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi, A., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H. OLMoe: Open mixture-of-experts language models.** (MoEモデルのベースラインとして使用)
*   **Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi-billion parameter language models using model parallelism.** (モデルの初期化に使用)

## 8. この論文を140字以内のツイートで要約すると？

HybridNorm：Transformerの学習安定性と性能を両立する新Normalization！Attention機構はQKV Norm、FFNはPostNormでLLMも爆速学習🚀 #HybridNorm #Transformer #LLM


---


# LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM

[View Paper](http://arxiv.org/abs/2503.04724v1)

## 1. 既存研究では何ができなかったのか

既存の音声対応LLM（Speech-enabled LLM）は、以下の点で課題がありました。

*   **ファインチューニングの必要性:** LLMを音声データでファインチューニングする必要があり、計算コストが高く、データ要件も大きかった。
*   **計算オーバーヘッド:** エンドツーエンドのシステムでは、LLM自体の構造を修正する必要があり、計算負荷が高かった。
*   **テキストと音声のずれ:** 音声データでのファインチューニングにより、LLMの言語能力（推論能力や表現力）が低下することがあった。
*   **LLMへの依存:** 多くのアーキテクチャがLLMの隠れ層に依存して音声合成を行っていたため、他のLLMに適用する際に再適応が必要だった。
*   **高レイテンシ:** 従来のTTSモデルは、LLMがテキストを生成し終わるまで音声合成を開始できなかった。また、非ストリーミングの音声デコーダを使用していたため、テキストと音声の間に大きな遅延が生じていた。

## 2. どのようなアプローチでそれを解決しようとしたか

LLMVoXは、上記の問題点を解決するために、以下のアプローチを採用しました。

*   **LLMに依存しない設計（LLM-agnostic）:** LLMと音声合成を完全に分離することで、LLMの能力を維持した。
*   **軽量なストリーミングTTSモデル:** 30Mパラメータの軽量なモデルを使用し、高速な音声合成を実現した。
*   **自己回帰型（Autoregressive）:** ストリーミングテキストから自己回帰的に音声を生成することで、低遅延を実現した。
*   **マルチキュー・トークン・ストリーミングシステム:** LLMのテキスト生成と並行して音声合成を行うことで、シームレスな対話を実現した。
*   **ByT5埋め込み:** 明示的な音素変換を行わずに、ByT5モデルの埋め込み層を使用して音韻情報を注入することで、計算コストを削減し、低遅延を実現した。
*   **適応的なチャンクサイズ:** ストリーミング推論時に、初期チャンクサイズを小さくし、デコードごとにチャンクサイズを倍増させることで、音声品質を維持しながら低遅延を実現した。

## 3. 結果、何が達成できたのか

LLMVoXによって、以下の成果が達成されました。

*   **低い単語誤り率（WER）:** 従来の音声対応LLMと比較して、大幅に低いWERを達成した（3.70%）。
*   **競争力のあるレイテンシ:** 音声対応LLMと同程度の低遅延（475ms）で動作した。
*   **高いUTMOSスコア:** 音声品質を示すUTMOSスコアで4.05を達成した。
*   **LLM能力の維持:** LLMを変更しないため、基盤となるLLMの能力を完全に維持できた。
*   **シームレスな無限長対話:** マルチキュー・トークン・ストリーミングシステムにより、途切れのない対話が可能になった。
*   **様々なバックボーンへの拡張性:** プラグアンドプレイ設計により、さまざまなLLMやVLM（Vision-Language Model）に容易に拡張できた。
*   **多言語への一般化:** データセットの適応のみで、新しい言語（アラビア語）に対応できた。
*   **マルチモーダル統合:** Vision-Language Modelと統合し、音声、テキスト、視覚機能を備えたオムニモデルを、追加のマルチモーダルトレーニングなしに実現できた。

## 4. Limitationや問題点は何か

LLMVoXのLimitationと問題点は以下の通りです。

*   **声のクローン機能の欠如:** 話者固有の声の特徴を生成する能力がないため、パーソナライズされた対話には不向きである。
*   **ASRストリーミングパイプラインの未統合:** WhisperをASRに使用しているが、完全にストリーミングパイプラインに統合されていないため、さらなる遅延削減の余地がある。
*   **単一話者での学習:** 著者らが考える問題点として、単一話者での学習のため、話者の多様性に乏しい可能性がある。
*   **品質とレイテンシのトレードオフ:** チャンクサイズを大きくすることで音声品質は向上するが、レイテンシが増加する可能性がある。
*   **LLMの規模によるレイテンシの影響:** LLMの規模が大きいほど、レイテンシが増加する傾向がある。
*   **未知の単語や文に対するロバスト性:** 学習データに含まれない単語や文に対する音声合成の品質が未知数である。

## 5. 技術的な詳細について

LLMVoXは、以下の技術要素で構成されています。

*   **WavTokenizer:** 音声信号を離散的なトークン列に変換するニューラルオーディオコーデック。Single-layer Residual Vector Quantization (RVQ)を使用し、4096のエントリを持つ固定語彙からトークンを生成。サンプリングレート24kHzの音声に対し、1秒あたり40-75トークンを生成する。
*   **ByT5 Grapheme-to-Phoneme（G2P）埋め込み:** 明示的な音素予測のオーバーヘッドを避けるため、ByT5モデルの埋め込み層を利用して音韻情報を注入。ByT5トークナイザーを用いてテキストをバイトレベルのサブトークンに分割し、各サブトークンを256次元の埋め込みベクトルにマッピング。
*   **デコーダ専用Transformer:** 4層の軽量なTransformerデコーダを使用し、音声トークン列を自己回帰的に予測。入力ベクトルは、ByT5埋め込みベクトルと前の音声トークンの特徴ベクトルを連結し、L2正規化と位置埋め込みを適用。
*   **ストリーミング推論:** LLMのテキスト生成と並行して音声合成を行うため、マルチキュー・トークン・ストリーミングシステムを採用。LLMから生成されたテキストトークンをFIFOキューに格納し、2つのレプリカTTSモジュールが並行してテキストを処理。初期チャンクサイズを小さくし、デコードごとにチャンクサイズを倍増させることで、音声品質を維持しながら低遅延を実現。

疑似コード:

```python
# ストリーミング推論の疑似コード
def streaming_inference(user_speech):
    asr_text = ASR(user_speech) # ASRでテキストに変換
    llm_text_stream = LLM(asr_text) # LLMでテキストを生成 (ストリーミング)

    queue1 = FIFOQueue()
    queue2 = FIFOQueue()
    tts_module1 = LLMVoX()
    tts_module2 = LLMVoX()

    # テキストを交互にキューにenqueue
    for sentence in llm_text_stream:
        if sentence_boundary(sentence):
            if len(queue1) <= len(queue2):
                queue1.enqueue(sentence)
            else:
                queue2.enqueue(sentence)

    # 各TTSモジュールで並行して音声合成
    speech_stream1 = tts_module1.generate_speech(queue1)
    speech_stream2 = tts_module2.generate_speech(queue2)

    # 音声をストリーミング再生
    play_speech_stream(merge_streams(speech_stream1, speech_stream2))

# LLMVoXの音声生成 (簡略化)
class LLMVoX:
    def __init__(self):
        self.wav_tokenizer = WavTokenizer()
        self.byt5_embed = ByT5Embeddings()
        self.transformer = DecoderOnlyTransformer()

    def generate_speech(self, text_queue):
        chunk_size = INITIAL_CHUNK_SIZE
        start_idx = 0
        speech_stream = []

        while not text_queue.is_empty():
            text_chunk = text_queue.dequeue(chunk_size)
            end_idx = start_idx + len(text_chunk)
            byt5_embeddings = self.byt5_embed.embed(text_chunk) # Byte5埋め込み
            # concatenate previous token feature vector + byt5 embedding
            input_vector = concatenate(byt5_embeddings, speech_stream[-1].features if speech_stream else zero_tensor)
            normalized_input = l2_normalize(input_vector) # 正規化
            input_with_position = add_positional_embedding(normalized_input, start_idx) # 位置埋め込み
            
            speech_tokens = self.transformer.predict(input_with_position) # トランスフォーマーでトークン予測
            speech_stream.extend(self.wav_tokenizer.decode(speech_tokens)) # WavTokenizerで音声生成
            
            start_idx = end_idx
            chunk_size *= 2
        return speech_stream

# Byte5埋め込み (疑似コード)
class ByT5Embeddings:
    def __init__(self):
        self.embedding_table = load_pretrained_byt5_embedding()
    def embed(self, text_chunk):
        byte_tokens = byt5_tokenize(text_chunk)
        return [self.embedding_table[token] for token in byte_tokens]

class WavTokenizer:
    def __init__(self):
        self.vq_model = load_pretrained_vq_model()
    def decode(self, discrete_tokens):
        return self.vq_model.decode(discrete_tokens)
```

## 6. コストや物理的な詳細について

*   **モデルサイズ:** 30Mパラメータ
*   **トレーニングデータ:**
    *   英語: 2,200時間の単一話者合成音声データ (GPT-4o生成の質問応答ペア)
    *   アラビア語: 1,500時間の単一話者合成音声データ (Hugging Faceコーパスから生成)
*   **GPU:** 4 x A100 GPUs
*   **トレーニング時間:** 約3日間
*   **最適化関数:** AdamW (weight decay=0.1)
*   **学習率:** 3e-4 から 3e-6まで、50Kステップのwarmup後、1Mステップかけてdecay.
*   **勾配クリッピング:** norm 1.0でクリップ
*   **推論時の量子化:** 使用 (具体的にどのライブラリかは不明)

## 7. 参考文献のうち、特に参照すべきもの

*   **Seamless:** (LoГҜc Barrault et al., 2023) 多言語音声翻訳に関する研究。
*   **XTTS:** (Edresson Casanova et al., 2024) 大規模な多言語ゼロショットTTSモデル。LLMVoXのアラビア語トレーニングデータ生成に使用されている。
*   **Moshi:** (Alexandre DГ©fossez et al., 2024) 低遅延対話のための音声テキスト基盤モデル。LLMVoXの比較対象。
*   **ByT5:** (Linting Xue et al., 2022) トークンフリーなByte-to-Byteモデル。LLMVoXの音韻情報注入に使用。
*   **Wavtokenizer:** (Shengpeng Ji et al., 2024) 効率的な音響離散コーデックトークナイザー。LLMVoXの音声トークン化に使用。

## 8. この論文を140字以内のツイートで要約すると？

LLMの能力を損なわずに、低遅延で高品質な音声合成を実現するLLMVoXを発表！🗣️ 30Mパラメータの軽量モデルで、様々なLLMにプラグイン可能。多言語対応やVLMとの連携も実現し、リアルタイムAI対話の新境地を開拓！✨ #LLM #TTS #AI #音声合成
