
# Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning

[View Paper](http://arxiv.org/abs/2503.11646v1)

## 1. 既存研究では何ができなかったのか

既存のロボット操作における模倣学習データ収集は、以下の点で限界がありました。

*   **データ効率の低さ:** 大量のデータを必要とするものの、各デモンストレーションに含まれる情報密度が低い。特に、静的な環境でのタスク実行を記録する従来の方法では、冗長なフレームや反復的な命令が多く、データセットの価値が希釈されていた。例えば、30秒の「物を掴んで置く」タスクでは、ほぼ同一のトレーニングサンプルが多数生成され、シーンや命令の多様性が不足していた。
*   **実世界への適用困難性:** シミュレーションデータはスケーラビリティに優れるものの、現実のロボットダイナミクスとの間にドメインギャップが存在し、学習した内容を実世界へ転移させることが難しかった。
*   **人間とのインタラクションの欠如:** 既存研究では、ロボットと人間との動的なインタラクションを考慮していなかった。特に、人間が介入するシナリオにおけるロバスト性が考慮されていなかった。
*   **言語と視覚の次元における多様性の欠如:** 既存研究では、視覚的な要素に関する構成的な汎化に焦点が当てられていたが、言語的な側面は考慮されていなかった。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の問題を解決するために、Adversarial Data Collection (ADC) という新しいHuman-in-the-Loop (HiL) フレームワークを提案しました。ADCは、以下の主要な要素によって構成されます。

*   **協調的な摂動パラダイム:** ADCは、従来の静的なデモンストレーション記録とは異なり、リアルタイムでの双方向的な人間と環境のインタラクションを通じてロボットのデータ収集を再定義します。
*   **敵対的オペレーターの導入:** データ収集中に、敵対的オペレーターがオブジェクトの状態、環境条件、言語命令を動的に変更します。
*   **テレオペレーターの適応的行動:** テレオペレーターは、進化する課題を克服するために、敵対的オペレーターによる摂動に応じてアクションを適応的に調整します。
*   **単一エピソード内での多様性の圧縮:** このプロセスにより、多様な失敗回復行動、構成的なタスクバリエーション、環境摂動が最小限のデモンストレーションに圧縮されます。
*   **視覚的および言語的次元の統合:** 視覚的な摂動（オブジェクトの位置変更、背景の変更など）と、言語的な摂動（タスク指示の言い換え、目標の変更など）を組み合わせることで、より多様なデータ収集を実現しました。

具体的には、ADCフレームワークは以下のメカニズムを使用します。

1.  **敵対的オペレーターによる摂動:**
    *   **視覚的摂動:** オブジェクトの位置や向きを動的に変更します。例えば、把持の途中でオブジェクトを回転させたり、背景を変更したりします。
    *   **言語的摂動:** タスクの指示をリアルタイムで変更します。例えば、「オレンジを掴む」から「スイカを掴む」へと言語命令を変更します。
2.  **テレオペレーターの適応:**
    *   敵対的オペレーターによる摂動に応じて、テレオペレーターは把持戦略の再計画や軌道修正など、アクションを適応的に再計画します。
3.  **データセットの構造化:**
    *   収集されたデータは、現在の視覚的シーン、言語命令、およびアクションのトリプレットとして構造化されます。
4.  **多様性の最大化:**
    *   ADCは、デモンストレーションあたりの多様性とロバスト性を最大化するために、データ取得中に制御された摂動を体系的に注入します。
    *   これらの摂動は、情報密度を高め、状態空間の範囲を拡大し、ターゲットオブジェクトのより完全な観測を提供します。
5.  **効率的なデータ収集:**
    *   ADCにより、テレオペレーターは動的な摂動下で適応的に行動を再計画する必要があるため、回復行動、命令の構成的な一般化、および環境の変化が統一されたデモンストレーションに圧縮されます。
    *   リアルタイムの人間主導の敵対と適応を体系的に組み合わせることで、デモンストレーションの効率が向上するだけでなく、知覚的な曖昧さ、言語的な変動、および物理的な不確実性に対するモデルのロバスト性が向上します。

## 3. 結果、何が達成できたのか

ADCを用いた実験の結果、以下の点が達成されました。

*   **構成的な汎化性能の向上:** ADCで学習したモデルは、未知のタスク指示に対する優れた構成的な汎化性能を示しました。特に、「キウイを置く」という未知のタスクにおいても、高い成功率を達成しました。
*   **知覚的摂動に対するロバスト性の向上:** ADCで学習したモデルは、視覚的な摂動（オブジェクトの位置変更など）に対して高いロバスト性を示しました。動的な環境においても、静的な環境と同等の成功率を維持しました。
*   **エラー回復能力の獲得:** ADCで学習したモデルは、空掴みなどのエラーが発生した場合に、自動的に再試行を行うエラー回復能力を獲得しました。
*   **データ効率の向上:** ADCで学習したモデルは、従来のデータ収集方法で収集したデータセットのわずか20%のデータ量で、同等以上の性能を達成しました。
*   **VLAモデルの性能向上:** ADCは、Vision-Language-Action（VLA）モデルのトレーニングに特に効果的であることが示されました。ADCでトレーニングされたVLAモデルは、言語と視覚の間のより強力なアライメントを示し、動的な環境や言語的な摂動に対するロバスト性が向上しました。
*   **現実世界の複雑さのエンコード:** ADCは、意図的な敵対を通じて現実世界の複雑さをエンコードすることにより、データ品質を量への代替として変換し、スケーラブルなロボット学習を大きく前進させることができました。
*   **ゼロショット一般化:** ADCデータセットでトレーニングされたVLAモデルは、テーブルクロスが導入されたさまざまなシーンで評価実験を実施し、優れたゼロショット一般化機能を発揮しました。
*   **人間とロボットの動的なインタラクション:** ADCを介してトレーニングされたモデルは、人間がオブジェクトを保持して操作中に動かすシナリオで評価され、アクション予測を動的に調整する能力を示しました。

## 4. Limitationや問題点は何か

ADCには多くの利点がある一方で、以下の様な限界や問題点も存在します。

*   **人的コスト:** ADCは、テレオペレーターに加えて、敵対的オペレーターを必要とするため、人的コストが増加する可能性があります。ただし、敵対的オペレーターは、テレオペレーターほど高い専門知識を必要とせず、断続的な関与で済むため、全体的なコスト効率は高いと考えられます。
*   **深度推定の困難性:** オブジェクトの配置高さが変化する場合、深度推定が困難になる可能性があります。これは、ADCによって状態の分散が増加するため、より小さなモデルでは、高い状態カバレッジと低いアクションの一貫性をカバーできないことが原因であると考えられます。
*   **言語摂動への対応:** 指示が把持の段階で変更された場合、ADCモデルのパフォーマンスが低下する可能性があります。これは、ロボットが現在のアクションを停止し、状況を再評価し、新しいターゲットを決定する必要があるためです。
*   **ハードウェアの安定性:** ハードウェアが不安定な場合、ADCの有効性が低下する可能性があります。
*   **汎用的な空間理解:** 空間理解はVLAにとって別個の研究の焦点であるため、空間記述子を動的に再定義するADCのメカニズムは、本研究では主に実証されていません。
*   **データ注釈の課題:** 動的な指示からの注釈を管理することは課題になる可能性がありますが、サブタスクレベルのラベル付けでタスクを分解し、時間的な連続性を維持することで管理できます。
*   **多様な環境への適用:** 本研究では、特定のタスクと環境に焦点を当ててADCを評価しました。ADCの有効性は、他のタスクや環境では異なる可能性があります。
*   **倫理的な考慮事項:** 特に現実世界での人間とロボットのインタラクションのコンテキストでは、敵対的なデータ収集の倫理的影響を考慮することが重要です。

私が考えるADCの潜在的な問題点として以下が挙げられます。

*   **敵対的オペレーターの熟練度:** 敵対的オペレーターの熟練度が低い場合、効果的な摂動を生成できず、ADCの性能が低下する可能性があります。
*   **安全性:** 敵対的な摂動が、ロボットや周囲の環境に危険を及ぼす可能性があります。安全性を確保するために、摂動の範囲を制限したり、安全機構を導入したりする必要があります。
*   **バイアス:** 敵対的オペレーターのバイアスが、生成されるデータセットに反映される可能性があります。バイアスを軽減するために、複数の敵対的オペレーターを使用したり、摂動をランダム化したりする必要があります。

## 5. 技術的な詳細について

本研究で提案されたADCフレームワークは、以下の技術的な要素に基づいて構成されています。

*   **Vision-Language-Action (VLA) モデル:**
    *   事前学習済みのマルチモーダル大規模言語モデル (MLLM) を、残差ポリシーヘッドを介してロボット制御に適応させます。
    *   これらの時間的に独立したモデルは、以下の確率分布に従ってアクションを予測します。

    ```python
    def VLA(visual_input, language_instruction):
        # visual_input: multi-viewからの視覚情報
        # language_instruction: 自然言語による指示
        action = model(visual_input, language_instruction) # VLAモデルによるアクション予測
        return action

    def action_probability(action, visual_input, language_instruction):
        # action: 行動
        # visual_input: multi-viewからの視覚情報
        # language_instruction: 自然言語による指示
        probability = VLA(visual_input, language_instruction) # VLAモデルによるアクション予測
        return probability
    ```

*   **データセットの構造:**
    *   トレーニングデータは、以下のトリプレットとして表現されます。
        ```python
        # U_t = (V_t, L_t, a_t*)
        # V_t: 時刻tにおける視覚情報
        # L_t: 時刻tにおける言語指示
        # a_t*: 時刻tにおける専門家による行動
        ```
    *   従来のデータセットは、静的な環境で固定されたエピソードで構成されていましたが、ADCでは、敵対的な摂動によって多様なデータが生成されます。
*   **敵対的摂動:**
    *   **視覚的摂動:**
        *   ターゲットオブジェクトの位置は、タスク固有のガウス分布 caligraphic_N(italic_μ_{task}, italic_σ^{2}_{task}) からサンプリングされます。
        *   エンドエフェクター (EEF) が把持を試みても近接閾値内に入らない場合、敵対的オペレーターは作業テーブル領域内でオブジェクトの位置をランダムに変更します。
        *   EEFが近接閾値内に入ると、制御された摂動インパルスがオブジェクトに適用され、オブジェクトの位置と向きが変更されます。これにより、システムは次の最適な把持構成を見つける必要が生じます。
    *   **言語的摂動:**
        *   タスクの目標は、ターゲットオブジェクトの変更 (例: "カップを置く" → "ボトルを置く")、必要なアクションの変更 (例: "カップを持ち上げる" → "カップを押す")、またはオブジェクトとアクションの両方の同時変更など、重要なフェーズ中に動的に変更されます。
        *   空間記述子 (例: "左" または "近く") は、変化するオブジェクトの位置に対して継続的に再定義され、固定された座標系への過剰適合を防ぎます。
*   **評価:**
    *   モデルの性能は、静的な環境と動的な環境の両方で評価されます。動的な環境では、敵対的な評価が適用され、人間がリアルタイムでオブジェクトの位置や言語指示を変更します。
    *   VLAモデルのアテンションマップを視覚化することで、モデルが重要な特徴に注目しているかどうかを分析します。
    *   カメラが故障した状況をシミュレートするために、ターゲットカメラからの入力をゼロ行列で置き換えることで、モデルのロバスト性を評価します。

## 6. コストや物理的な詳細について

論文中に記載されているコストや物理的な詳細は以下の通りです。

*   **ロボットプラットフォーム:**
    *   従来のロボットポリシー実験にはAlohaロボットが使用され、VLAポリシー実験にはAgiBot G1ロボットが使用されました。
*   **データ収集:**
    *   ARテレオペレーションを使用して、AgiBot G1ロボットプラットフォームでVLAモデルのデータを収集しました。
*   **敵対的データ収集の人的コスト:**
    *   ADCと従来の方法では、フレームごとの注釈レイテンシが同程度（45.8ms vs. 46.7ms）でした。
    *   ADCのエピソードあたりの時間コストはわずかに高くなりました（1.2倍）。
*   **データセットの規模:**
    *   ADC-Roboticsデータセットは、敵対的な摂動を伴う現実世界の操作タスクで構成されています。
    *   20%のADCデータで学習したモデルは、完全な従来のデータセットで学習した従来の方法を大幅に上回りました。
*   **VLAモデルの設定:**
    *   2つの手首画像と1つの頭部画像を視覚入力として使用しました。

論文には、トレーニングに使用したGPUの数や時間、モデルのサイズなどの具体的な情報は記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

ADCフレームワークの理解を深めるために、以下の参考文献を参照することを推奨します。

*   **データ収集に関する既存研究:**
    *   A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, “Droid: A large-scale in-the-wild robot manipulation dataset,” 2024.
    *   F. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao, “Data scaling laws in imitation learning for robotic manipulation,”
    *   J. Gao, A. Xie, T. Xiao, C. Finn, and D. Sadigh, “Efficient data collection for robotic manipulation via compositional generalization,”
*   **Vision-Language-Action (VLA) モデルに関する研究:**
    *   M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, “Openvla: An open-source vision-language-action model,”
    *   K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, “Vla: A vision-language-action flow model for general robot control,”
*   **ロボット操作における一般化とロバスト性に関する研究:**
    *   S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, “R3m: A universal visual representation for robot manipulation,”
    *   A. Xie, L. Lee, T. Xiao, and C. Finn, “Decomposing the generalization gap in imitation learning for visual robotic manipulation,” in 2024 IEEE International Conference on Robotics and Automation (ICRA)
*    **CALVIN: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks:**
    *   O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, “Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks,”
*   **OCTO: An open-source generalist robot policy:**
    *   O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, “Octo: An open-source generalist robot policy,”

## 8. この論文を140字以内のツイートで要約すると？

データ収集に革命！🤖 ADCは、敵対的な人間介入でロボットに試練を与え、少量データで高汎化＆高ロバストを実現。実世界で活躍する賢いロボットへの道を開く！ #ロボット #機械学習 #データ効率


---


# Open-World Skill Discovery from Unsegmented Demonstrations

[View Paper](http://arxiv.org/abs/2503.10684v1)

## 1. 既存研究では何ができなかったのか

既存研究は、オープンワールド環境における未分割のデモンストレーションビデオからのスキル発見において、以下の点で課題がありました。

*   **スキル識別のためのセグメンテーションとラベリングの難しさ:** オンラインのデモンストレーションビデオは通常長く、未分割であるため、スキルを識別するためのセグメンテーションとラベリングが困難でした。
*   **人手によるラベル付けへの依存:** 既存の手法は、シーケンスサンプリングまたは人手によるラベル付けに依存しており、コストがかかり、スケーラビリティに問題がありました。
*   **部分的な観測可能性への対応:** 多くの手法は、完全に観測可能な環境に限定されており、視覚的に部分的に観測可能な環境では困難でした。
*   **報酬信号への依存:** 環境からの報酬信号に依存する方法は、報酬が関連付けられていないスキルを捉えることができず、単一のスキルが複数のセグメントに分割されるリスクがありました。
*   **スキル多様性の制限と計算コスト:** トップダウンおよびボトムアップの手法は、スキル多様性の制限や高い計算コストにつながる可能性がありました。
*   **LLMベースのエージェントにおける課題:** LLMベースの命令追従エージェントは、命令をアトムスキルに変換し、条件付きポリシーを使用してアクションに変換する2層構造を採用していますが、実世界のビデオは長期的で未分割であり、詳細な言語スキルが含まれているため、学習が困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の問題を解決するために、以下の新しい自己教師あり学習ベースのアプローチを開発しました。

*   **Skill Boundary Detection (SBD)の導入:** 人間の認知イベントセグメンテーション理論に着想を得て、アノテーションフリーな時間ビデオセグメンテーションアルゴリズムであるSBDを導入しました。
*   **予測誤差の利用:** SBDは、事前学習済みのアクション予測モデルからの予測誤差を利用して、ビデオ内のスキル境界を検出します。予測誤差の有意な増加は、実行されているスキルの変化を示すという仮定に基づいています。
*   **自己教師あり学習:** SBDは自己教師あり学習に依存しているため、追加の人手によるラベル付けの必要性がなく、広範なYouTubeビデオを使用して命令追従エージェントを訓練できます。
*   **Minecraftでの評価:** 提案手法を、豊富なオープンワールドシミュレーターであり、オンラインで広範なゲームプレイビデオが利用可能なMinecraftで評価しました。
*   **階層型エージェントの訓練:** SBDで生成されたセグメントを使用して、短期的なアトムスキルタスクに対する条件付きポリシーと、長期的なタスクに対する対応する階層型エージェントのパフォーマンスを向上させました。

## 3. 結果、何が達成できたのか

提案手法SBDにより、以下の成果を達成しました。

*   **条件付きポリシーのパフォーマンス向上:** SBDで生成されたセグメントは、短期的なアトムスキルタスクにおける条件付きポリシーの平均パフォーマンスを63.7%および52.1%向上させました。
*   **階層型エージェントのパフォーマンス向上:** SBDで生成されたセグメントは、長期的なタスクにおける対応する階層型エージェントのパフォーマンスを11.3%および20.8%向上させました。
*   **多様なYouTubeビデオの活用:** 提案手法は、多様なYouTubeビデオを活用して、命令追従エージェントを訓練できます。
*   **効果的なスキル発見:** 提案手法は、未分割のデモンストレーションからのスキル発見に効果的であり、オープンワールドスキル学習を進歩させる可能性を示しました。

## 4. Limitationや問題点は何か

この論文で提案されたSBD手法には、いくつかの制限事項と問題点が存在します。

*   **計算コスト:** アルゴリズムは比較的時間消費的であり、NVIDIA RTX 4090Tiで5分間のビデオごとに約1分の処理時間を要します。これは、軌跡全体にわたってアクションを予測し、損失を計算する必要があるためです。
*   **大規模データセットでのテスト不足:** 計算コストのため、より大規模なデータセットでのテストは行われていません。
*   **事前学習モデル選択とハイパーパラメータ調整:** 評価には、データセット全体のリプロセスとコントローラーの再トレーニングが必要であるため、事前学習モデルの選択やハイパーパラメータの調整に関する広範なアブレーションスタディは実施されていません。
*   **アクション集約的なシナリオでの不安定性:** 格闘シーンなど、アクション集約的なシナリオでは、アルゴリズムが不安定になり、短いセグメントが多数生成されることがあります。

**私見による追加の制限事項と問題点:**

*   **GAPパラメータの調整:** GAPパラメータの調整がタスクやデータセットに依存する可能性があり、汎用性に課題が残る可能性があります。
*   **Minecraft特化の可能性:** Minecraftという特定の環境でのみ評価されているため、他のオープンワールド環境への適用可能性は不明確です。
*   **外部情報の依存:** 外部情報を使用した場合の性能向上は示されていますが、外部情報がない場合でも十分な性能を発揮できることを示しているものの、タスクによっては外部情報に性能が左右される可能性があります。

## 5. 技術的な詳細について

SBD (Skill Boundary Detection) は、未分割のデモンストレーションビデオからスキル境界を検出するための自己教師あり学習に基づくアプローチです。 以下に技術的な詳細を説明します。

1.  **事前学習済みアクション予測モデルの訓練:**

    *   未分割のビデオデータセット `D = [(o_i, a_i)]_{i=1}^T`  を使用して、行動クローニング (Behavioral Cloning) により、無条件ポリシー `π_uncondition` を学習します。 ここで `o_i` は観測、`a_i` はアクションを表します。
    *   損失関数は、アクションの負の対数尤度です。
        ```python
        def train_unconditional_policy(dataset):
            model = create_model()
            optimizer = Adam(model.parameters())
            for epoch in range(num_epochs):
                for observations, actions in dataset:
                    predictions = model(observations)
                    loss = -torch.mean(torch.log(predictions.gather(1, actions))) # Negative Log-Likelihood
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
            return model
        ```

2.  **スキル境界の検出:**

    *   事前学習済みモデル `π_uncondition` を使用して、未分割のビデオの各タイムステップにおける予測誤差を計算します。
    *   予測誤差 `loss_t` が、過去の損失履歴の平均 `mean(loss_history)` をハイパーパラメータ `GAP` を超えて上回る場合、または外部情報 `e_t` が真である場合、そのタイムステップをスキル境界としてマークします。
        ```python
        def segment_video(model, video, gap_threshold, external_info):
            loss_history = []
            segments = []
            start_index = 0
            for t in range(1, len(video)):
                observations = video[:t]
                true_action = video[t][1] # a_t
                predicted_action_probs = model(observations) # π_uncondition(a_t|o_1:t)
                predicted_action = torch.argmax(predicted_action_probs)

                loss = calculate_loss(predicted_action, true_action) # Loss calculation
                loss_history.append(loss)

                if len(loss_history) > window_size:
                    loss_history.pop(0) # Maintain window

                if loss - torch.mean(torch.tensor(loss_history)) > gap_threshold or external_info[t]:
                    segments.append(video[start_index:t])
                    start_index = t
                    loss_history = [] # Reset history after segmenting
            segments.append(video[start_index:]) # Append the last segment

            return segments
        ```

3.  **条件付きポリシーの学習:**

    *   SBDによって生成された短いセグメントを使用して、条件付きポリシーを学習します。 条件付きポリシーには、ビデオ条件付きポリシー (GROOT) や言語条件付きポリシー (STEVE-1) があります。

4.  **階層型エージェントの構築:**
    *   学習されたスキルをビジョン言語モデルと統合して、階層型エージェントを構築し、長期タスクを実行します。

## 6. コストや物理的な詳細について

本研究で使用されたコストや物理的な詳細は以下の通りです。

*   **GPU:** 4台のNVIDIA RTX 4090Ti GPUを使用
*   **データセット:**
    *   OpenAI contractor dataset 7.x (early game): 68Mフレーム、約1000時間のオフライン軌跡データ
    *   SBDによるセグメント化されたデータセット: 130k個のサブ軌跡
*   **事前学習済みモデル:** vpt-3xモデルを使用。大規模なYouTubeデータで事前学習され、行動クローニングによって早期ゲームデータセットでファインチューニングされたTransformerベースのモデル。
*   **処理時間:** 5分間のビデオあたり約1分の処理時間 (NVIDIA RTX 4090Tiを使用)
*   **訓練エポック:** GROOTはデータセットで3エポック、STEVE-1は1.5エポック訓練。

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下の通りです。

*   **B. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems**：　VPTモデルは、大規模なYouTubeデータで事前学習された基盤モデルであり、本研究のunconditional policyの事前学習に利用されている。
*   **S. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions by watching gameplay videos. The Twelfth International Conference on Learning Representations**：　本研究のvideo-conditioned policyであるGROOTのアーキテクチャと学習方法について記述されている。
*   **S. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for text-to-behavior in minecraft. Advances in Neural Information Processing Systems**：　本研究のlanguage-conditioned policyであるSTEVE-1のアーキテクチャと学習方法について記述されている。
*   **Z. Wang, S. Cai, Z. Mu, H. Lin, C. Zhang, X. Liu, Q. Li, A. Liu, X. S. Ma, and Y. Liang. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. Advances in Neural Information Processing Systems**：　本研究で評価に使用された階層型エージェントOmniJARVISのアーキテクチャと学習方法について記述されている。
*   **Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng, Y. Yang, , X. Ma, and Y. Liang. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. IEEE Transactions on Pattern Analysis and Machine Intelligence**：　本研究で評価に使用された階層型エージェントJARVIS-1のアーキテクチャと学習方法について記述されている。
*   **J. M. Zacks, N. K. Speer, K. M. Swallow, T. S. Braver, and J. R. Reynolds. Event perception: a mind-brain perspective. https://doi.org/10.1016/j.neuropsychologia.2009.02.003 https://www.sciencedirect.com/science/article/pii/S0028393209000645**：人間の認知イベントセグメンテーション理論（EST）について述べられており、SBDの着想の元になった理論。

## 8. この論文を140字以内のツイートで要約すると？

未分割のゲーム動画から #SBD でスキル発見！予測誤差からスキル境界を自動検出し、 #Minecraft エージェントの性能を大幅UP🚀人手不要で多様なスキル学習を実現。 #AI #ゲームAI


---


# Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers

[View Paper](http://arxiv.org/abs/2503.11579v1)

## 1. 既存研究では何ができなかったのか

既存のTransformerベースの大規模マルチモーダルモデル(LMMs)は、以下の点で hour-long の長尺ビデオの理解に苦戦していました。

*   **計算コスト**: Causal self-attention の計算量が入力系列長の二乗に比例するため、長尺ビデオを扱う際の計算コストが非常に高くなります。学習時・推論時ともにメモリ消費量が多く、処理速度が低下します。
*   **情報損失**: トークン圧縮による効率化は、ビデオトークン数を減らす際に重要な情報を失う可能性があります。特に hour-long のような非常に長いビデオでは、わずかな情報の損失が大きな影響を与える可能性があります。
*   **効率の限界**: 既存のトークン圧縮手法は、計算量は削減できるものの、圧縮処理自体のオーバーヘッドがあり、全体的な処理速度の向上に限界がありました。また、入力フレーム数が増加すると、依然として二乗の計算量の問題が残ります。

既存研究では、主にトークン圧縮によって計算コストを削減するアプローチが取られていましたが、上記のように情報損失や効率の限界といった課題が残されていました。

## 2. どのようなアプローチでそれを解決しようとしたか

本論文では、トークン圧縮とは異なるアプローチで、長尺ビデオの効率的な処理を目指しました。具体的には、ハイブリッドな Mamba-Transformer モデル (VAMBA) を提案し、以下の戦略を取りました。

*   **Mamba-2 ブロックの導入**: ビデオトークンのエンコードに、計算量が線形である Mamba-2 ブロックを使用することで、長尺ビデオにおける計算コストを削減します。これにより、トークン数を削減せずに、より多くのフレームを処理できるようになります。
*   **Cross-Attention によるテキストトークンの更新**: テキストトークンをビデオトークンに基づいて更新するために、cross-attention を使用します。テキストトークンはビデオトークンと比較して数が少ないため、cross-attention の計算コストは比較的低く抑えられます。
*   **ハイブリッドアーキテクチャ**: Transformer の self-attention を完全に置き換えるのではなく、テキストトークンに対しては self-attention を維持しつつ、ビデオトークンに対しては Mamba-2 ブロックを使用するハイブリッドアーキテクチャを採用しました。これにより、既存の Transformer ベースの LMM の表現力を維持しつつ、効率的な処理を実現します。

## 3. 結果、何が達成できたのか

提案手法 VAMBA により、以下の成果が達成されました。

*   **GPUメモリ使用量の削減**: 長尺ビデオ入力に対して、TransformerベースのLMMと比較してGPUメモリ使用量を50%以上削減。
*   **学習速度の向上**: 学習ステップあたりの処理速度がほぼ2倍に向上。
*   **精度の向上**: hour-long のビデオ理解ベンチマーク LVBench において、既存の効率的なビデオLMMと比較して4.3%の精度向上。
*   **多様なタスクでの高い性能**: 長尺および短尺ビデオ理解タスクの両方において、高い性能を維持。
*   **より長いシーケンスの処理**: 単一のGPU上で、Transformerベースのモデルが256フレームしかエンコードできないのに対し、VAMBAは1024フレーム以上をエンコード可能。
*   **より少ないGPUでの学習**: LongVUなどの他の効率的なビデオLMMがトレーニングに多数のGPUを必要とするのに対し、VAMBAは8XA100 GPUで効率的にトレーニング可能。

これらの成果は、VAMBA が長尺ビデオ理解において、計算効率と精度を両立できることを示しています。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

### 本文で言及されているLimitation

*   **Mamba のハードウェア最適化**: Mamba アーキテクチャは比較的新しいため、Transformer ほどハードウェア最適化が進んでおらず、理論的な計算効率を最大限に発揮できない可能性があります。
*   **クロスアテンションのみへの依存**: クロスアテンションのみに依存すると、ビデオトークンの更新が不十分になり、モデルの表現力が低下する可能性があります。

### その他のLimitationと問題点

*   **ハイブリッドアーキテクチャの複雑さ**: Transformer と Mamba-2 を組み合わせたハイブリッドアーキテクチャは、設計とトレーニングが複雑になる可能性があります。それぞれのブロックの特性を理解し、適切に組み合わせる必要があります。
*   **長尺ビデオ特化の可能性**: VAMBA は長尺ビデオに特化して設計されているため、短尺ビデオや画像理解タスクにおける性能が最適化されていない可能性があります。
*   **データセットへの依存**: 実験結果は、使用したデータセットに依存する可能性があります。異なるデータセットやタスクにおいては、性能が異なる可能性があります。
*   **汎用性の検証**: HourEvalのような長時間の動画に関する特定の質問に答える能力は高いものの、より一般的な推論や常識的な知識を必要とするタスクでの性能は検証が必要です。
*   **段階的学習**: 二段階学習戦略（事前学習と命令調整）は、複雑であり、各段階で適切な損失関数とハイパーパラメータを調整する必要がある。
*   **Mambaのパラメータ**: Mambaブロックのパラメータ（状態次元のサイズなど）は、最適なパフォーマンスを得るために、特定の動画の特性に合わせて調整する必要がある。
*   **計算リソース**: A100 GPUでのトレーニングは計算集約的であり、リソースが限られた研究者にとっては障壁となる可能性がある。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

VAMBA は、Transformer ベースの LMM の効率を改善するために、Mamba-2 ブロックと cross-attention を導入したハイブリッドアーキテクチャです。

**アーキテクチャ**

1.  **ビジョンエンコーダ**: 事前学習済みの Transformer ベースのビジョンエンコーダ (Qwen2-VL-7B) を使用し、入力ビデオフレームからビジョントークンを生成します。
2.  **LMMデコーダ**: LMMデコーダは、Transformer レイヤと新しい VAMBA レイヤを組み合わせたものです。
    *   **VAMBAレイヤ**: 各VAMBAレイヤは、Mamba-2ブロックとcross-attentionの2つの主要コンポーネントで構成されています。
        1.  **Mamba-2ブロック**: 連続したビデオフレームを効率的に処理するために、標準的なTransformerレイヤにおける自己注意メカニズムを置き換えます。Mamba-2ブロックは、選択的スキャンアルゴリズムを利用して、状態空間モデル（SSM）のパラメータを動的に調整します。
        2.  **クロスアテンション**: テキストトークンは、ビデオトークンに関する文脈情報を取得するために、クロスアテンションメカニズムを使用して更新されます。これにより、テキストトークンはビデオからの関連情報を考慮して、自己注意を計算することができます。
3.  **学習**:
    *   **事前学習**: まず、事前学習済みの Transformer ベースの LMM の重みで初期化し、cross-attention と Mamba-2 レイヤのみを学習します。この段階では、画像キャプションデータを使用して、モデルの視覚理解能力を回復させます。
    *   **命令調整**: 次に、画像およびビデオ命令追従データを使用して、モデル全体を fine-tuning します。GPUメモリの制約がある場合は、ビジョンエンコーダを凍結し、LMM デコーダのみを fine-tuning します。

**計算量の削減**

Transformer レイヤにおける自己注意の計算量は O(d(M+N)^2) ですが、VAMBA では以下のようになります。

*   cross-attention: O(dMN + dN^2)
*   Mamba-2: O(d^2M)

ここで、M はビデオトークン数、N はテキストトークン数、d は隠れ層の次元数です。

VAMBA は、Mamba-2 ブロックによりビデオトークンの計算量を線形に削減し、cross-attention によりテキストトークンの計算量を効率化することで、全体的な計算量を大幅に削減します。

**実装上の注意点**

*   **cross-attention の初期化**: cross-attention の重みを、対応する self-attention レイヤの重みで初期化することが重要です。これにより、学習が安定し、性能が向上します。
*   **Mamba-2 の選択**: Mamba-2 は、Mamba よりも大きな状態次元をサポートし、性能が向上する可能性があります。
*   **ハードウェア最適化**: Mamba アーキテクチャは比較的新しいため、最新のハードウェアで最適に動作させるためには、さらなる最適化が必要です。

**疑似コード**

以下に、VAMBA の主要なコンポーネントの疑似コードを示します。

```python
# Mamba-2 ブロック
def mamba2_block(video_tokens, state):
  """
  Mamba-2 ブロックによるビデオトークンの更新

  Args:
    video_tokens: ビデオトークンの系列 (M, d)
    state: 内部状態 (N, d)

  Returns:
    更新されたビデオトークン (M, d)
  """
  for i in range(len(video_tokens)):
    # 選択的スキャンアルゴリズム
    A, B, C = selective_scan(video_tokens[i], state)
    state = A @ state + B @ video_tokens[i]
    output = C @ state
    video_tokens[i] = output

  return video_tokens

# クロスアテンション
def cross_attention(text_tokens, video_tokens):
  """
  クロスアテンションによるテキストトークンの更新

  Args:
    text_tokens: テキストトークンの系列 (N, d)
    video_tokens: ビデオトークンの系列 (M, d)

  Returns:
    更新されたテキストトークン (N, d)
  """
  Q = linear_projection(text_tokens, W_q)
  K = linear_projection(video_tokens, W_k)
  V = linear_projection(video_tokens, W_v)

  attention_scores = Q @ K.T / sqrt(d)
  attention_weights = softmax(attention_scores)
  updated_text_tokens = attention_weights @ V

  return updated_text_tokens

# VAMBA レイヤ
def vamba_layer(text_tokens, video_tokens, state):
  """
  VAMBA レイヤによるテキストトークンとビデオトークンの更新

  Args:
    text_tokens: テキストトークンの系列 (N, d)
    video_tokens: ビデオトークンの系列 (M, d)
    state: Mamba-2 の内部状態 (N, d)

  Returns:
    更新されたテキストトークン (N, d) とビデオトークン (M, d)
  """
  # Mamba-2 ブロックによるビデオトークンの更新
  updated_video_tokens = mamba2_block(video_tokens, state)

  # クロスアテンションによるテキストトークンの更新
  updated_text_tokens = cross_attention(text_tokens, updated_video_tokens)

  return updated_text_tokens, updated_video_tokens
```

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

*   **GPU**: 8 NVIDIA A800 80G GPUs
*   **モデル初期化**: 事前学習済みQwen2-VL-7Bを教師モデルとした
*   **データセット**:
    *   **事前学習**: CC12M, PixelProse (3M image caption data)
    *   **命令調整**: LLaVA-Video-178K, LLaVA-OneVision (2M image-text data)
*   **ハイパーパラメータ**:
    *   学習率: 事前学習 (1e-5), 命令調整 (5e-6)
    *   バッチサイズ: 128
*   **最適化手法**: Flash-Attention 2, sequence parallelism

**その他**:

*   実験環境の一貫性を保つため、Qwen2-VL-7Bとの比較実験では、ビデオの解像度を同じに設定し、各GPUでバッチサイズ1を維持した。
*   トレーニングのコスト削減のためにFlash-Attention 2などの最適化手法を利用。

## 7. 参考文献のうち、特に参照すべきもの

*   **Mamba: Linear-time sequence modeling with selective state spaces.** この論文は、VAMBA の基盤となる Mamba アーキテクチャについて詳しく解説しています。
*   **Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.** VAMBA のベースラインモデルである Qwen2-VL-7B について解説しています。
*   **Flashattention-2: Faster attention with better parallelism and work partitioning.** トレーニングの高速化に使用されているFlashAttentionについて解説しています。

## 8. この論文を140字以内のツイートで要約すると？

長尺動画理解に革命！VAMBAはMamba-2とCross-AttentionでTransformerの計算量を打破。GPUメモリ50%削減、学習速度2倍、LVBenchで精度4.3%向上！効率と性能を両立した新アーキテクチャ #Vamba #MLLM #LongVideo


---

はい、承知いたしました。以下に、ご質問いただいた内容について詳細な回答をmarkdown形式で記述します。


# TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools

[View Paper](http://arxiv.org/abs/2503.10970v1)

## 1. 既存研究では何ができなかったのか

既存の治療法におけるAIエージェント研究は、以下の点で限界がありました。

*   **リアルタイムな医学知識へのアクセス不足:** 既存のLLMは、学習データに基づいて応答を生成しますが、最新の医学知識（新薬の承認情報、最新の臨床ガイドラインなど）にリアルタイムでアクセスできませんでした。そのため、情報が古く、誤った推奨を行う可能性がありました。
*   **複雑な臨床変数の推論能力の欠如:** 多くの治療選択は、患者の特性、併存疾患、薬物相互作用など、複数の変数を考慮する必要があるため、複雑な多段階の推論が必要です。既存のLLMは、このような複雑な推論を正確に行うことが困難でした。
*   **情報の信頼性の欠如とハルシネーション:** LLMは、学習データに含まれる不正確な情報や誤解を招く情報に基づいて誤った情報を生成することがあります。また、既存のLLMでは、回答の根拠となる情報を検証するメカニズムがなく、ユーザーが信頼性を判断するのが困難でした。
*   **ツール利用における制約:** 既存のツール利用型LLMは、利用できるツール数に制限があり、複雑なタスクに必要な複数ステップのツール選択や反復的な推論を苦手としていました。また、ブランド名、一般名、詳細な記述など、薬物名のバリエーションに対する汎化能力も不足していました。

## 2. どのようなアプローチでそれを解決しようとしたか

これらの課題を解決するために、TxAgentは以下のアプローチを採用しました。

*   **リアルタイムな知識アクセス:** TxAgentは、FDAの薬物ラベルやOpen Targetsなどの信頼できる情報源からリアルタイムに知識を取得するために、211個の専門ツールからなるツールボックス（ToolUniverse）を利用します。
*   **多段階推論:** TxAgentは、患者の特性、併存疾患、薬物相互作用などの複数の変数を考慮した多段階の推論を行います。これにより、より正確で個別化された治療推奨を生成できます。
*   **エビデンスに基づく意思決定:** TxAgentは、ツールから取得した検証済みの情報に基づいて回答を生成します。また、意思決定プロセスの各ステップを透明化する推論トレースを提供することで、ユーザーが回答の信頼性を検証できるようにします。
*   **動的なツール選択:** TxAgentは、タスクの目的に基づいてツールを動的に選択し、必要な情報を取得するために複数のツールを連携させます。Tool Retrieval Model (TRM)を利用し、大規模なツールボックスから適切なツールを検索・選択します。
*   **データの多様性への対応:** TxAgentは、薬物名のバリエーション（ブランド名、一般名、記述）に対する汎化能力を高めるために、多様な表現形式で学習を行います。
*   **マルチエージェントシステム:** APIドキュメントから構造化されたツール仕様を生成するツール構築、質問生成、推論トレース生成の3つの補助エージェントシステムを採用し、トレーニングデータの作成を効率化しました。

## 3. 結果、何が達成できたのか

TxAgentは、以下の点で優れた成果を達成しました。

*   **高い精度:** DrugPCベンチマークにおいて、GPT-4oを25.8%上回る92.1%の精度を達成し、DeepSeek-R1(671B)を上回る構造化された多段階推論を実現しました。
*   **優れた汎化性能:** 薬物名のバリエーション（ブランド名、一般名、詳細な記述）に対する精度変動が非常に低く、既存のツール利用型LLMを大幅に上回る汎化性能を示しました。
*   **個別化された治療推奨:** TreatmentPCベンチマークにおいて、患者固有の治療シナリオにおいてGPT-4oを13.6%上回る性能を達成し、個別化医療における優位性を示しました。
*   **透明性の高い意思決定:** TxAgentは、意思決定プロセスの各ステップを説明する推論トレースを提供することで、ユーザーが回答の信頼性を検証できるようにします。
*   **大規模なツールボックスの活用:** 211個の専門ツールからなる大規模なツールボックスを活用することで、既存のLLMでは困難であった複雑な治療タスクを解決できます。
*   **最新知識の活用:** 常に更新される知識ベースにアクセスすることで、最新の薬物承認情報や臨床ガイドラインを反映した治療推奨を生成できます。

## 4. Limitationや問題点は何か

論文で言及されている制限事項と問題点は以下の通りです。

*   **ツールボックスの制約:** TxAgentはツール呼び出しに依存しているため、ToolUniverseにない特定のデータタイプへのアクセスが制限されています。
*   **内部知識の課題:** 現在のアプローチでは外部ツールを通じて推論の根拠を検証していますが、内部知識とツールからのフィードバックを統合することで、探索的なタスクにおける柔軟性を高めることができると考えられます。
*   **入力モダリティの制限:** TxAgentは自然言語入力のみをサポートしており、病理画像、EHRデータ、Webベースの検査結果などの他のモダリティをまだサポートしていません。

加えて、私が考える問題点としては以下のような点が挙げられます。

*   **計算コスト:** 211個ものツールを管理し、リアルタイムでAPIを呼び出す必要があるため、推論の計算コストが高くなる可能性があります。
*   **APIの依存性:** 外部APIの可用性や信頼性に依存しているため、APIがダウンした場合や変更された場合にTxAgentの性能が影響を受ける可能性があります。
*   **倫理的な考慮事項:** 医療分野での利用であるため、誤った推奨による患者へのリスクを最小限に抑えるための厳格な検証と監視が必要です。
*   **専門家による検証の必要性:** TxAgentの推奨はあくまで補助的なものであり、最終的な治療判断は医師などの専門家が行う必要があります。

## 5. 技術的な詳細について

TxAgentの技術的な詳細について、技術者向けに説明します。

*   **アーキテクチャ:** TxAgentは、多段階推論を行うLLM、Tool Retrieval Model (TRM)、ToolUniverseという3つの主要コンポーネントで構成されています。LLMは、ユーザーの質問を解釈し、推論ステップを生成し、ツールを呼び出す役割を担います。TRMは、クエリに基づいてToolUniverseから適切なツールを検索・選択します。ToolUniverseは、FDAの薬物ラベルやOpen Targetsなどの信頼できる情報源にアクセスするための211個のツールを提供します。
*   **多段階推論:** TxAgentは、各ステップで思考（Thought）とツール呼び出し（Function Call）を生成することで、多段階の推論を実現します。各ステップでは、LLMは過去の推論トレースと利用可能なツールに基づいて、次のアクションを決定します。
*   **ツール検索:** ツール検索には、Tool Retrieval Model（TRM）を使用します。TRMは、埋め込みモデルであり、ツール記述とクエリの間のセマンティックな類似度に基づいてツールを検索します。
*   **ToolUniverse:** ToolUniverseは、APIドキュメントから構造化されたツール仕様を生成するマルチエージェントシステムによって構築されます。このシステムは、APIの機能をツールとして整理し、LLMが理解できる明確な記述を提供します。
*   **学習:** LLMは、質問、推論トレース、ツール呼び出しを含む大規模なデータセットで学習されます。このデータセットは、ツール構築、質問生成、推論トレース生成のための3つの補助エージェントシステムによって生成されます。学習には、Low-Rank Adaptation (LoRA) fine-tuningを使用します。

Python風疑似コードで表現すると、以下のようになります。

```python
# TxAgentの推論プロセス
def txagent_inference(question, tool_universe, trm_model, llm_model):
    reasoning_trace = []
    available_tools = tool_universe.get_initial_tools()  # 初期ツールを取得
    for step in range(MAX_STEPS):
        # 思考の生成
        thought = llm_model.generate_thought(question, reasoning_trace, available_tools)
        # ツールの選択
        selected_tools = trm_model.select_tools(thought, available_tools)
        # ツールの実行
        function_calls = llm_model.generate_function_calls(question, reasoning_trace, selected_tools)
        tool_results = execute_tools(function_calls)
        # 推論トレースの更新
        reasoning_step = {
            "thought": thought,
            "function_calls": function_calls,
            "tool_results": tool_results
        }
        reasoning_trace.append(reasoning_step)

        # 終了判定
        if llm_model.is_final_answer(thought):
            final_answer = llm_model.generate_final_answer(reasoning_trace)
            return final_answer, reasoning_trace
        # 利用可能なツールの更新
        available_tools = tool_universe.update_tools(tool_results)

    return "推論ステップ数の上限に達しました", reasoning_trace

# Tool Retrieval Model (TRM) のツール選択
def select_tools(query, available_tools):
    # 各ツールの埋め込み表現を計算
    tool_embeddings = [embed(tool.description) for tool in available_tools]

    # クエリの埋め込み表現を計算
    query_embedding = embed(query)

    # 類似度を計算し、上位N個のツールを選択
    similarities = [cosine_similarity(query_embedding, tool_embedding) for tool_embedding in tool_embeddings]
    selected_tools = select_top_n(available_tools, similarities, N=5) # 例: 上位5個を選択
    return selected_tools

# LoRAを用いたファインチューニング
def lora_finetuning(model, training_data, lora_rank):
    # モデルの重みを固定
    freeze_parameters(model)

    # LoRAレイヤーを追加
    for layer in model.transformer_layers:
        layer.add_lora_layer(rank=lora_rank)

    # LoRAレイヤーのみを学習
    trainable_parameters = get_lora_parameters(model)
    optimizer = AdamW(trainable_parameters)

    for batch in training_data:
        loss = compute_loss(model, batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    return model
```

## 6. コストや物理的な詳細について

論文に記載されているコストや物理的な詳細を以下に示します。

*   **モデルサイズ:** TxAgentは、80億パラメータのLlama-3.1-8B-Instructモデルをベースにしています。
*   **トレーニングデータセット:** トレーニングデータセットは、378,027のinstruction-tuningサンプルで構成されています。これは、85,340の多段階推論トレースから派生し、177,626の推論ステップと281,695の関数呼び出しを含んでいます。
*   **GPU:** トレーニングには、Harvard UniversityのKempner Institute for the Study of Natural and Artificial Intelligenceが提供するNvidia H100 GPUクラスタを使用しました。
*   **GPUメモリ:** Llama-3.1-8Bモデルのトレーニングには、4つのGPU（合計320GBのGPUメモリ）を使用しました。
*   **分散トレーニング:** モデルの分散トレーニングには、Fully Sharded Data Parallel（FSDP）を使用しました。

ただし、具体的なトレーニング時間やクラウド利用料金などの詳細なコスト情報については、論文に記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下の通りです。

*   **Llama: Open and efficient foundation language models.:** TxAgentのベースモデルであるLlamaのアーキテクチャと性能に関する情報を提供します。
*   **Retrieval-augmented generation for large language models: A survey.:** 検索拡張生成（RAG）の概要と、LLMにおける知識統合の重要性について解説しています。
*   **Openfda: an innovative platform providing access to a wealth of fda’s publicly available data.:** ToolUniverseの重要な情報源であるOpenFDAの詳細な情報を提供します。
*   **The next-generation open targets platform: reimagined, redesigned, rebuilt.:** ToolUniverseのもう一つの重要な情報源であるOpen Targetsの詳細な情報を提供します。
*   **Lora: Low-rank adaptation of large language models.:** ファインチューニングに利用されたLoRAに関する情報を提供します。

## 8. この論文を140字以内のツイートで要約すると？

TxAgent: 211個のツールを駆使し、リアルタイムな医学知識と多段階推論で個別化された治療を提案するAIエージェント。GPT-4o超えの高精度と透明性で、臨床意思決定を支援します！ #AI #医療 #LLM


上記の回答で、ご質問に対する詳細な説明ができていることを願っています。もし追加の質問や不明な点がありましたら、お気軽にお知らせください。


---


# Large-scale Pre-training for Grounded Video Caption Generation

[View Paper](http://arxiv.org/abs/2503.10781v1)

## 1. 既存研究では何ができなかったのか

既存研究では、ビデオ内のオブジェクトを正確に特定し、自然な言語で説明する「Grounded Video Caption Generation」において、以下の点が課題となっていました。

*   **大規模な学習データセットの不足:** 既存のデータセットは、手動でアノテーションする必要があるため、規模が小さく、学習データが不足していました。また、多くは1つのテキスト記述に対して1つの時空間チューブ（オブジェクトの動きを追跡した領域）しかアノテーションされていませんでした。
*   **時間的な一貫性の欠如:** 既存のデータセットは、オブジェクトの動きを追跡するために、ビデオ全体を通して高密度なバウンディングボックスを提供していませんでした。一部のデータセットは、ビデオ内のいくつかのフレームにしかバウンディングボックスを提供していませんでした。
*   **オブジェクトの遮蔽への対応不足:** ビデオでは、オブジェクトが一時的にフレームから外れたり、他のオブジェクトに遮られたりすることが頻繁に起こりますが、既存研究では、これに対する考慮が不足していました。
*   **複数のオブジェクトの同時Groundingの困難さ:** 既存のデータセットやモデルの多くは、1つのキャプションに対して1つのオブジェクトしかGroundingできませんでした。そのため、複数のオブジェクトが同時に存在する複雑なシーンを扱うことが困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

本論文では、上記の問題を解決するために、以下の3つの主要なアプローチを採用しました。

1.  **大規模自動アノテーション:** HowTo100Mデータセットを活用し、自動アノテーションパイプラインを構築しました。このパイプラインは、以下のステップで構成されます。
    *   **フレームごとのGrounded Caption生成:** まず、画像ベースのGrounded Caption生成モデル（GLaMMなど）を使用して、個々のフレームに対してキャプションとバウンディングボックスを生成します。
    *   **ビデオレベルのキャプション集約:** 大規模言語モデル（LLM、Llama-2）を用いて、フレームレベルのキャプションを統合し、ビデオ全体の内容を要約するキャプションを生成します。LLMには、Subject-Verb-Object (SVO) tripletsが入力され、より一貫性のあるビデオレベルのキャプションを生成します。
    *   **時間的に一貫性のあるアノテーション:** LLMを用いて、フレームレベルのオブジェクトとビデオレベルのキャプションを関連付け、時間的に一貫性のあるバウンディングボックスのアノテーションを作成します。
2.  **新しいデータセットiGroundの導入:** 手動でアノテーションされた高品質なデータセットiGroundを新たに作成しました。iGroundは、3500本のビデオと23万個以上のバウンディングボックスのアノテーションを含んでいます。
3.  **GROVEモデルの提案:** Grounded Video Caption Generationのための新しいモデルGROVEを提案しました。GROVEは、以下の主要な技術的要素を備えています。
    *   **時空間アダプター:** ビデオ内の時間的な情報を効率的にモデル化するために、時空間アダプターを導入しました。
    *   **バウンディングボックスデコーダ:** 時間的に一貫性のあるバウンディングボックスを生成するためのバウンディングボックスデコーダを設計しました。
    *   **時間的Objectnessヘッド:** オブジェクトが一時的にフレームから外れたり、遮蔽されたりする場合を明示的にモデル化するための時間的Objectnessヘッドを導入しました。

## 3. 結果、何が達成できたのか

本研究によって、以下の成果が得られました。

*   **大規模な事前学習データセットHowToGround1Mの構築:** 自動アノテーション手法を用いて、100万本のビデオからなる大規模な事前学習データセットHowToGround1Mを構築しました。
*   **新しいデータセットiGroundの導入:** 手動でアノテーションされた高品質なデータセットiGroundを導入しました。
*   **GROVEモデルの提案と性能向上:** 新しいモデルGROVEを提案し、提案したiGroundデータセットだけでなく、既存のVidSTGおよびActivityNet-Entitiesデータセットにおいて、最先端の性能を達成しました。
*   **事前学習の有効性の実証:** HowToGround1Mでの事前学習とiGroundでのファインチューニングの組み合わせが、性能向上に大きく貢献することを示しました。
*   **モデルの主要な技術的要素の有効性検証:** 時空間アダプター、バウンディングボックスデコーダ、時間的ObjectnessヘッドといったGROVEモデルの主要な技術的要素の有効性を実験的に検証しました。

## 4. Limitationや問題点は何か

本研究には、以下の制限事項と問題点が存在します。

*   **自動アノテーションのノイズ:** HowToGround1Mは自動アノテーションによって生成されているため、手動アノテーションと比較してノイズが含まれる可能性があります。
*   **iGroundデータセットの規模:** iGroundは高品質な手動アノテーションデータセットですが、HowToGround1Mと比較すると規模が小さいため、モデルの汎化能力を十分に評価できない可能性があります。
*   **計算コスト:** GROVEモデルは、大規模な事前学習とファインチューニングを必要とするため、計算コストが高くなる可能性があります。
*   **特定のオブジェクトのGroundingの難しさ:** GROVEモデルは、小さすぎる、または背景に溶け込んでいるオブジェクトのGroundingが難しい可能性があります。定性的な結果では「yarn」を正確にGroundingできていない例が示されています。
*   **自動アノテーションにおける複数主体の取り扱い:** 自動アノテーションの手法では、複数の主体が明確に区別できる場合にのみ、正確なアノテーションが可能ですが、区別が難しい場合は、アノテーションが不正確になる可能性があります。
*   **自動アノテーションの依存:** 自動アノテーションの品質は、基盤となる画像キャプションモデルやLLMの性能に大きく依存します。したがって、これらのモデルの弱点が、アノテーションの品質に影響を与える可能性があります。

## 5. 技術的な詳細について

### 5.1 GROVEモデルのアーキテクチャ

GROVEモデルは、Grounded Video Caption Generationを実現するために、以下のコンポーネントで構成されています。

*   **Global Video Encoder (V_e):**
    *   動画全体をグローバルに表現するエンコーダ。事前に学習された画像エンコーダをベースに、時空間アダプターを組み込むことで、動画の時間情報を効率的に処理します。
    *   入力：動画クリップ（Tフレーム、H1 x W1の解像度）
    *   出力：グローバルな特徴マップ（T x H1/p x W1/p、pはパッチサイズ）
    ```python
    def global_video_encoder(video):
        # video: (T, H1, W1, C)  # T: フレーム数, H1: 高さ, W1: 幅, C: チャンネル数
        features = pretrained_image_encoder(video)  # 各フレームに対して画像特徴を抽出
        for i in range(len(features)): # エンコーダの各層に対して時空間アダプタを適用
            if i % 3 == 0:
                features[i] = spatio_temporal_adapter(features[i])
        pooled_features = spatio_temporal_pooling(features) # 時空間プーリング
        return pooled_features
    ```

*   **Grounding Video Encoder (V_g):**
    *   動画内の詳細な情報を表現するエンコーダ。グローバルエンコーダと同様に、事前に学習された画像エンコーダをベースに、時空間アダプターを組み込みます。
    *   入力：動画クリップ（Tフレーム、H2 x W2の解像度、H2 > H1, W2 > W1）
    *   出力：詳細な特徴マップ（T x H2/p x W2/p）
    ```python
    def grounding_video_encoder(video):
        # video: (T, H2, W2, C)  # T: フレーム数, H2: 高さ, W2: 幅, C: チャンネル数
        features = pretrained_image_encoder(video)  # 各フレームに対して画像特徴を抽出
        for i in range(len(features)): # エンコーダの各層に対して時空間アダプタを適用
            if i % 3 == 0:
                features[i] = spatio_temporal_adapter(features[i])
        return features
    ```

*   **Multimodal LLM:**
    *   グローバルな動画の特徴とテキスト情報を組み合わせて、キャプションを生成します。
    *   入力：テキストプロンプトと、グローバルビデオエンコーダからの特徴量
        *   テキストプロンプトの例: "The provides an overview of the video. Could you please give me a description of the video? Please respond with interleaved bounding boxes for the corresponding parts of the answer."
    *   出力：キャプション（オブジェクトに対応する名詞句には特別なトークンが付与される）
    ```python
    def multimodal_llm(video_features, text_prompt):
        # video_features: グローバルビデオエンコーダの出力
        # text_prompt: テキストプロンプト
        multimodal_input = concatenate(text_prompt, video_features)
        caption = llm(multimodal_input)  # LLMによるキャプション生成
        return caption
    ```

*   **Bounding Box Decoder:**
    *   LLMによって生成されたキャプション中の名詞句に対応するバウンディングボックスを予測します。
    *   入力：LLMの出力（キャプション）、Grounding Video Encoderからの特徴量
    *   出力：各フレームにおける各オブジェクトのバウンディングボックス座標
    ```python
    def bounding_box_decoder(llm_output, grounding_features):
        # llm_output: LLMの出力
        # grounding_features: Grounding Video Encoderの出力
        detection_tokens = extract_detection_tokens(llm_output) # LLMの出力から検出トークンを抽出
        queries = project_to_query(llm_output) # LLMの隠れ状態をクエリに変換
        for t in range(grounding_features.shape[0]): # 各フレームに対して処理
            frame_features = grounding_features[t]
            # フレームごとの特徴量とクエリを用いて、バウンディングボックスを予測
            frame_output = cross_attention(queries, frame_features)
        return frame_output
    ```
*   **Temporal Objectness Head:**
    *   各フレームにおいて、特定のオブジェクトが存在するかどうかを予測します。これにより、オブジェクトが一時的にフレームから外れたり、遮蔽されたりする場合を考慮したGroundingが可能になります。
    *   入力：バウンディングボックスデコーダの出力
    *   出力：各フレームにおける各オブジェクトの存在確率
    ```python
    def temporal_objectness_head(decoder_output):
        # decoder_output: バウンディングボックスデコーダの出力
        objectness_scores = linear_layer(decoder_output) # 線形層による存在確率の予測
        return objectness_scores
    ```

### 5.2 時空間アダプター

時空間アダプターは、事前に学習された画像エンコーダに時間的な情報を組み込むためのモジュールです。3D畳み込み層を使用し、時間方向と空間方向の両方に対して畳み込みを行います。
```python
def spatio_temporal_adapter(features):
    # features: 入力特徴マップ (T, C, H, W)
    output = conv3d(features)  # 3D畳み込み
    return output
```
### 5.3 損失関数

GROVEモデルの学習には、以下の損失関数を組み合わせて使用します。

*   **言語モデル損失（L_LM）:** LLMによるキャプション生成の損失。クロスエントロピー損失を使用します。
*   **gIoU損失（L_gIoU）:** 予測されたバウンディングボックスと正解バウンディングボックスの間のgIoU（Generalized Intersection over Union）を最大化するための損失。
*   **L1損失（L_L1）:** 予測されたバウンディングボックスの座標と正解バウンディングボックスの座標の間のL1距離を最小化するための損失。
*   **Temporal Objectness損失（L_tobj）:** Temporal Objectnessヘッドによるオブジェクト存在確率の予測の損失。二値クロスエントロピー損失を使用します。
```python
def loss_function(llm_output, bb_pred, tobj_pred, gt_bb, gt_tobj):
    # llm_output: LLMの出力
    # bb_pred: バウンディングボックスデコーダの出力
    # tobj_pred: Temporal Objectnessヘッドの出力
    # gt_bb: 正解バウンディングボックス
    # gt_tobj: 正解オブジェクト存在確率

    l_lm = cross_entropy(llm_output)
    l_giou = giou_loss(bb_pred, gt_bb)
    l_l1 = l1_loss(bb_pred, gt_bb)
    l_tobj = binary_cross_entropy(tobj_pred, gt_tobj)

    total_loss = lambda_lm * l_lm + lambda_giou * l_giou + lambda_l1 * l_l1 + lambda_tobj * l_tobj
    return total_loss
```

## 6. コストや物理的な詳細について

*   **データセット:**
    *   **HowToGround1M:** 100万本のビデオクリップからなる自動アノテーションデータセット。
    *   **iGround:** 3500本のビデオクリップからなる手動アノテーションデータセット（train/val/test = 2000/500/1000）。
*   **ビデオ処理:** HowTo100Mのビデオは可変フレームレート（25-30fps）であり、5fpsで処理されました。クリップの長さは主に8秒で、空間解像度は455x256ピクセルでした。
*   **実装:** 実装の詳細はsupplimentary materialに記載。
*   **トレーニング:** 訓練エポック数は20、バッチサイズは128。学習率は 5e-5。
*   **モデルサイズ:** モデルサイズに関する具体的な数値は記載されていません。
*   **GPU:** GPUに関する詳細は記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

*   **Flamingo:** 視覚言語モデルの基礎となるモデル。
*   **GLaMM:** フレームごとのGrounded Caption生成に使用されるモデル。本研究では、このモデルを拡張してビデオに応用しています。
*   **Llama 2:** ビデオレベルのキャプション集約に用いられる大規模言語モデル。
*   **HowTo100M:** 大規模なビデオデータセット。HowToGround1Mを構築する際に使用されています。

## 8. この論文を140字以内のツイートで要約すると？

動画キャプション生成に新風！大規模自動アノテーションでHowToGround1Mを構築し、新モデルGROVEでSOTA達成。iGroundデータセットも公開。事前学習+ファインチューニングが鍵！ #VideoCaptioning #GroundedCaptioning #AI


---


# ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy

[View Paper](http://arxiv.org/abs/2503.06542v1)

## 1. 既存研究では何ができなかったのか

既存のUnified Models (UniMs) は、マルチモーダルな理解と生成の両方の能力を同時に学習するように設計されています。しかし、このアプローチには以下の問題点がありました。

*   **計算リソースの要求**: マルチモーダル理解と生成を同時に学習するため、膨大な計算リソースが必要となり、モデルのスケールアップやカスタマイズが困難でした。
*   **text-imageのinterleaved生成の困難**: 多くのUniMsは、テキストと画像を自然に織り交ぜたinterleavedなコンテンツを生成することが苦手でした。
*   **理解能力の低下**: 既存研究では、生成能力を獲得する際に、元々持っていた理解能力が損なわれる可能性がありました。

## 2. どのようなアプローチでそれを解決しようとしたか

ARMORは、これらの課題を解決するために、以下の3つの主要なアプローチを採用しました。

1.  **非対称エンコーダー・デコーダーアーキテクチャ**: 既存のMultimodal Large Language Models (MLLMs) に、forward-switching機構を備えた非対称エンコーダー・デコーダーアーキテクチャを導入しました。これにより、テキストと画像の埋め込み空間を統合し、最小限の計算オーバーヘッドで自然なテキストと画像のinterleaved生成を可能にしました。
2.  **高品質なinterleavedデータセットの構築**: MLLMsのファインチューニングのために、細心の注意を払ってキュレーションされた高品質なinterleavedデータセットを収集しました。具体的には以下のデータセットを作成、または選定しました。
    *   t2t: 標準的な対話形式の質問応答データセット (100K)。
    *   ti2t: 画像理解タスクで、画像を与えられた後にテキストで説明を生成するデータセット(100K)。ShareGPT4Vから選定。
    *   t2i: テキストから画像を生成するタスクのデータセット (2.5M)。自作データとLAION-nolang-aesthetics-27Mから選定。
    *   t2ti: テキストと画像を組み合わせた応答を必要とする対話形式のデータセット (2.5M)。

3.  **"What or How to Generate" (WoHG)アルゴリズム**: 収集されたデータセットに基づいて、3つの段階的なトレーニングステージを通じて、既存のMLLMsにマルチモーダル生成能力を付与しつつ、マルチモーダル理解能力を維持するためのアルゴリズムを提案しました。

    *   **Stage 1 (What to generate)**: MLLMsに応答のモダリティ（テキストまたは画像）を決定させる。t2t, ti2t, t2i, t2tiのデータセットを使用(各100K)。損失関数の重みを調整。
    *   **Stage 2 (How to generate)**: MLLMsの画像生成能力を特異的に向上させる。t2i, t2tiのデータセットを使用(各2.5M)。損失関数の重みを調整。
    *   **Stage 3 (How to answer better)**: テキストと視覚モダリティをより良く統合するためにMLLMsの応答を洗練させる。t2t, ti2t, t2i, t2tiのデータセットを使用(各50K)。損失関数の重みを調整。

## 3. 結果、何が達成できたのか

ARMORは、限られたトレーニングリソースを使用して、既存のMLLMsを有望な画像生成能力を備えたUniMsにアップグレードすることに成功しました。

*   **マルチモーダル理解の向上**: 9つのベンチマークにおける実験結果は、ARMORがマルチモーダル理解において既存のUniMsを大幅に上回ることを示しました（例：Oursは78.8、Janus-proは62.6）。また、InternVL2.5の理解能力の95%以上を維持しています。
*   **マルチモーダル生成の同等な性能**: マルチモーダル生成においては、既存のモデルと同等の性能を達成しました（例：Oursは0.37、Chameleonは0.39）。
*   **リソース効率**: InternVL2.5のファインチューニングに必要なパラメータは7%増加しただけですが、既存のUniMsはゼロから全てのパラメータをトレーニングする必要があります。
*   **interleavedテキスト画像生成の実現**: ARMORは、画像、テキスト、またはinterleavedテキスト画像を出力できます。

## 4. Limitationや問題点は何か

*   **データセットへの依存**: ARMORの性能は、トレーニングに使用するinterleavedデータセットの品質に大きく依存する可能性があります。データセットの偏りやノイズがモデルの性能に悪影響を与える可能性があります。
*   **タスクの多様性**: 評価実験では、特定のマルチモーダル理解および生成ベンチマークに焦点が当てられています。ARMORの汎化能力は、評価されていない他のタスクやドメインでは異なる可能性があります。
*   **制御の限界**: forward-switching機構によりテキストと画像の生成パスを制御できますが、生成されるコンテンツの細かな制御や編集は難しいかもしれません。例えば、特定の位置に特定のオブジェクトを配置するような指示を出すことは困難であると考えられます。
*   **モデルサイズ**: 既存のMLLMをベースにしているため、MLLM自体のサイズがARMORの性能に影響を与える可能性があります。より大きなMLLMを使用することで性能向上が期待できますが、計算コストも増加します。
*   **評価指標**: GenEvalプラットフォームで評価していますが、テキストから画像を生成するタスクの品質と精度を客観的に評価するための完璧な評価指標は存在しません。主観的な評価も重要になります。

## 5. 技術的な詳細について

ARMORの技術的な詳細は以下の通りです。

1.  **モデルアーキテクチャ**:

    *   既存のMLLM（InternVL2.5）をベースに、非対称エンコーダー・デコーダーアーキテクチャを導入。エンコーダーとデコーダーはMLLMのものをそのまま使用し、画像デコーダーを新たに追加。これにより、MLLMの強力な意味認識と理解能力をほぼ維持しつつ、最小限の計算オーバーヘッドで画像生成を可能にする。

    *   forward-switching機構を導入し、モデル入力に基づいて、テキストまたは画像のどちらの答え空間を予測に使用するかを動的に制御。コードで示すと以下のようになります。

        ```python
        def forward(self, input_ids, images=None):
            # input_ids: テキストトークンIDのシーケンス
            # images: 画像特徴量 (オプション)

            # MLLMのエンコーダーでテキストと画像を処理
            hidden_states = self.encoder(input_ids, images)

            # 特殊トークンに基づいて出力ヘッドを切り替え
            if contains_image_token(input_ids):
                # 画像トークンが含まれている場合、画像出力ヘッドを使用
                logits = self.image_decoder(hidden_states)
            else:
                # それ以外の場合、テキスト出力ヘッドを使用
                logits = self.text_decoder(hidden_states)

            return logits
        ```

2.  **トレーニングアルゴリズム**:

    *   "What or How to Generate" (WoHG)アルゴリズムを使用。これは、以下の3つの段階で構成される段階的なトレーニングアプローチです。

        *   **Stage 1: What to generate**: 応答のモダリティを決定する能力を学習。

            ```python
            # 損失関数の計算
            loss_text = calculate_text_loss(logits, labels, mask_text)
            loss_image = calculate_image_loss(logits, labels, mask_image)
            loss = alpha * loss_text + beta * loss_image
            ```

        *   **Stage 2: How to generate**: 画像生成能力を向上させる。
        *   **Stage 3: How to answer better**: テキストと画像間の相乗効果を高める。

3.  **損失関数**:

    *   テキスト予測と画像予測のために、異なる損失関数を使用。ラベルマスクを使用して、テキストまたは画像予測の損失を個別に計算。

        ```python
        # テキスト予測の損失関数
        def calculate_text_loss(logits, labels, mask):
            # logits: モデルの出力
            # labels: 正解ラベル
            # mask: テキストトークンを示すマスク

            masked_logits = logits[mask]
            masked_labels = labels[mask]
            loss = cross_entropy(masked_logits, masked_labels)
            return loss

        # 画像予測の損失関数 (同様の構造)
        def calculate_image_loss(logits, labels, mask):
            ...
        ```

## 6. コストや物理的な詳細について

論文には具体的なGPUの数やトレーニング時間に関する記述はありません。しかし、以下の情報からある程度の推測が可能です。

*   **リソース効率**: ARMORは、既存のMLLMsをアップグレードすることでUniMsを構築するため、ゼロからモデルをトレーニングするよりもリソース効率が高いとされています。
*   **パラメータ数**: モデルの修正により、トレーニング可能なパラメータが0.7B追加されたと記載されています。これは、既存のMLLMと比較して大幅な増加ではないことを示唆しています。
*   **データセット**: データセットサイズから、学習にはある程度の計算資源が必要になると考えられます。

上記の要素を考慮すると、ARMORのトレーニングには、高性能GPUを複数台使用し、数日から数週間程度の時間が必要となる可能性があります。

## 7. 参考文献のうち、特に参照すべきもの

*   **InternVL**: ARMORのベースとなるMLLMであるInternVLについて詳しく知ることができます。
*   **Chameleon**: 事前学習済みのVQGANデコーダーを使用しており、その詳細について知ることができます。
*   **BLIP-2**: 視覚特徴をテキストにアラインメントさせる手法として、Q-Formerが紹介されています。

## 8. この論文を140字以内のツイートで要約すると？

ARMOR: 既存MLLMを活かし、低コストでマルチモーダル理解&生成を実現！非対称アーキテクチャと独自学習で、テキストと画像の自然なinterleaved生成が可能に。リソース効率◎ #AI #Multimodal #MLLM


---


# ReCamMaster: Camera-Controlled Generative Rendering from A Single Video

[View Paper](http://arxiv.org/abs/2503.11647v1)

## 1. 既存研究では何ができなかったのか

既存研究では、与えられた動画のカメラ軌道を自由に変更することが十分に研究されていませんでした。特に以下の点が課題でした。

*   **複数フレームにおける外観の一貫性と動的な同期の維持:** カメラ軌道を変更する際に、動画全体を通して被写体の外観を維持し、動きのタイミングを合わせることが困難でした。
*   **実世界の動画への適用:** 既存の手法は、特定のドメイン（例：シミュレーターで生成された動画）に特化しており、実世界の複雑な動画への汎化が難しかったです。
*   **per-video最適化の必要性:** 一部の手法は動画ごとに最適化が必要で、実用的な応用が制限されていました。
*   **単一動画からの4D再構成の困難さ:** 明示的な4D再構成に基づく手法は、単一動画からの正確な4D再構成が難しく、性能が制限されていました。
*   **汎用的なテキスト-動画生成モデルの活用不足:** 既存研究では、事前学習済みのテキスト-動画生成モデル（T2Vモデル）の潜在的な能力（特に強力な動画条件付けメカニズム）が十分に活用されていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

ReCamMasterでは、以下の要素を取り入れることで、既存研究の課題を克服しようとしました。

*   **事前学習済みT2Vモデルの活用:** 強力な動画条件付けメカニズムを通じて、事前学習済みのT2Vモデルの生成能力を活用しました。これにより、外観の一貫性と動的な同期を維持しつつ、新しいカメラ軌道で動画を再レンダリングできます。
*   **大規模なマルチカメラ同期動画データセットの構築:** Unreal Engine 5を使用して、現実世界の撮影特性を模倣した大規模なマルチカメラ同期動画データセットを構築しました。このデータセットは、多様なシーンとカメラの動きをカバーしており、モデルが実世界の動画に汎化するのを支援します。
*   **綿密に設計された学習戦略:** 多様な入力に対するモデルのロバスト性を向上させるために、入念に設計された学習戦略を採用しました。具体的には、空間-時間(3D)注意レイヤーのみをファインチューンし、他のパラメータをフリーズしました。さらに、学習中に条件付き動画の潜在表現にノイズを追加し、合成データと実世界データのドメインギャップを縮小しました。
*   **フレーム次元連結による動画条件付け:** ソース動画のトークンとターゲット動画のトークンをフレーム次元で連結する新しい動画条件付け手法を提案しました。これにより、条件付きトークンとターゲットトークン間の時空間的な相互作用を促進し、動画ペア間の相関関係をよりよく理解できるようになります。

## 3. 結果、何が達成できたのか

ReCamMasterによって、以下の点が達成されました。

*   **最先端技術を大幅に上回る性能:** 既存の最先端手法や強力なベースラインを大幅に上回る性能を達成しました。
*   **実世界の動画に対する汎化:** 現実世界の動画に対して、新しいカメラ軌道で動的なシーンを再生成できることを示しました。
*   **多様な応用:** 動画の手ぶれ補正、超解像、アウトペインティングなど、さまざまな応用において有望な結果を示しました。
*   **高品質なマルチカメラ同期動画データセットの公開:** カメラ制御された動画生成、4D再構成、および関連分野の研究を促進するために、高品質なマルチカメラ同期動画データセットを公開しました。

## 4. Limitationや問題点は何か

論文で言及されている制限事項は以下の通りです。

*   **計算コスト:** ソース動画とターゲット動画のトークンを連結することで生成品質は向上するものの、計算コストが増加します。
*   **T2Vモデルの制限の継承:** ベースとなるT2Vモデルの制限（例：手の生成の品質が低い）を受け継いでいます。

さらに、私が考える制限事項/問題点は以下の通りです。

*   **データセットのリアリティ:** Unreal Engine 5で生成されたデータセットは、現実世界の動画と完全に一致するわけではありません。特に、複雑な照明条件や、現実世界の動画に特有のアーティファクト（例：モーションブラー）を完全に再現することは難しいかもしれません。
*   **カメラ軌道の複雑さ:** 論文では、明瞭にカメラの動きの軌跡を捉えるために、複雑な軌跡に沿って生成された結果を示していません。複雑な軌跡を生成した場合の性能は未知数です。
*   **カメラパラメータの推定:** 推論時に、入力動画の正確なカメラパラメータを必要とします。実世界の動画では、これらのパラメータを正確に推定することが難しい場合があります。
*   **処理時間:** 高解像度の動画を処理する場合、処理時間が長くなる可能性があります。リアルタイムでの応用には、さらなる最適化が必要になるかもしれません。

## 5. 技術的な詳細について

ReCamMasterは、以下の主要な技術要素で構成されています。

1.  **ベースとなるT2V拡散モデル:** トランスフォーマーベースの潜在拡散モデルをベースとして使用します。動画をピクセル空間から潜在空間に変換するために3D-VAEを使用し、その潜在空間でトランスフォーマーベースの動画拡散モデルを構築します。従来の1D時間Attentionモジュールの代わりに、3D自己Attentionを使用することで、高品質で一貫性のある動画生成を実現しています。

    ```python
    # 拡散過程の疑似コード (Rectified Flow)
    def forward_process(z0, epsilon, t):
        """
        z0: データ分布からのサンプル
        epsilon: 標準正規分布からのノイズ
        t: タイムステップ (0から1)
        """
        zt = (1 - t) * z0 + t * epsilon
        return zt
    ```

2.  **動画条件付けメカニズム:** ソース動画の情報を効果的に活用するために、フレーム次元連結を提案します。ソース動画とターゲット動画の潜在表現をパッチ化し、それらをフレーム次元で連結します。

    ```python
    def frame_dimension_conditioning(source_video, target_video):
        """
        source_video: ソース動画の潜在表現
        target_video: ターゲット動画の潜在表現
        """
        # パッチ化
        source_tokens = patchify(source_video)
        target_tokens = patchify(target_video)

        # フレーム次元での連結
        input_tokens = concat(source_tokens, target_tokens, dim="frame")
        return input_tokens
    ```

3.  **カメラエンコーダ:** ターゲットカメラ軌道 (回転行列と並進行列) をモデルに入力するために、学習可能なカメラエンコーダを使用します。

    ```python
    class CameraEncoder(nn.Module):
        def __init__(self, input_dim=12, output_dim=768): # 例
            super().__init__()
            self.fc = nn.Linear(input_dim, output_dim) # 全結合層

        def forward(self, camera_trajectory):
            """
            camera_trajectory: ターゲットカメラ軌道 (f x (3x4))
            """
            encoded_features = self.fc(camera_trajectory)
            return encoded_features
    ```

4.  **学習戦略:** モデルの汎化能力と生成能力を向上させるために、以下の学習戦略を採用します。
    *   空間-時間(3D)注意レイヤーのみをファインチューンし、他のパラメータをフリーズ。
    *   学習中にソース動画の潜在表現の一部をドロップアウト。
    *   T2V、I2V、V2Vのカメラ制御された生成タスクを統一的に学習。

## 6. コストや物理的な詳細について

論文中には、使用したGPUの数やトレーニング時間など、詳細なコストに関する記述はありません。しかし、以下のような推測が可能です。

*   **データセット:** 136K個の動画、40種類の3D環境、122K種類のカメラ軌道を含む大規模なデータセットを使用しています。
*   **モデルサイズ:** ベースとなるT2Vモデルのサイズは不明ですが、テキスト-動画生成モデルは一般的に大規模なパラメータ数を持つため、ReCamMasterも比較的大きなモデルであると推測できます。
*   **トレーニング:** モデルは10Kステップで、384x672の解像度で学習されています。バッチサイズは40、学習率は0.0001です。
*   **GPU:** 大規模な動画データを扱うため、複数の高性能GPU（例：NVIDIA A100, V100など）を使用している可能性が高いです。
*   **時間:** 10Kステップのトレーニングには、GPUの性能やバッチサイズにもよりますが、数日〜数週間程度の時間がかかる可能性があります。

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下のようなものが考えられます。

*   **AnimateDiff, MotionCtrl, CameraCtrl, CVD, AC3D:** これらの論文は、カメラパラメータを動画生成モデルに組み込む方法を扱っており、ReCamMasterの基礎となる研究です。
*   **Recapture:** ユーザー提供の動画に対して、生成的な動画カメラ制御を行う研究であり、ReCamMasterと直接的に関連しています。
*   **Stable Video Diffusion, VideoCrafter2:** 大規模な動画生成モデルに関する研究であり、ReCamMasterのベースとなるT2Vモデルの選択に影響を与えている可能性があります。
*   **Esser et al. (Scaling Rectified Flow Transformers for high-resolution image synthesis):** Rectified Flowフレームワークは、ReCamMasterのノイズスケジュールとデノイズ処理の基盤となっています。
*   **各動画安定化、超解像、アウトペインティングに関する参考文献:** ReCamMasterの応用可能性を示すために参照されています。

## 8. この論文を140字以内のツイートで要約すると？

ReCamMaster: 動画からカメラ制御で再レンダリング！既存研究では難しかった外観の一貫性や実写への適用を実現。Unreal Engine 5製のデータセットと新動画条件付けで高品質生成。手ぶれ補正、超解像にも応用可能！


---


# Exploring the Vulnerabilities of Federated Learning: A Deep Dive into Gradient Inversion Attacks

[View Paper](http://arxiv.org/abs/2503.11514v1)

## 1. 既存研究では何ができなかったのか

既存研究は、連合学習(FL)におけるGradient Inversion Attacks (GIA)の詳細な分析、評価、およびまとめが不足していました。多くのGIA手法が提案されているにもかかわらず、その有効性や限界要因を明らかにするための大規模な実験的調査が不足していました。既存のサーベイ論文はFLにおけるプライバシー攻撃を概説していましたが、GIAの性能に影響を与える要因や、その実用性、潜在的な脅威に関する深い洞察を提供できていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、以下の3段階のアプローチでこの問題を解決しようとしました。

1.  **系統的なレビューと分類:** 既存のGIA手法を詳細に調査し、`optimization-based` (OP-GIA)、`generation-based` (GEN-GIA)、`analytics-based` (ANA-GIA)の3つのタイプに分類しました。
2.  **包括的な分析と評価:** 分類された3つのタイプのGIAについて、FL環境における性能、実用性、潜在的な脅威に影響を与える要因を分析し、評価しました。
3.  **防御パイプラインの提案:** より良いプライバシー保護のために、FLフレームワークとプロトコルを設計する際に利用できる3段階の防御パイプラインを提案しました。

## 3. 結果、何が達成できたのか

本研究により、以下の成果が達成されました。

*   GIAに関する系統的なレビューと分類を提供しました。
*   OP-GIAが最も実用的な攻撃設定であるものの、性能は不十分であること、GEN-GIAは依存関係が多く、ANA-GIAは検出が容易であるため、どちらも非現実的であることを明らかにしました。
*   FLフレームワークの設計におけるプライバシー保護のための防御戦略を提案しました。
*   攻撃者と防御者の両方の視点から、今後の研究方向性を示しました。

## 4. Limitationや問題点は何か

本研究の限界点と問題点は以下の通りです。

*   **実験的な検証の範囲:** Abstractのみから判断すると、使用したデータセット、モデルアーキテクチャ、具体的な攻撃シナリオなどの詳細が不明であり、実験結果の一般化可能性が制限される可能性があります。
*   **OP-GIAの性能:** OP-GIAが最も実用的であると結論付けていますが、その性能は不十分であると指摘されています。この性能を改善するための具体的な解決策や、他のより実用的な攻撃手法の探索は今後の課題となります。
*   **GEN-GIAとANA-GIAの実用性:** GEN-GIAとANA-GIAは非現実的であると結論付けていますが、特定の条件下では有効となる可能性も考慮に入れる必要があります。例えば、敵対者がより多くの情報を利用できる場合や、防御側が脆弱な実装を行っている場合などが考えられます。
*   **防御パイプラインの有効性:** 提案された防御パイプラインの有効性に関する実験的な検証が不足している可能性があります。さまざまな攻撃シナリオやデータセットに対して、防御パイプラインのロバスト性を評価する必要があります。
*   **今後の研究方向性の具体性:** 示された今後の研究方向性が抽象的である可能性があります。より具体的な研究課題や、それらを解決するためのアプローチを明確にする必要があります。
*   **ノイズのあるコンテンツ:** 本文がHTMLから抽出されたものであり、ノイズや不要な部分が含まれている可能性があるため、分析の精度に影響を与える可能性があります。

## 5. 技術的な詳細について

本研究で扱われるGIAは、連合学習におけるクライアントの学習済みモデルの勾配情報から、クライアントのプライベートデータを再構築しようとする攻撃です。

*   **OP-GIA (Optimization-based GIA):** 勾配情報と既知のモデル構造を利用して、目的関数を最適化することで入力データを復元します。例えば、以下のPython風疑似コードで表現できます。

```python
def optimize_input(gradient, model, initial_input):
  # 損失関数：復元された勾配と実際の勾配の差を最小化
  def loss(input_data):
    reconstructed_gradient = calculate_gradient(model, input_data)
    return difference(reconstructed_gradient, gradient)

  # 最適化アルゴリズム（例：Adam）を使用して入力データを更新
  optimized_input = optimize(loss, initial_input, learning_rate=0.01)
  return optimized_input
```

*   **GEN-GIA (Generation-based GIA):** 生成モデル（GANなど）を使用して、勾配情報からデータ分布を学習し、それに基づいて入力データを生成します。

```python
def generate_input(gradient, generator_model):
  # 勾配情報を元に潜在ベクトルを生成
  latent_vector = encode_gradient(gradient)

  # 生成モデルを使用して入力データを生成
  generated_input = generator_model.decode(latent_vector)
  return generated_input
```

*   **ANA-GIA (Analytics-based GIA):** 勾配情報の統計的特性を分析し、データに関する情報を推測します。例えば、メンバーシップ推論攻撃などがあります。

```python
def infer_data(gradient, model):
  # 勾配の統計的特性を分析
  statistics = analyze_gradient(gradient)

  # モデルの構造と統計情報からデータを推測
  inferred_data = predict_data(model, statistics)
  return inferred_data
```

## 6. コストや物理的な詳細について

論文のAbstractと内容から、具体的なコストや物理的な詳細（トレーニングに使用したGPUの数、時間、データセット、モデルのサイズなど）に関する情報は得られません。これらの情報は、論文の本文（実験セクションなど）に記載されている可能性があります。

## 7. 参考文献のうち、特に参照すべきもの

Abstractからは参考文献のリストは不明であり、特に参照すべきものを特定できません。ただし、一般的にGIAの研究を理解するためには、以下の論文が重要です。

*   **Deep Leakage from Gradients:** Zhao, B., Mopuri, K. R., & Vedaldi, A. (2020). Deep leakage from gradients. *Advances in Neural Information Processing Systems*, *33*, 16705-16716. (OP-GIAの代表的な論文)

## 8. この論文を140字以内のツイートで要約すると？

FLのGradient Inversion Attacks(GIA)を徹底分析！OP-GIAが実用的だが性能課題。GEN/ANA-GIAは非現実的。3段階防御パイプライン提案。FLのプライバシー保護強化へ！ #FederatedLearning #Privacy #AIsecurity


---


# From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM

[View Paper](http://arxiv.org/abs/2503.10620v1)

## 1. 既存研究では何ができなかったのか

既存のLLMへの音声統合に関する研究は、主に以下の点で課題を残していました。

*   **テキスト性能の維持:** 既存の音声LLM (MLLM) の開発は、音声関連タスクに重点が置かれすぎており、LLM本来のテキスト処理能力が損なわれることが多かった。特に、テキストのみのタスクにおける元のLLMの性能を維持することに焦点を当てた研究は限られていた。
*   **コスト効率:** Spirit-LMなどの既存の手法では、DSU (Discrete Speech Units) とテキストデータを混合してLLMを適応させていたが、高価なDSUから文字起こしへの変換ステップが必要だった。
*   **言語の汎用性:** 既存手法には、特定言語のアライナーの利用を前提とするものがあり、全ての言語への拡張が困難であった。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、これらの課題を解決するために、以下の主要なアプローチを採用しました。

*   **音声の離散化と継続的な事前学習 (CPT):** 音声をDiscrete Speech Units (DSUs) に離散化し、これを追加の「翻訳言語」として扱うことで、既存の多言語LLM (TOWER) に音声情報を統合。
*   **2段階のトレーニング:** CPT (継続的な事前学習) と IT (指示チューニング) の2段階でトレーニングを実施。CPTでは、ASRデータとTOWERのテキストCPTデータの一部を使用し、ITでは、TOWERのMTデータと追加の英語ASR/STデータを使用。
*   **テキスト性能の維持:** CPT段階で、元のLLM (TOWER) のテキストデータの一部を組み込むことで、テキストタスクにおけるLLMの元の性能を維持。
*   **コスト効率の高い入力表現:** 音声アライナーに依存しないDSUを使用することで、高価な文字起こしステップを回避。
*   **再現可能なパイプラインの提供:** モデル、データセット、スクリプトを公開し、コミュニティが再現可能な形で研究を進められるようにした。

## 3. 結果、何が達成できたのか

*   **SPIREの実現:** 既存のテキストLLMであるTOWERに音声処理能力を追加したMLLMであるSPIREを開発。
*   **音声認識と翻訳:** SPIREは、英語の音声入力を文字起こしおよび翻訳できるようになった。
*   **テキスト性能の維持:** 音声処理能力を追加した後も、TOWERの翻訳関連タスクにおける元の性能を維持。
*   **競合モデルとの比較:** ASRおよびSTの結果は、より大規模なデータで訓練された強力な音声中心モデル (Whisper-large-v3) に近い性能を達成。
*   **再現可能なパイプライン:** モデル、データセット、スクリプトを公開し、再現可能な研究を可能にした。

## 4. Limitationや問題点は何か

*   **言語の限定:** 現在、SPIREは英語の音声入力にのみ対応。多言語音声処理への拡張が今後の課題。
*   **評価タスクの限定:** 評価は主にMTおよびASR/STに限定されており、モデルの全体的な性能を完全に評価できていない。LM-harnessのようなテキストベースのベンチマークを使用した評価が今後の課題。
*   **出力の限定:** 現在のモデルは、音声とテキストを入力として処理できるが、テキストしか生成できない。音声生成タスクへの統合が今後の課題。
*   **DSUの長さ:** 実験において、DSUシーケンスの長さがin-context learningの性能を悪化させた可能性がある。
*   **直接ST性能の課題:** ASRとMTのデータでCPTを行っても、直接STへの汎化は十分ではなかった。

その他に考えられる問題点としては、以下のようなものが挙げられます。

*   **DSUの品質:** HuBERT-largeモデルとk-meansクラスタリングによってDSUを生成しているが、このDSUがどの程度音韻的な情報を保持しているか、言語的な特徴を捉えられているかについて更なる分析が必要。
*   **ドメイン適応:** トレーニングデータと評価データのドメインが異なる場合、性能が低下する可能性がある。特に、多様なアクセントや録音環境に対応できるかどうかが重要。
*   **リアルタイム処理:** 論文ではリアルタイム処理に関する記述はないが、実用的なアプリケーションを考えると、リアルタイムでの音声認識・翻訳の性能が重要になる。

## 5. 技術的な詳細について

*   **音声の離散化:**
    *   HuBERT-largeモデルの22層目から連続的な音声表現を抽出。
    *   k-meansクラスタリングを用いて、連続的な音声表現を離散的なクラスタIDのシーケンス (DSU) に変換。
    *   720時間の音声データ (CoVoST-2, VoxPopuli, MLS) でk-meansモデルをトレーニング。
*   **モデルアーキテクチャ:**
    *   TOWERをベースとしたデコーダー専用LLM。
    *   語彙サイズを5000タイプ拡張し、DSUを組み込む。
    *   拡張された語彙の埋め込みは、元の埋め込みの多変量ガウス分布から初期化。
        ```python
        # 疑似コード
        original_embeddings = get_original_embeddings()
        mean = np.mean(original_embeddings)
        covariance = np.cov(original_embeddings)
        scaled_covariance = covariance * 1e-5
        new_embeddings = np.random.multivariate_normal(mean, scaled_covariance, size=(5000,))
        ```
*   **トレーニング:**
    *   CPT: ASRデータ (5Bトークン) とTOWERのテキストデータ (1Bトークン) を混合。
    *   IT: ASR, MT, STのテキストおよび音声指示データを使用。
*   **損失関数:**
    *   論文中に明示的な記述はないが、LLMの一般的な損失関数であるクロスエントロピー損失を使用していると考えられる。
*   **推論:**
    *   貪欲法によるデコーディングを使用。
    *   最大生成トークン数は256。

## 6. コストや物理的な詳細について

*   **トレーニングデータ:**
    *   CPT: 42.5K時間の音声データ。
    *   IT: 0.8K時間のASRデータ (CommonVoice v17), 842時間のSTデータ (FLEURS, Europarl-ST, CoVoST-2), および疑似ラベル付きSTデータ。
*   **ハードウェア:**
    *   CPT: 8 x A100-80GB GPUs (6日間)。
    *   IT: 4 x H100-80GB GPUs (2.7日間)。
*   **ハイパーパラメータ:**
    *   CPT: TOWERと同じハイパーパラメータを使用 (バッチサイズ以外)。有効バッチサイズは2304。
    *   IT: 学習率 7e-6, コサインスケジューラ (ウォームアップ100ステップ), 4エポック, 有効バッチサイズ 576, weight decay 0.01, 最大シーケンス長 4096, AdamWオプティマイザ。
*   **モデルサイズ:**
    *   ベースモデル: Llama-2から開発されたTOWER。具体的なパラメータ数は不明。

## 7. 参考文献のうち、特に参照すべきもの

*   **Touvron et al., 2023. Llama 2: Open foundation and fine-tuned chat models:** SPIREのベースモデルであるTOWERがLlama-2から開発されたため、Llama-2のアーキテクチャとトレーニング方法を理解する上で重要。
*   **Yang et al., 2021. SUPERB: Speech Processing Universal PERformance Benchmark:** 音声処理タスクのベンチマークとしてSUPERBが引用されており、関連するタスクの背景知識を得る上で役立つ。
*   **Radford et al., 2023. Robust speech recognition via large-scale weak supervision:** 大規模な弱教師あり学習による音声認識に関する研究であり、Whisperモデルに関する理解を深める上で有用。
*   **Hsu et al., 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units:** DSUの生成に使用されているHuBERTモデルに関する論文であり、音声表現学習の基礎を理解する上で重要。
*   **Nguyen et al., 2025. Spirit-lm: Interleaved spoken and written language model:** 比較対象として言及されているSpirit-LMに関する論文であり、既存研究の課題を把握する上で役立つ。

## 8. この論文を140字以内のツイートで要約すると？

テキストLLM「TOWER」に音声認識能力を追加した「SPIRE」発表！ 音声を離散化し追加言語として学習させることで、翻訳性能を維持しつつ音声認識・翻訳が可能に。コードも公開！ #LLM #音声認識 #機械翻訳


---


# SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion

[View Paper](http://arxiv.org/abs/2503.11576v1)

## 1. 既存研究では何ができなかったのか

既存の研究は、主に以下の点で限界がありました。

*   **多様なドキュメント形式への対応不足:** 多くの既存研究は、科学論文に焦点を当てており、ビジネス文書、技術レポート、特許、フォームなど、より多様なドキュメント形式への対応が不十分でした。
*   **大規模モデルへの依存または複雑なパイプライン:** 大規模な基盤モデルに依存するか、複数の専門モデルを手作業で組み合わせた複雑なパイプラインを使用しており、計算コストが高く、汎用性に欠けていました。
*   **高品質なオープンアクセスデータセットの不足:** 堅牢なマルチモーダルモデルをトレーニングするための、高品質でオープンアクセスなデータセットが不足していました。特に、コードリスティング、数式、チャートなど、特定の種類のアノテーションが不足していました。
*   **ドキュメント要素間の関係性の欠如:** 既存の手法では、キャプションと図表の関連付けや、リストの階層構造など、ドキュメント内の要素間の関係性を捉えることが困難でした。
*   **要素の局在化の精度:** 既存研究では要素の局在化の精度が低く、コンテンツのグラウンディングや視覚的特徴の抽出をサポートするドキュメント表現の再構築が困難でした。
*   **統一されたドキュメント表現の欠如:** コンテンツ、構造、空間的な位置を全て捉えた、統一的なドキュメント表現を学習・生成することができませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

SmolDoclingは、これらの課題を解決するために、以下の様なアプローチを採用しました。

*   **超小型ビジョン言語モデルの開発:** 256Mパラメータという超小型のビジョン言語モデル（VLM）であるSmolDoclingを開発し、計算コストを大幅に削減しました。
*   **DocTagsという新しいマークアップ形式の導入:** DocTagsは、ドキュメントのすべての要素を、そのコンテキストと位置情報とともに捉えることができる、新しい汎用的なマークアップ形式です。これにより、コンテンツ、構造、空間的な位置を統一的に表現できます。
*   **データセットの拡充:** チャート、テーブル、数式、コード認識のための新しい公開データセットを構築し、既存のドキュメント事前学習データセットを拡張しました。具体的には、DocLayNet-PTを基にDocLayNet v2を構築し、SynthDocNetという合成データセットを作成しました。また、FinTabNetデータセットからチャートを生成し、LaTeXとPygmentsを用いてコードスニペットと数式をレンダリングしました。
*   **カリキュラム学習の採用:** モデルの学習を高速化するために、カリキュラム学習のアプローチを採用しました。まず、DocTagsをトークンとしてトークナイザに組み込み、ビジョンエンコーダを固定して残りのネットワークをトレーニングし、新しい出力形式に適合させました。次に、ビジョンエンコーダを固定解除し、すべてのタスク固有の変換データセットとともにモデルをトレーニングしました。最後に、利用可能なすべてのデータセットを使用してファインチューニングを行いました。
*   **エンドツーエンドのドキュメント変換:** 画像を入力として、テキスト、レイアウト、構造を同時に認識し、DocTags形式で出力するエンドツーエンドのシステムを構築しました。
*   **SmolVLMアーキテクチャの利用:** Hugging FaceのSmolVLM-256Mをベースに、より小さく効率的なモデルを構築しました。具体的には、SigLIPエンコーダを視覚バックボーンとして採用し、アグレッシブなピクセルシャッフル戦略で視覚的特徴を圧縮しました。
*   **指示データセットの作成:** レイアウト要素の抽出やOCRの実行など、多様なタスクを学習させるために、規則ベースの手法とLLMを用いて指示データセットを作成しました。

## 3. 結果、何が達成できたのか

SmolDoclingによって、以下の成果が達成されました。

*   **計算コストの削減:** 最大27倍の大きさの他のビジョン言語モデルに匹敵する性能を発揮しながら、計算コストを大幅に削減しました。A100 GPUでの推論において、わずか0.489 GBのVRAMを使用し、1ページあたり0.35秒で変換を完了します。
*   **多様なドキュメント形式への対応:** ビジネス文書、学術論文、技術レポート、特許、フォームなど、幅広いドキュメント形式に対して、コードリスティング、テーブル、数式、チャートなどのドキュメント要素を正確に再現する能力を示しました。
*   **高精度なドキュメント変換:** コンテンツ、構造、空間的な位置を正確に捉えたDocTags形式で、ドキュメントを変換することができました。
*   **オープンなデータセットの提供:** チャート、テーブル、数式、コード認識のための新しい公開データセットを構築し、研究コミュニティに貢献しました。
*   **優れたテキスト認識精度:** 全ページの転写において、GOT、Nougat、Qwen2.5-VLを大幅に上回るテキスト認識精度を達成しました。
*   **優れた要素局在化精度:** DocLayNetテストセットにおける要素局在化精度において、Qwen2.5-VL-7bを大幅に上回る性能を示しました。
*   **テーブル構造認識の競争力:** FinTabNetテストセットにおいて、低解像度の画像にもかかわらず、他の大規模モデルと遜色ないテーブル構造認識性能を達成しました。
*   **コードリスティング認識の確立:** コードリスティング認識という新しいタスクにおいて、ベンチマーク結果を確立し、将来の評価のための基準を設定しました。
*   **分子画像認識の可能性:** 分子構造の認識において、SmolDoclingがハイレベルな分子特徴を捉える能力を示しました。

## 4. Limitationや問題点は何か

SmolDoclingには、以下の様なLimitationや問題点があります。

*   **要素局在化の課題:** 要素の境界ボックスの再現率が低く、要素局在化の精度がまだ不十分であり、特に複雑なレイアウトやカラフルな要素を含むページでは、その傾向が顕著です。
*   **出力の不完全性:** DocTagsの欠落、構造の誤り（閉じタグの欠落）、ページ内のトークンの無限ループなどの問題が、出力に見られることがあります。これらの問題は、出力の解析やレンダリングを妨げる可能性があります。
*   **小規模モデルの限界:** 大規模モデルと比較して、複雑な推論や知識を必要とするタスクでは性能が劣る可能性があります。
*   **データセットのバイアス:** トレーニングデータセットの偏りにより、特定のドキュメント形式や要素に対する性能が低い可能性があります。
*   **汎用性の課題:** 特殊なドメインやタスクにおいては、専門モデルと比較して性能が劣る可能性があります。
*   **分子構造認識の課題:** 詳細な分子構造の再構築に課題が残っており、専門モデルと比較して性能が劣ります。

**その他に考えられるLimitation**

*   **DocTags形式への依存:** DocTags形式が特殊であるため、他のツールやシステムとの連携が困難な場合があります。
*   **マルチページ対応の限界:** 最大シーケンス長が8,192トークンに制限されているため、非常に長いドキュメントや複雑なレイアウトを持つドキュメントの処理が難しい場合があります。

## 5. 技術的な詳細について

SmolDoclingは、以下の様な技術的特徴を持っています。

*   **アーキテクチャ:**
    *   視覚バックボーン: SigLIP base patch-16/512 (93M parameters)
    *   言語バックボーン: 軽量版のSmolLM-2ファミリー (135M parameters)
    *   ピクセルシャッフル: 512x512の画像パッチを64個の視覚トークンに圧縮する、アグレッシブなピクセルシャッフル戦略を採用。
    *   特殊トークン: サブイメージセパレータ用の特殊トークンを導入し、トークン化効率を向上。
*   **DocTags形式:**
    *   XMLライクな記法で、`<block_type bbox="x1,y1,x2,y2">content</block_type>` のようにドキュメントのブロックタイプ、位置情報、内容を表現。
    *   テーブル構造は、OTSL形式で表現。OTSLタグは、`<c>`, `<tr>`, `<th>` など。
    *   リスト構造は、`<list>` および `<li>` タグで表現。
    *   コードは、`<code language="programming_language">` タグで表現。
    *   数式は、`<formula>` タグで表現。
*   **学習:**
    *   カリキュラム学習: DocTagsをトークンとしてLLMに学習させ、その後、ビジョンエンコーダを固定解除してファインチューニング。
    *   損失関数: 画像からDocTagsシーケンスを生成する際のクロスエントロピー損失。
*   **その他:**
    *   トークナイザ: SentencePieceなどを使用。
    *   データ拡張: 画像の回転、スケール変更、ノイズ付加など。

Python風疑似コードで示すと、以下の様になります。

```python
# モデルの定義 (擬似コード)
class SmolDocling(nn.Module):
    def __init__(self, vision_encoder, language_model, projection_layer):
        super().__init__()
        self.vision_encoder = vision_encoder # SigLIPなど
        self.language_model = language_model # SmolLM-2など
        self.projection_layer = projection_layer # 視覚特徴を言語モデルの空間に射影

    def forward(self, image, prompt):
        # 1. 画像を視覚エンコーダでエンコード
        visual_features = self.vision_encoder(image) # (batch_size, seq_len, feature_dim)
        # 2. 視覚特徴を射影
        projected_visual_features = self.projection_layer(visual_features) # (batch_size, seq_len, language_model_dim)
        # 3. プロンプトをテキストエンコーダでエンコード
        text_features = self.language_model.encode(prompt) # (batch_size, prompt_len, language_model_dim)
        # 4. 視覚特徴とテキスト特徴を連結
        combined_features = torch.cat([projected_visual_features, text_features], dim=1) # (batch_size, seq_len + prompt_len, language_model_dim)
        # 5. 言語モデルでDocTagsを自己回帰的に生成
        output = self.language_model.decode(combined_features) # (batch_size, output_len, vocab_size)
        return output

# DocTagsの生成 (擬似コード)
def generate_doctags(model, image, prompt):
    """
    モデルを使用して、与えられた画像とプロンプトからDocTagsを生成します。
    """
    output = model(image, prompt)
    # モデルの出力をDocTags形式に変換 (デコード)
    doctags = decode_to_doctags(output)
    return doctags

# OTSL形式のテーブル構造の表現 (擬似コード)
def represent_table_in_otsl(table_data):
    """
    テーブルデータをOTSL形式で表現します。
    """
    otsl_representation = ""
    for row in table_data:
        otsl_representation += "<tr>"
        for cell in row:
            if cell.is_header:
                otsl_representation += "<th>" + cell.content + "</th>"
            else:
                otsl_representation += "<c>" + cell.content + "</c>"
        otsl_representation += "</tr>"
    return otsl_representation
```

## 6. コストや物理的な詳細について

*   **トレーニング:**
    *   GPU: NVIDIA A100 80GB * 64基
    *   時間: 1エポックあたり38時間、合計4エポック
    *   オプティマイザ: AdamW
    *   学習率:
        *   初期: 2e-4
        *   固定解除後: 2e-6
    *   勾配クリッピング: 1.0
    *   ウォームアップ率: 0.03
*   **推論:**
    *   GPU: NVIDIA A100
    *   VRAM使用量: 0.489 GB
    *   ページ変換時間: 0.35秒 (1ページあたり)
    *   最大シーケンス長: 8,192トークン
    *   対応ページ数: 最大3ページ
*   **モデルサイズ:** 256Mパラメータ
*   **データセット:**
    *   DocLayNet-PT: 1.4Mページ
    *   DocLayNet v2: 60Kページ (ファインチューニング)
    *   WordScape: 63Kページ
    *   SynthDocNet: 250Kページ
    *   FinTabNet: 90,000テーブル (チャート生成に使用)
    *   SynthCodeNet: 9.3Mコードスニペット
    *   SynthFormulaNet: 5.5M数式

データセットサイズの内訳は以下の通りです。

*   DocLayNet-PT (document pre-training): 1.4M pages
*   Docmatix (DocVQA and full conversion): 1.3M documents
*   DocLayNet v2 (layout and table structure): 76K pages (60K used for fine-tuning)
*   WordScape (text and tables): 63K pages
*   SynthDocNet (diverse layouts): 250K pages
*   PubTables-1M (table structure recognition): Public dataset
*   Charts (chart reconstruction): 2.5M charts (generated from FinTabNet)
*   Code (code recognition): 9.3M code snippets
*   Formulas (formula transcription): 5.5M formulas

## 7. 参考文献のうち、特に参照すべきもの

*   **[36] SmolVLM: Redefining small and efficient multimodal models:** SmolDoclingの基盤となるSmolVLMアーキテクチャについて詳しく解説しています。
*   **[39] DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation:** SmolDoclingの評価に使用されているDocLayNetデータセットについて詳しく解説しています。
*   **[27] Donut: Document understanding transformer without ocr:** OCRフリーのドキュメント理解モデルであるDonutについて解説しています。SmolDoclingと比較する上で参考になります。
*   **[15] BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models:** 大規模言語モデルと画像エンコーダを組み合わせたBLIP-2について解説しています。マルチモーダルモデルの構築において参考になります。
*   **[25] Layoutlmv3: Pre-training for document ai with unified text and image masking:** ドキュメントAIのためのLayoutLMv3について解説しています。SmolDoclingと比較する上で参考になります。
*   **[45] Optimized Table Tokenization for Table Structure Recognition:** テーブル構造認識のためのOTSL形式について詳しく解説しています。

## 8. この論文を140字以内のツイートで要約すると？

超小型VLM「SmolDocling」発表！256Mパラメータで高精度なドキュメント変換を実現。DocTagsで構造・内容・位置を統一表現。新データセットも公開！リソース効率の良いドキュメント理解モデルへの道を開く。 #VLM #ドキュメント理解 #AI


---


# VGGT: Visual Geometry Grounded Transformer

[View Paper](http://arxiv.org/abs/2503.11651v1)

## 1. 既存研究では何ができなかったのか

既存の3Dコンピュータビジョンモデルは、主に以下の点で課題がありました。

*   **タスクの専門化:** 多くのモデルは、単一の3Dタスク（単眼深度推定、新規視点合成など）に特化しており、複数の3D属性を同時に推論する能力がありませんでした。
*   **幾何学的後処理への依存:** 従来のStructure-from-Motion (SfM) 手法やMulti-View Stereo (MVS) 手法は、Bundle Adjustment (BA) のような反復最適化技術に大きく依存しており、計算コストが高く、リアルタイムアプリケーションには不向きでした。近年では、機械学習と組み合わせたVGGSfMのような手法も登場しましたが、複雑さや計算コストは依然として課題でした。
*   **限られた入力ビュー数:** 既存のニューラルネットワークベースの手法の中には、一度に処理できる画像が2枚に限られていたり、複数の画像から再構成するために後処理が必要となるものがありました。
*   **3D表現の学習の欠如:** いくつかの3Dニューラルネットワークは存在するものの、3Dに関するinductive bias（先験的な知識）をほとんど持たず、大量の3Dアノテーション付きデータから学習できるモデルがありませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

VGGTは、これらの課題を解決するために、以下の特徴を持つアプローチを採用しました。

*   **エンドツーエンドのフィードフォワードネットワーク:** 複数の画像を入力とし、カメラパラメータ、点群マップ、デプスマップ、3D点追跡などの主要な3D属性を一度に直接推論するフィードフォワードニューラルネットワークを構築しました。
*   **大規模Transformerアーキテクチャ:** VGGTは、3Dに関するinductive biasを最小限に抑えた大規模なTransformerをベースとしています。フレーム単位のself-attentionとグローバルなself-attentionを交互に行うAlternating Attention (AA)機構を導入することで、異なる画像間の情報統合と各画像内の特徴の正規化を両立させています。
*   **多様なデータセットでの学習:** 大量の3Dアノテーション付きデータセットでモデルを学習させることで、3D構造の理解を深め、汎化性能を高めました。
*   **特徴バックボーンとしての利用:** 事前学習済みのVGGTを特徴バックボーンとして利用することで、動的なビデオでの点追跡や新規視点合成などのダウンストリームタスクの性能を向上させました。

## 3. 結果、何が達成できたのか

VGGTは、複数の3Dタスクにおいて最先端の結果を達成しました。

*   **高速な3D再構成:** 1秒以内に画像を再構成でき、幾何学的最適化技術による後処理を必要とする他の手法よりも優れた性能を発揮しました。
*   **高精度な3D属性推定:** カメラパラメータ推定、多視点深度推定、高密度点群再構成、3D点追跡などのタスクにおいて、最先端の精度を達成しました。
*   **ダウンストリームタスクの性能向上:** 事前学習済みVGGTの特徴バックボーンを使用することで、非剛体点追跡やフィードフォワード新規視点合成などのダウンストリームタスクの性能を大幅に向上させることができました。
*   **汎用性と適応性:** VGGTは、様々なデータセットやシナリオに適応可能であり、特定のタスクに特化したモデルよりも汎用性が高いことを示しました。

## 4. Limitationや問題点は何か

VGGTには、以下の制限事項と問題点があります。

*   **特殊なカメラモデルへの非対応:** 現在のモデルは、魚眼レンズやパノラマ画像などの特殊なカメラモデルをサポートしていません。
*   **極端な回転に対する性能低下:** 極端な入力画像の回転がある場合、再構成の性能が低下します。
*   **大きな非剛体変形への対応困難:** わずかな非剛体運動を含むシーンは扱えますが、大きな非剛体変形を含むシナリオでは失敗します。
*   **計算コスト:** 大規模Transformerモデルであるため、計算リソースを大量に消費します。
*   **学習データへの依存:** モデルの性能は、学習に使用するデータセットの品質と多様性に大きく依存します。

**その他に考えられる問題点:**

*   **Single-view reconstruction**　シングルビューでの再構成は、DUSt3Rのように、画像を複製してペアにする必要のあるシステムと比較して、構造的にサポートされているが、明示的にシングルビューを学習しているわけではないので、学習方法によっては精度が十分ではない可能性がある。
*   **Inductive bias**　3D inductive biasを最小限に抑えたTransformerを使用しているため、学習データに偏りがあると、モデルが特定の3D構造に過剰適合してしまう可能性があります。
*   **照明条件への依存:** 学習データにない照明条件で撮影された画像に対しては、性能が低下する可能性があります。

## 5. 技術的な詳細について

VGGTの技術的な詳細は以下の通りです。

*   **アーキテクチャ:** VGGTは、Alternating Attention (AA)機構を導入したTransformerアーキテクチャをベースとしています。AA機構は、フレーム単位のself-attention層とグローバルなself-attention層を交互に配置することで、異なる画像間の情報統合と各画像内の特徴の正規化を両立させています。Transformerの各ブロックは、DINOv2で使用されているViT-Lモデルと同様に、特徴次元`C = 1024`を持ちます。また、安定した学習のために、QKNormとLayerScaleを使用しています。
*   **入力表現:** 入力画像は、DINOv2モデルによってパッチ化され、トークンに変換されます。各フレームの画像トークンに加えて、カメラパラメータ予測用のカメラトークンと、点追跡用のレジスタトークンが追加されます。
*   **出力予測:** Transformerの出力トークンは、カメラパラメータ、デプスマップ、点群マップ、点追跡特徴の予測に使用されます。カメラパラメータは、追加のself-attention層と線形層からなるカメラヘッドによって予測されます。デプスマップと点群マップは、DPTヘッドによって予測されます。点追跡特徴は、トラッキングヘッドに入力され、点追跡に使用されます。
*   **損失関数:** モデルは、カメラパラメータ損失、デプスマップ損失、点群マップ損失、点追跡損失の加重和として定義される損失関数を最適化するように学習されます。デプスマップ損失と点群マップ損失には、予測の不確実性を考慮したaleatoric uncertainty lossが使用されます。

```python
# Python風の疑似コード

def VGGT(images):
  """
  VGGTモデル
  Args:
    images: 入力画像のリスト (shape: [N, C, H, W])

  Returns:
    camera_params: カメラパラメータのリスト (shape: [N, 9])
    depth_maps: デプスマップのリスト (shape: [N, H, W])
    point_maps: 点群マップのリスト (shape: [N, 3, H, W])
    track_features: 点追跡特徴のリスト (shape: [N, C, H, W])
  """

  tokens = patchify(images) # DINOv2でパッチ化
  tokens = add_camera_tokens(tokens) # カメラトークンを追加
  tokens = add_register_tokens(tokens) # レジスタトークンを追加

  transformer_output = alternating_attention_transformer(tokens) # AA Transformer

  camera_params = camera_head(transformer_output) # カメラパラメータを予測
  depth_maps, point_maps, track_features = dpt_head(transformer_output) # デプスマップ、点群マップ、点追跡特徴を予測

  return camera_params, depth_maps, point_maps, track_features

def loss_function(camera_params, depth_maps, point_maps, track_features,
                    ground_truth_camera_params, ground_truth_depth_maps,
                    ground_truth_point_maps, ground_truth_track_points):
  """
  損失関数
  """
  camera_loss = camera_loss(camera_params, ground_truth_camera_params)
  depth_loss = depth_loss(depth_maps, ground_truth_depth_maps)
  pmap_loss = pmap_loss(point_maps, ground_truth_point_maps)
  track_loss = track_loss(track_features, ground_truth_track_points)

  total_loss = camera_loss + depth_loss + pmap_loss + lambda_track * track_loss
  return total_loss
```

## 6. コストや物理的な詳細について

*   **モデルサイズ:** 約12億パラメータ
*   **学習データセット:** CO3Dv2, DTU, ETH3D, ScanNet, RealEstate10K, Replica, GSO, Objaverseに類似の合成データセットなど、大規模かつ多様なデータセットコレクションを使用。
*   **GPU:** 64台のA100 GPU
*   **学習時間:** 9日間
*   **学習率スケジューラ:** コサイン学習率スケジューラ
*   **入力サイズ:** 入力フレーム、デプスマップ、点群マップは最大次元が384ピクセルにリサイズ。アスペクト比は0.33〜1.0の間でランダム化。
*   **その他:** 勾配ノルムクリッピング（閾値0.25）、bfloat16精度、勾配チェックポイントを使用。

## 7. 参考文献のうち、特に参照すべきもの

*   **Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.** *Attention is all you need.* Advances in neural information processing systems
    *   Transformerアーキテクチャの基礎
*   **Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.** *Emerging properties in self-supervised vision transformers.*
    *   DINO: 自己教師あり学習によるVision Transformerの特性
*   **René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.** *Vision transformers for dense prediction.* Proceedings of the IEEE/CVF international conference on computer vision
    *   Dense predictionのためのVision Transformer
*   **Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud.** *MASt3R-SfM: a fully-integrated solution for unconstrained structure-from-motion.*
    *   関連研究（MASt3R-SfM）
*   **Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny.** *VGGSfM: visual geometry grounded deep structure from motion.*
    *   関連研究(VGGSfM)

## 8. この論文を140字以内のツイートで要約すると？

VGGT: 3Dシーンを高速再構成するTransformerモデルを発表！複数画像からカメラパラメータ、深度、点群を直接予測。幾何最適化不要で高精度！点追跡等、他タスクへの応用も可能。コードは[https://github.com/facebookresearch/vggt](https://github.com/facebookresearch/vggt) で公開中。 #3D #CV #Transformer


---


# API Agents vs. GUI Agents: Divergence and Convergence

[View Paper](http://arxiv.org/abs/2503.11069v1)

## 1. 既存研究では何ができなかったのか

既存研究では、APIベースのLLMエージェントとGUIベースのLLMエージェントの包括的な比較研究が不足していました。具体的には、以下のような点が課題でした。

*   **体系的な比較分析の欠如:** アーキテクチャ、開発ワークフロー、ユーザーインタラクションモデルなど、重要な側面における両者の違いを体系的に分析した研究が不足していました。
*   **ハイブリッドアプローチの検討不足:** 両者の強みを組み合わせるハイブリッドアプローチの可能性や、具体的なシナリオにおける有効性に関する検討が不十分でした。
*   **明確な意思決定基準の欠如:** 実務者や研究者が、プロジェクトの要件、リソース制約、ユーザーエクスペリエンスの目標に基づいて、どちらのエージェントタイプを選択すべきか、または組み合わせるべきかに関する明確な指針がありませんでした。
*   **APIエージェントとGUIエージェントの進化**：LLMベースの自動化技術の継続的な発展が、APIとGUI駆動のエージェントの境界線を曖昧にする可能性が十分に検討されていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、APIベースおよびGUIベースのLLMエージェントの相違点と潜在的な収束を体系的に調査することで、上記の課題を解決しようとしました。具体的には、以下のアプローチを採用しました。

*   **概念的基礎の明確化:** 各パラダイムの概念的な基礎を明確に定義し、両者の違いを明確にしました。
*   **主要な側面における比較分析:** モダリティ、信頼性、効率、可用性、柔軟性、セキュリティ、透明性、人間らしいインタラクション、および保守性といった主要な側面において、詳細な比較分析を行いました。
*   **ハイブリッドアプローチの検討:** 両者の長所を活かし、短所を軽減するハイブリッドアプローチについて検討し、具体的なユースケースを提示しました。
*   **明確な意思決定基準の提案:** プロジェクトの要件、リソース制約、ユーザーエクスペリエンスの目標に基づいて、最適なエージェントタイプを選択するための明確な意思決定基準を提案しました。
*   **ハイブリッドエージェントシステムの具体例の提示**: GUI-onlyソフトウェアに対するAPIラッパー、APIアクションとGUIアクションを管理する統合オーケストレーター、API呼び出しとGUIエージェントを統合するローコードプラットフォームなど、ハイブリッドエージェントシステムの具体例を提示しました。

## 3. 結果、何が達成できたのか

本研究によって、以下の成果が達成されました。

*   **APIベースおよびGUIベースのLLMエージェントの包括的な比較:** 両者の違いと潜在的な収束に関する包括的な理解が得られました。
*   **ハイブリッドアプローチの可能性の明確化:** ハイブリッドアプローチが、より広範なユースケースに対応し、高い効率と人間らしいインタラクションスタイルを実現する可能性が明確になりました。
*   **エージェントタイプ選択のための指針の提供:** 実務者や研究者が、プロジェクトの要件に基づいて最適なエージェントタイプを選択するための明確な指針が得られました。
*   **APIとGUIエージェントの境界線の曖昧化に対する理解の深化:** LLMベースの自動化技術の発展が、APIとGUIエージェントの境界線を曖昧にし、より柔軟で適応性の高いソリューションを可能にするという理解が深まりました。
*   **ハイブリッドモデルの実現方法の具体例の提示**: APIラッパー、統一オーケストレーター、ローコードプラットフォームといった様々な形態での、APIとGUIエージェントを組み合わせたハイブリッドモデルの実現方法の具体例が提示されました。

## 4. Limitationや問題点は何か

本研究には、以下の制限事項と問題点があります。

*   **評価の客観性:** 性能評価は、具体的なタスクと環境に依存します。一般的な評価基準を確立することが難しい場合があります。
*   **技術的な複雑さ:** LLMエージェントの設計と実装には、高度な専門知識が必要です。特に、GUIエージェントのビジュアル認識や操作は、技術的な難易度が高いです。
*   **APIとGUIのトレードオフ**: APIエージェントは効率的だが、利用可能なAPIに制限されます。GUIエージェントは柔軟だが、信頼性と効率が低い可能性があります。
*   **セキュリティ上のリスク**: GUIエージェントは、ユーザーインターフェースへの広範なアクセスにより、意図しない操作や悪用に対するリスクが高まる可能性があります。
*   **メンテナンスの課題**: GUIエージェントは、インターフェースの変更に弱く、メンテナンスコストが高くなる可能性があります。
*   **抽象的な議論**: 具体的な実装の詳細や、特定ドメインにおける応用事例に関する議論が不足している可能性があります。
*   **LLMへの依存**: 両タイプのエージェントはLLMの性能に大きく依存しており、LLMの弱点（幻覚、偏見など）を受け継ぐ可能性があります。

**今後考慮すべき点：**

*   **長期的なメンテナンスコスト:** GUIの変更による影響を軽減するための、より堅牢なGUIエージェントの設計。
*   **セキュリティ対策:** GUIエージェントの権限を制限し、不正な操作を防止するセキュリティメカニズム。
*   **多様なタスクと環境での評価:** さまざまなドメインとタスクにおける、APIエージェント、GUIエージェント、およびハイブリッドエージェントの性能評価。
*   **倫理的な配慮:** LLMエージェントがもたらす可能性のある、偏見や公平性に関する倫理的な問題への対処。

## 5. 技術的な詳細について

APIエージェントとGUIエージェントの技術的な詳細を以下に示します。

### APIエージェント

*   **アーキテクチャ:** LLMをコアとして、API記述（OpenAPI仕様など）に基づいてAPIを呼び出すアーキテクチャ。プロンプトには、APIのエンドポイント、パラメータ、および説明が含まれます。
*   **実装:** Pythonの`requests`ライブラリ、またはJavaScriptの`fetch` APIなどを使用して、HTTPリクエストを送信します。LLMは、どのAPIを呼び出すべきかを決定し、必要なパラメータを抽出します。
*   **例:**
    ```python
    def call_api(api_description, user_query):
        llm_response = llm(f"{api_description}\nUser Query: {user_query}")
        api_name = extract_api_name(llm_response)
        parameters = extract_parameters(llm_response)
        
        if api_name == "search_google":
            url = f"https://www.google.com/search?q={parameters['query']}"
            response = requests.get(url)
            return response.text
        elif api_name == "send_email":
            # ... (メール送信処理)
            pass
        else:
            return "API not found."
    ```
*   **技術的課題:** APIの選択、パラメータの抽出、エラー処理、APIの認証、レート制限への対応。

### GUIエージェント

*   **アーキテクチャ:** LLMをコアとして、スクリーンショットまたはアクセシビリティツリーを解析し、GUI要素を操作するアーキテクチャ。
*   **実装:**
    *   **画像認識:** OpenCV、TensorFlow、PyTorchなどを使用して、GUI要素を検出します。
    *   **操作:** `pyautogui` (Python) や Selenium などのライブラリを使用して、マウスのクリックやキーボード入力をシミュレートします。
    *   **例:**
    ```python
    def interact_with_gui(screenshot, task_description):
        llm_response = llm(f"Screenshot: {screenshot}\nTask: {task_description}")
        element_to_click = extract_element_name(llm_response)
        x, y = find_element_coordinates(screenshot, element_to_click)
        pyautogui.click(x, y)
    ```
*   **技術的課題:** GUI要素の正確な検出、レイアウトの変化への対応、エラー処理、状態管理、操作のシーケンスの計画。

### ハイブリッドエージェント

*   **アーキテクチャ:** APIエージェントとGUIエージェントを統合し、タスクに応じて適切なエージェントを選択するアーキテクチャ。オーケストレーションレイヤーを使用して、API呼び出しとGUI操作を組み合わせます。
*   **実装:**
    *   **ルールベース:** 特定のタスクに対して、APIまたはGUIエージェントを優先するルールを定義します。
    *   **LLMベース:** LLMを使用して、タスクの要件に基づいて最適なエージェントを選択します。
    *   **例:**
    ```python
    def execute_task(task_description):
        if task_requires_api(task_description):
            return call_api(api_description, task_description)
        else:
            screenshot = capture_screenshot()
            return interact_with_gui(screenshot, task_description)
    ```

## 6. コストや物理的な詳細について

論文には、具体的なコストや物理的な詳細（トレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど）に関する記述はありません。これは、研究が概念的な比較分析に焦点を当てており、特定の実装の詳細に踏み込んでいないためと考えられます。

ただし、一般的なLLMエージェントのトレーニングと実行に関連するコストと物理的な詳細を以下に示します。

*   **モデルサイズ:** LLMのパラメータ数は数十億から数千億に及びます。
*   **GPU:** LLMのトレーニングには、複数の高性能GPU（例：NVIDIA A100、H100）が必要です。
*   **トレーニング時間:** モデルのサイズとデータセットのサイズに応じて、数日から数週間かかる場合があります。
*   **データセット:** 大量のテキストデータ（数テラバイト）が必要です。GUIエージェントの場合、GUIのスクリーンショットと操作のデータも必要になります。
*   **推論コスト:** LLMの推論には、GPUまたはTPUが必要です。推論のレイテンシとスループットは、モデルのサイズとハードウェアの性能に依存します。

具体的なコストは、使用するLLM、ハードウェア、クラウドプラットフォーム、およびデータセットによって大きく異なります。

## 7. 参考文献のうち、特に参照すべきもの

論文の参考文献のうち、特に参照すべきものは以下の通りです。

*   **Anytool: Self-reflective, hierarchical agents for large-scale api (Yu Du et al., 2024):** 大規模なAPIを操作するための自己反省的な階層型エージェントについて説明しています。APIエージェントの設計と実装に関する洞察が得られます。
*   **Cogagent: A visual language model for gui agents (Wenyi Hong et al., 2024):** GUIエージェントのための視覚言語モデルについて説明しています。GUIエージェントのビジュアル認識と操作に関する技術的な詳細が記載されています。
*   **Ufo: A ui-focused agent for windows os interaction (Chaoyun Zhang et al., 2024b):** Windows OSとのインタラクションに特化したUIエージェントについて説明しています。GUIエージェントの実装における具体的な課題と解決策が示されています。
*   **A survey on large language model based autonomous agents (Lei Wang et al., 2024a):** LLMベースの自律エージェントに関する包括的な調査です。APIエージェントとGUIエージェントの両方を含む、さまざまなエージェントアーキテクチャの概要が示されています。
*   **Taskweaver: A code-first agent framework (Bo Qiao et al., 2023):** コードを優先するエージェントフレームワークについて説明しています。APIエージェントとGUIエージェントの統合に関する洞察が得られます。

## 8. この論文を140字以内のツイートで要約すると？

APIエージェント vs GUIエージェント徹底比較！🤖💻 APIは効率的だが柔軟性低め、GUIは汎用性高いが不安定。ハイブリッド型で両者の長所を活かすのが鍵🔑 #LLM #AIエージェント #自動化


---

はい、承知いたしました。以下、ご指示のフォーマットに従い、詳細な回答を記述します。


# MaRI: Material Retrieval Integration across Domains

[View Paper](http://arxiv.org/abs/2503.08111v1)

## 1. 既存研究では何ができなかったのか

既存の材料検索手法は、以下の点で課題がありました。

*   **データセットの不足:** 形状不変かつ照明変化のある材料の表現を捉えたデータセットが不足しており、多様性が限られ、現実世界への汎化が困難でした。
*   **材料空間の特性の未考慮:** 従来の画像検索技術をそのまま適用しているため、テクスチャ、反射率、表面の粗さといった材料固有の特性を捉えきれず、検索性能が最適ではありませんでした。
*   **合成データと現実世界データのギャップ:** 合成データのみでは、現実世界の多様でニュアンスのある外観を完全に表現できず、ドメインギャップが生じていました。

## 2. どのようなアプローチでそれを解決しようとしたか

MaRIは、以下の要素を取り入れた新しいフレームワークを導入することで、これらの課題を解決しようとしました。

*   **共有埋め込み空間の構築:** 画像エンコーダと材料エンコーダを共同で訓練し、コントラスト学習戦略を通じて、視覚的属性と材料属性を調和させる共有埋め込み空間を構築しました。これにより、類似した材料と画像を特徴空間内で近づけ、異なるペアを分離します。
*   **包括的なデータセットの構築:** 制御された形状変化と多様な照明条件でレンダリングされた高品質の合成材料と、材料転送技術で処理および標準化された現実世界の材料を含む包括的なデータセットを構築しました。
*   **デュアルエンコーダアーキテクチャの採用:** CLIPに着想を得て、材料と画像の両方のためのジョイント埋め込み空間を学習するMaRIを導入しました。MaRIは、デュアルエンコーダを採用し、材料特性と視覚的特徴を共有空間で整列させるように共同で訓練し、直接的かつ効率的な比較を促進します。
*   **DINOv2モデルの活用:** 材料エンコーダと画像エンコーダの両方のバックボーンとして事前訓練済みのDINOv2モデルを活用することで、一般化可能性を維持しながら、最後のTransformerブロックのみを微調整して、ドメイン固有のニュアンスを効果的に捉えます。

## 3. 結果、何が達成できたのか

MaRIは、以下の成果を達成しました。

*   **優れた検索性能:** 多様で複雑な材料検索タスクにおいて、既存の手法を上回る優れた性能、精度、および汎化能力を実証しました。
*   **ドメインギャップの橋渡し:** 合成データと現実世界データの間のドメインギャップを効果的に橋渡しし、インスタンスレベルおよびクラスレベルの検索タスクにおいて大幅な改善を実現しました。
*   **多様な材料への対応:** より広範な種類の材料にわたって正確にスケーリングできるMaRIの検索パイプラインの能力を示し、複雑で多様な材料タイプに対して正確な検索を実現しました。
*   **定性的な結果:** タイル、レンガ、コケなどのテクスチャの細部やパターンを捉え、Unseen Materialsデータセットから視覚的に類似した材料を特定できることを示しました。

## 4. Limitationや問題点は何か

MaRIのLimitationと問題点は以下の通りです。

*   **データセットへの依存:** MaRIの性能は、構築したデータセットの品質と多様性に大きく依存します。データセットが不十分な場合、性能が低下する可能性があります。
*   **計算コスト:** DINOv2のような大規模な事前学習モデルを使用しているため、トレーニングと推論に高い計算コストがかかる可能性があります。特に、大規模な材料ライブラリを検索する場合、効率的な検索アルゴリズムの開発が重要になります。
*   **材料転送技術の限界:** 現実世界の材料を標準化するために使用される材料転送技術（ZeSTなど）は、完全ではありません。複雑な材料の特性を正確に再現できない場合があります。
*   **クラスレベルの検索性能の飽和:** 合成データのスケールを大きくしても、クラスレベルの検索精度が飽和する傾向があります。これは、カテゴリ情報の飽和が原因と考えられます。
*   **現実世界の複雑な照明条件への対応:** MaRIは、多様な照明条件で訓練されていますが、極端に複雑な照明条件や影の影響を完全に排除できるわけではありません。
*   **新規材料への適応:** Unseen Materialsデータセットで高い性能を示していますが、完全に未知の、データセットに全く含まれない種類の材料への対応は、今後の課題となります。

## 5. 技術的な詳細について

MaRIは、以下の技術的な要素で構成されています。

*   **アーキテクチャ:**
    *   DINOv2をベースとしたデュアルエンコーダ（画像エンコーダ `E_I` と材料エンコーダ `E_M`）
    *   各エンコーダの最終Transformerブロックのみをファインチューニング（その他のパラメータは固定）
*   **データセット:**
    *   合成データセット `D_synthetic`: Objaverseの3Dモデル、AmbientCGのPBRテクスチャ、HDRI照明を使用してBlenderでレンダリング
        *   `obj_normalized = normalize(obj)` # 正規化
        *   `image, mask = render(obj_normalized, material, hdri)` # レンダリング
    *   現実世界データセット `D_real`: オンラインソースから収集された画像、Grounded SAMでセグメンテーション、ZeSTで材料球に変換
        *   `mask = segment(image, material_prompt)` # セグメンテーション
        *   `material_sphere = ZEST(image, mask)` # 材料転送
*   **学習:**
    *   コントラスト学習（InfoNCE損失関数 `L_contrast`を使用）
        *   `sim = cosine_similarity(image_embedding, material_embedding)`
        *   `L_contrast = -log(exp(sim / tau) / sum(exp(sim_all / tau)))`
    *   バッチ内の正のペアの類似度を最大化し、負のペアの類似度を最小化
*   **検索:**
    *   クエリ画像から材料ギャラリー内の最も類似した材料を検索
    *   コサイン類似度を最大化する材料を特定
        *   `best_match = argmax(cosine_similarity(query_embedding, gallery_embeddings))`

## 6. コストや物理的な詳細について

論文中に明示的な記載はありませんが、以下は一般的な推測です。

*   **GPU:** DINOv2のような大規模モデルのファインチューニングには、少なくとも数台の高性能GPU（NVIDIA A100など）が必要と推測されます。
*   **時間:** データセットの規模、モデルの複雑さ、GPUの性能によって異なりますが、数日から数週間程度のトレーニング時間が必要となる可能性があります。
*   **データセット:**
    *   合成データセット: 394,560サンプル
    *   現実世界データセット: 30,000サンプル
*   **モデルサイズ:** DINOv2のモデルサイズは数十GB程度と推定されます。
*   **その他:** データセットの作成には、3Dモデリング、レンダリング、画像処理、セグメンテーションなどのツールと専門知識が必要です。

## 7. 参考文献のうち、特に参照すべきもの

以下の参考文献は、MaRIを理解する上で特に重要です。

*   **Oquab et al. Dinov2: Learning robust visual features without supervision, 2024.** : MaRIのバックボーンとして使用されているDINOv2の論文であり、自己教師あり学習によるロバストな視覚的特徴の学習について理解を深めることができます。
*   **Radford et al. Learning transferable visual models from natural language supervision, 2021.** : MaRIのインスピレーションの元となったCLIPの論文であり、コントラスト学習の概念と、異なるモダリティ間のアライメントについて学ぶことができます。
*   **Cheng et al. Zest: Zero-shot material transfer from a single image, 2024.** : 現実世界の材料を標準化するために使用されているZeST（Zero-Shot Material Transfer）の論文であり、材料転送技術の詳細について知ることができます。

## 8. この論文を140字以内のツイートで要約すると？

MaRI: 現実世界の3Dアセット作成に必須な材料検索を革新！✨ 合成＆現実データで学習したAIが、画像から高精度に材料を特定。ドメインギャップを克服し、 #3D #AI #マテリアル 検索の未来を拓く！



---


# TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree Sequencing

[View Paper](http://arxiv.org/abs/2503.11629v1)

## 1. 既存研究では何ができなかったのか

既存のメッシュ生成研究は、特にアーティスティックなメッシュ生成において、以下の課題を抱えていました。

*   **高いポリゴン数のメッシュ生成の困難さ:** 従来の自己回帰型Transformerを用いたメッシュ生成手法（MeshGPT, MeshAnythingなど）は、Transformerの計算コストがシーケンス長に対して二次関数的に増加するため、生成できるメッシュのポリゴン数に制限がありました。複雑なオブジェクトを表現するためにはより多くのポリゴンが必要ですが、従来のモデルでは十分な表現力を持てませんでした。
*   **メッシュ品質の課題:** 従来のメッシュ生成手法では、ギャップ、欠落、反転した法線などのアーティファクトが発生しやすく、高品質なメッシュの生成が困難でした。特に、法線の向きが不整合であることは、レンダリングやアニメーションにおいて大きな問題となります。
*   **トークン化効率の悪さ:** 従来の自己回帰モデルでは、各頂点を3つのトークン（x, y, z座標）で表現するため、シーケンス長が長くなりやすく、効率的な学習が困難でした。例えばMeshGPTでは、各三角形の面を9つの潜在的なトークンで表現していました。
*   **ポイントクラウド条件付けの弱さ:** アーティスティックなメッシュを生成するために、ポイントクラウドなどの外部条件を効果的に活用できていませんでした。ポイントクラウドは、3Dスキャンデータや生成されたボクセル/点群データから容易に得られるため、条件付けに使うことでメッシュ生成を誘導できます。

## 2. どのようなアプローチでそれを解決しようとしたか

TreeMeshGPTは、これらの課題を解決するために、以下のアプローチを採用しました。

*   **Autoregressive Tree Sequencing:** 従来のnext-token predictionの代わりに、メッシュの三角形の隣接関係に基づいて動的に成長する木構造から次のトークンを取得する、新しいAutoregressive Tree Sequencingを提案しました。これにより、メッシュは各ステップで最後に生成された三角形の面からローカルに拡張され、学習の困難さを軽減し、メッシュの品質を向上させます。
*   **効率的なトークン化:** 各三角形の面を2つのトークンで表現することで、従来の単純なトークン化（9トークン/面）と比較して、約22％の圧縮率を達成しました。これにより、より多くの面を持つ詳細なメッシュの生成が可能になりました。
*   **法線方向制約の強化:** 法線方向の一貫性を保つための制約を導入することで、法線の反転を最小限に抑え、メッシュの品質を向上させました。
*   **ポイントクラウド条件付け:** ポイントクラウドを入力として、軽量なエンコーダで潜在空間に埋め込み、Transformer decoderのcross-attention層でメッシュ生成を誘導しました。これにより、ポイントクラウドに忠実なメッシュを生成できます。
*   **Hierarchical MLP heads:** 頂点のx, y, z座標を同時に予測する代わりに、x座標、y座標、z座標の順に階層的に予測するMLPヘッドを導入しました。これにより、座標のサンプリングが容易になり、メッシュの品質が向上しました。
*   **DFS (Depth-First Search) 探索:** メッシュの三角形を探索する際に、BFS (Breadth-First Search) ではなくDFSを用いることで、局所的な依存関係を強化し、学習効率を向上させました。

## 3. 結果、何が達成できたのか

TreeMeshGPTによって、以下の成果が達成されました。

*   **高品質なアーティスティックメッシュ生成:** 従来のメッシュ生成手法と比較して、より詳細で高品質なアーティスティックメッシュを生成できるようになりました。特に、法線方向の一貫性が向上し、アーティファクトが大幅に減少しました。
*   **高ポリゴン数のメッシュ生成:** 7-bitの離散化を使用した場合、強力なポイントクラウド条件の下で、最大5,500個の三角形の面を持つメッシュを生成できるようになりました。9-bitの離散化を使用した場合、最大11,000個の三角形の面を持つメッシュを生成できるようになりました。
*   **トークン化効率の向上:** 提案されたAutoregressive Tree Sequencingにより、三角形の面あたり2つのトークンという効率的な表現が可能になり、シーケンス長を大幅に削減しました。従来のnaiveなトークン化と比較して、約22%の圧縮率を実現しました。
*   **実世界の3Dスキャンへの汎化:** 実世界の3Dスキャンデータセット（GSO dataset）を用いた実験により、TreeMeshGPTが実世界のデータに対しても優れた汎化能力を持つことが示されました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

TreeMeshGPTには、以下のLimitationsと問題点が存在します。

*   **シーケンス長の増加に伴う成功率の低下:** 従来のメッシュ生成手法と同様に、シーケンス長が長くなるにつれて、メッシュ生成の成功率が低下する傾向があります。より多くの面を持つ複雑なメッシュの生成は依然として課題です。
*   **最適なメッシュトポロジーの強制の困難さ:** より高い面数を持つメッシュを生成する能力が向上しましたが、最適なメッシュトポロジーを強制することは依然として困難です。TreeMeshGPTによって生成されたメッシュは、必ずしも人間が設計したメッシュのトポロジーと一致するとは限りません。
*   **ポイントクラウド条件付けへの依存:** ポイントクラウドを入力として使用することで、メッシュ生成の品質が向上しますが、ポイントクラウドの品質が低い場合や、ポイントクラウドがオブジェクトの形状を十分に表現していない場合には、生成されるメッシュの品質が低下する可能性があります。
*   **データセットの偏り:** モデルは、Objaverseデータセットをトレーニングに使用していますが、Objaverseデータセットは特定の種類のオブジェクトに偏っている可能性があります。そのため、TreeMeshGPTが生成できるメッシュの種類に制限がある可能性があります。
*   **計算コスト:** Transformerアーキテクチャを使用しているため、モデルのトレーニングと推論には多大な計算コストがかかります。特に、高ポリゴン数のメッシュを生成する場合には、より多くの計算リソースが必要となります。
*   **9-bitモデルの学習コスト:** 9-bitモデルは7-bitモデルよりも高品質なメッシュを生成できますが、トレーニングには25日もの時間を要します。今後の研究では、学習効率を向上させるための手法を検討する必要があります。
*   **テキストプロンプトからの直接生成:** 論文では、テキストプロンプトからメッシュを生成するために、既存のテキスト-3DモデルとTreeMeshGPTを組み合わせたマルチステップパイプラインを使用しています。テキストプロンプトから直接メッシュを生成できるモデルを開発することで、より自然なワークフローを実現できると考えられます。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

TreeMeshGPTの技術的な詳細を以下に示します。

*   **アーキテクチャ:**
    *   Transformer decoderベースの自己回帰モデル。
    *   24 layers, 16 attention heads, hidden dimension of 1024。
    *   Sinusoidal positional encodingでトークンの位置情報をエンコード。
    *   Latent code conditionにはfull self-attention、autoregressive decoderにはcausal self-attention maskを使用。
    *   PyTorchのFlexAttentionでattention maskを効率的に実装。
*   **トークン化:**
    *   Autoregressive Tree Sequencing: 三角形の隣接関係に基づいた木構造からトークンを取得。
    *   各三角形の面を2つのトークンで表現。
    *   Half-edgeデータ構造とDFS探索を用いて入力-出力ペアを構築。
*   **学習:**
    *   Loss function: 各座標軸のcross-entropy lossの和。
    *   Optimizer: Adam (lr=1e-4, β1=0.9, β2=0.99)。
    *   fp16 mixed-precisionで計算速度とメモリ効率を最適化。
    *   A100-80GB GPUsで5日間トレーニング (effective batch size of 128)。
*   **推論:**
    *   Multinomial sampling strategy with a top-k of 5。
    *   スタック長に応じて温度を調整 (stack_length < 10 -> T=1, stack_length < 30 -> T=0.4, otherwise T=0.2)。
    *   生成された三角形の重複をチェックし、重複する場合は次の入力をスタックから取得。
    *   [STOP]トークンのlogitに加算ファクタを適用し、[STOP]トークンの予測を促進。

**疑似コード:**

```python
# Autoregressive Tree Sequencing
def autoregressive_tree_sequencing(mesh, start_edge):
    stack = [start_edge, reverse_edge(start_edge)] # stackの初期化
    visited_triangles = set() # 訪問済みの三角形を保持
    input_output_pairs = []

    while stack:
        current_edge = stack.pop() # stackのtopからedgeを取得
        if is_boundary(current_edge) or forms_visited_triangle(current_edge, visited_triangles):
            output = STOP_TOKEN
            input_output_pairs.append((current_edge, output))
        else:
            new_vertex = predict_new_vertex(current_edge) # 新しい頂点を予測
            output = new_vertex
            input_output_pairs.append((current_edge, output))

            triangle = form_triangle(current_edge, new_vertex)
            visited_triangles.add(triangle) # 三角形を訪問済みにする

            # 新しいedgeを作成
            edge1 = (new_vertex, current_edge[1])
            edge2 = (current_edge[0], new_vertex)

            # stackに新しいedgeを追加
            stack.append(edge1)
            stack.append(edge2)

    return input_output_pairs
```

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

TreeMeshGPTのトレーニングに関するコストと物理的な詳細を以下に示します。

*   **GPU:** A100-80GB GPUsを使用
*   **GPU数:** 明記されていませんが、batch sizeなどから複数GPUを使用していると考えられます。
*   **トレーニング時間:**
    *   7-bitモデル: 5日間
    *   9-bitモデル: 25日間
*   **データセット:**
    *   Objaverseデータセットのサブセット (75,000メッシュ)
    *   うち1,000メッシュをvalidation用に使用
    *   多様性を増すためにデータ拡張を実施
*   **モデルサイズ:**
    *   Transformer decoder: 24 layers, hidden dimension of 1024
    *   Positional embedding dimension: 512
    *   7-bit quantization: 128 classes
    *   9-bit quantization: (記載なし、128よりも多いクラス数)
*   **バッチサイズ:**
    *   Effective batch size of 128

## 7. 参考文献のうち、特に参照すべきもの

TreeMeshGPTの理解を深めるために、以下の参考文献を特に参照することを推奨します。

*   **MeshGPT (Siddiqui et al., CVPR 2021):** 自己回帰型Transformerを用いたメッシュ生成の初期の研究。メッシュのトークン化と生成の基本的なアイデアを理解するために重要です。
*   **MeshAnything (Chen et al.):** ポイントクラウド条件付けを導入したアーティスティックメッシュ生成の研究。TreeMeshGPTのベースラインとして使用されています。
*   **MeshAnythingV2 (Chen et al.):** 三角形の隣接関係を利用した効率的なトークン化手法を提案した研究。TreeMeshGPTのAutoregressive Tree Sequencingのアイデアの源泉となっています。
*   **Transformer (Vaswani et al., NeurIPS 2017):** Transformerアーキテクチャの原著論文。TreeMeshGPTの基盤となるアーキテクチャを理解するために必須です。

## 8. この論文を140字以内のツイートで要約すると？

TreeMeshGPT：木構造でメッシュを効率生成！三角形隣接関係からトークンを再帰的に取得し、高精細なアーティスティックメッシュを生成。ポイントクラウド条件付けで品質向上！ #3D #MeshGeneration #AI


---


# Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models

[View Paper](http://arxiv.org/abs/2503.11224v1)

## 1. 既存研究では何ができなかったのか

既存研究、特に Transformer ベースのモデルは、sequential data や長い文脈を扱うタスクにおいて、計算効率の面で課題がありました。 Transformer は attention mechanism を用いるため、sequence length に対して計算量が二次関数的に増加します。一方、State Space Models (SSMs) は、より効率的な計算が可能になるポテンシャルを持っていますが、その理論的背景や様々な応用に関する包括的な調査が不足していました。

## 2. どのようなアプローチでそれを解決しようとしたか

本論文では、SSMs に関する包括的なサーベイを提供することによって、既存研究の課題を解決しようとしました。具体的には、以下の３つのアプローチをとっています。

1.  **理論的動機と数学的定式化の提示:** SSMs の理論的基盤を明確にし、数学的な定式化を行うことで、研究者が SSMs を理解しやすくする。
2.  **既存のモデルクラスとの比較:** SSMs を Transformer などの他のモデルクラスと比較することで、SSMs の利点と欠点を明らかにする。
3.  **様々な応用例の紹介:** SSMs の様々な応用例を紹介することで、SSMs の可能性を示す。

論文では、SSM を以下の3つの主要なセクションに分けて、詳細な解説をしています。

*   Original SSM
*   Structured SSM (S4)
*   Selective SSM (Mamba)

特に、SSM の有効性と効率を向上させるために導入された様々な主要技術に焦点を当てています。

## 3. 結果、何が達成できたのか

この論文はサーベイ論文であるため、直接的な実験結果や性能向上を示すものではありません。しかし、以下の点が達成できたと考えられます。

*   **SSMs の包括的な理解:** 研究者にとって、SSMs の理論的背景、数学的定式化、既存モデルとの比較、応用例などを包括的に理解するための出発点となる。
*   **SSMs の研究促進:** SSMs の様々な技術や応用例を紹介することで、研究者が SSMs の研究を始めるきっかけとなる。
*   **SSMs の適用可能性の認識:** SSMs が sequential data や長い文脈を扱うタスクにおいて、Transformer に匹敵する性能を、より効率的に達成できる可能性を示す。

## 4. Limitationや問題点は何か

*   **サーベイの網羅性:** SSMs の研究は急速に進展しているため、サーベイ論文の性質上、全ての最新の研究を網羅することは困難です。
*   **実験的検証の欠如:** サーベイ論文であるため、提案された技術の有効性を実験的に検証した結果は含まれていません。実験的な結果は、原著論文を参照する必要があります。
*   **実装の複雑さ:** SSMs は Transformer に比べて、実装が複雑になる可能性があります。論文では、実装の詳細については触れられていません。

私が考える問題点：

*   **Selective SSM (Mamba)に偏った焦点:** 論文の抽象にも記載があるように、selective SSMであるMambaに焦点が当たっているように見受けられます。そのため、Mamba以外のSSMについては、情報が不足している可能性があります。（ただし、これは論文の焦点の問題であり、必ずしも limitation とは言えません。）

## 5. 技術的な詳細について

SSMs の基本的な構造は、以下の疑似コードで表すことができます。

```python
def ssm_step(state, input_value, A, B, C, D):
  """
  State Space Model の１ステップを計算する関数

  Args:
    state: 現在の状態 (shape: (state_dim,))
    input_value: 現在の入力値 (shape: (input_dim,))
    A: 状態遷移行列 (shape: (state_dim, state_dim))
    B: 入力行列 (shape: (state_dim, input_dim))
    C: 出力行列 (shape: (output_dim, state_dim))
    D: 直接的な入力から出力への影響行列 (shape: (output_dim, input_dim))

  Returns:
    new_state: 次の状態 (shape: (state_dim,))
    output_value: 現在の出力値 (shape: (output_dim,))
  """

  # 状態の更新
  new_state = A @ state + B @ input_value

  # 出力の計算
  output_value = C @ state + D @ input_value

  return new_state, output_value

def ssm(input_sequence, A, B, C, D, initial_state):
  """
  State Space Model を入力シーケンスに適用する関数

  Args:
    input_sequence: 入力シーケンス (shape: (seq_len, input_dim))
    A: 状態遷移行列 (shape: (state_dim, state_dim))
    B: 入力行列 (shape: (state_dim, input_dim))
    C: 出力行列 (shape: (output_dim, state_dim))
    D: 直接的な入力から出力への影響行列 (shape: (output_dim, input_dim))
    initial_state: 初期状態 (shape: (state_dim,))

  Returns:
    output_sequence: 出力シーケンス (shape: (seq_len, output_dim))
  """

  state = initial_state
  output_sequence = []

  for input_value in input_sequence:
    state, output_value = ssm_step(state, input_value, A, B, C, D)
    output_sequence.append(output_value)

  return output_sequence
```

この基本的な構造に加えて、S4 では structured matrices を用いることで計算効率を向上させ、Mamba では selective mechanism を導入することで、文脈に応じた情報の選択を行うことができます。

Mamba の selective mechanism は、入力に依存した状態空間モデルのパラメータ更新を可能にします。これにより、モデルは文脈に応じて重要な情報に焦点を当て、不要な情報を無視することができます。この仕組みは、長期依存関係を捉える上で非常に重要です。

## 6. コストや物理的な詳細について

サーベイ論文であるため、具体的なトレーニングコストや物理的な詳細に関する記述はありません。これらの情報は、各 SSM の原著論文を参照する必要があります。

## 7. 参考文献のうち、特に参照すべきもの

この論文自体がサーベイであるため、参考文献リストを確認し、特に興味のある SSM や技術に関する原著論文を参照することが重要です。Abstract に記載されているように、S4 と Mamba は重要なキーワードとなるでしょう。

## 8. この論文を140字以内のツイートで要約すると？

State Space Models (SSMs) のサーベイ論文。Transformer の代替として注目されるSSMsの理論、構造（S4, Mamba）、応用を解説。効率的なsequential data処理に期待！ #SSM #StateSpaceModel #AI


---


# Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?

[View Paper](http://arxiv.org/abs/2503.10632v1)

## 1. 既存研究では何ができなかったのか

既存研究では、Vision Transformer (ViT) のMulti-Head Self-Attention (MHSA) モジュールを、学習可能なKolmogorov-Arnold Network (KAN) で置き換える試みが限られていました。具体的には、以下の点が課題でした。

*   **KANのVisionタスクへの有効性:** KANは、記号表現の発見や1次元関数の継続学習には有効ですが、ViTのような高度なアーキテクチャにおける画像認識タスクでの有効性は不明確でした。
*   **計算コストとメモリ消費:** 学習可能なAttentionモジュールの学習には、通常のSelf-Attentionよりも高い計算コストとメモリが必要となり、大規模なデータセットでの学習が困難でした。既存研究のKATでは、MLP層をKANに置き換える際に、グループKANという手法を用いてパラメータ数を削減していましたが、単純な置き換えでは性能向上が保証されませんでした。
*   **学習可能なMulti-Head Attentionモジュールの設計:** Transformerの根幹であるMulti-Head Attentionモジュール自体を学習可能にするという試みがほとんどありませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の課題を解決するために、以下の３つのアプローチをとりました。

*   **Learnable Kolmogorov-Arnold Attention (KArAt)の設計:** ViTのMHSAを置き換える、汎用的な学習可能なAttentionモジュールKArAtを設計しました。KArAtは任意の基底関数上で動作するように設計されています。
*   **Fourier-KArAtの提案:** KArAtの計算コストとメモリ消費を削減するために、Fourier基底を用いた、よりモジュール化されたFourier-KArAtを提案しました。
*   **スペクトル解析と損失風景の分析:** KArAtとViTの性能と汎化能力を詳細に分析するために、損失風景、重み分布、最適化パス、Attentionの可視化、スペクトル挙動などを分析し、ViTと比較しました。

具体的には、

1.  ViTのencoder block内のMHSAをKArAtに置き換えます。
2.  KArAtの各attention headのattention matrixに対し、以下の処理を行います。
    a.  Softmaxの代わりに、学習可能な活性化関数を適用します。
    b.  学習可能な活性化関数として、Fourier基底を利用します。
    c.  学習可能な活性化関数の計算コストを削減するため、低ランク構造を利用します。
    d.  必要に応じて、l1-unit ballへのprojectionを行います。
3.  Fourier KArAtの性能を評価するため、CIFAR-10, CIFAR-100, ImageNet-1Kデータセットで、ViTとの比較実験を行います。

## 3. 結果、何が達成できたのか

本研究の結果、以下の点が達成されました。

*   **Fourier-KArAtの有効性:** Fourier-KArAtは、CIFAR-10およびCIFAR-100データセットにおいて、ViTを上回る性能を達成しました。ImageNet-1Kデータセットでは、ViTと同程度の性能を達成しました。
*   **性能と汎化能力の分析:** KArAtのスペクトル解析、損失風景の可視化などを行い、ViTとの比較を通じて、KArAtの性能と汎化能力に関する知見を得ました。
*   **学習可能なAttentionモジュールの設計:** ViTのMHSAを置き換える、学習可能なAttentionモジュールの設計に関する知見が得られました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

本研究のLimitationと問題点は以下の通りです。

*   **計算コストとメモリ消費:** Fourier-KArAtは、ViTと比較して計算コストとメモリ消費が大きいです。特に、B-splineを基底関数として使用した場合、メモリ消費が非常に大きくなり、学習が困難になる場合があります。
*   **汎化性能:** ViT-SmallやViT-Baseといった大規模モデルでは、Fourier-KArAtはViTを上回る性能を達成できませんでした。これは、Fourier-KArAtが過剰パラメータ化されており、最適化が難しいことが原因である可能性があります。
*   **l1-projectionの有効性:** 学習したattention vectorを確率シンプレックスに射影するl1-projectionは、性能を低下させる場合がありました。
*   **基底関数の選択:** 本研究ではFourier基底に焦点を当てましたが、他の基底関数（B-spline, Waveletなど）の有効性は十分に検証されていません。
*   **データセット:** 本研究では、CIFAR-10、CIFAR-100、ImageNet-1Kデータセットで評価を行いましたが、他のデータセットでの有効性は不明です。特に、タスクによっては、MHSAが持つデータ固有の性質が重要になる場合があり、そのような場合にKArAtが有効かどうかは検証が必要です。
*   **パラメータ効率:** KArAtは、KANが持つパラメータ効率という利点を活かせていません。今後の研究で、パラメータ効率の高いKArAtの設計が求められます。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

Fourier-KArAtの技術的な詳細について説明します。

### KArAtの基本構造

KArAtは、ViTのMHSAを置き換えるモジュールであり、各attention headのattention matrixに対して学習可能な活性化関数を適用します。attention matrixを`A`とすると、KArAtは以下のように計算されます。

```python
# A: attention matrix (N x N)

# Φ: 学習可能な活性化関数 (N x N)
# σ_tilde(A) = Φ(A)

# softmaxの代わりに学習可能な活性化関数を適用
sigma_tilde = Φ(A)

#必要に応じて、l1-unit ballへのprojection
#これにより、attention mapが確率分布になることを保証する
if use_l1_projection:
  sigma_tilde = project_to_l1_unit_ball(sigma_tilde)

# V: value matrix (N x d/h)
# d: embedding dimension
# h: attention head数
output = sigma_tilde @ V
```

### Fourier基底の利用

KArAtでは、学習可能な活性化関数`Φ`として、Fourier基底を利用します。Fourier基底を用いることで、`Φ`を以下のように表現できます。

```python
# A_k,q: attention matrix Aの(k,q)要素

# G: グリッドサイズ
# a_pqm, b_pqm: 学習パラメータ

# Φ_pq(A_k,q) = Σ a_pqm * cos(m*A_k,q) + b_pqm * sin(m*A_k,q)  (m=1..G)
# Φ_pq: Φの(p,q)要素

# r: 低次元空間の次元数
# W: 学習可能な重み行列 (N x r)

# Φ_hat: 低次元空間への射影を行う学習可能な活性化関数 (r x N)
# Φ_hat_p[(A_k,:)^T] = Σ Φ_hat_pq(A_k,q) (q=1..N)

#W @ Φ_hat[(A_k,:)^T]
```

### 低ランク構造の利用

attention matrixの低ランク構造を利用することで、計算コストを削減します。具体的には、`Φ`を直接計算する代わりに、低次元空間への射影`Φ_hat`と、射影後の空間での重み`W`を用いて、`Φ(A) = W @ Φ_hat(A)`のように近似します。

### BlockwiseとUniversal operator configuration

本研究では、学習可能なoperator `Φ_hat`の更新方法として、BlockwiseとUniversalの2つのconfigurationを検討しました。

*   **Blockwise:** 各encoder blockで異なる`Φ_hat`を学習します。
*   **Universal:** 全てのencoder blockで同じ`Φ_hat`を共有します。

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

本研究で使用したコストや物理的な詳細についてまとめます。

*   **GPU:** 80 GB NVIDIA H100 GPU x 2
*   **データセット:**
    *   CIFAR-10
    *   CIFAR-100
    *   ImageNet-1K
*   **バッチサイズ:**
    *   ImageNet-1K: 128
    *   CIFAR-10 & CIFAR-100: 32
*   **学習率:**
    *   ImageNet-1K: 1.25 × 10<sup>-4</sup>
    *   CIFAR-10 & CIFAR-100: 3.125 × 10<sup>-5</sup>
*   **最適化:** AdamW
*   **エポック数:** 300
*   **スケジューラ:** Cosine scheduler
*   **weight decay:** 5 × 10<sup>-2</sup>
*   **モデル:** ViT-Tiny, ViT-Small, ViT-Base

また、Fourier KArAtの学習には、通常のMHSAよりも長い時間がかかります。詳細な学習時間の比較は、論文の付録に記載されています。

## 7. 参考文献のうち、特に参照すべきもの

参考文献のうち、特に参照すべきものは以下の通りです。

*   **Vaswani et al., 2017:** Transformerの原論文です。MHSAの基本的な仕組みを理解する上で重要です。
*   **Liu et al., 2024 (KAN):** Kolmogorov-Arnold Network (KAN)の論文です。KANの構造と動作原理を理解する上で重要です。
*   **Dosovitskiy et al., 2021:** Vision Transformer (ViT)の原論文です。ViTの基本的な仕組みを理解する上で重要です。

## 8. この論文を140字以内のツイートで要約すると？

ViTのAttentionを学習可能なKANで置き換えるKArAtを提案！Fourier基底で計算効率化。CIFARでViT超え達成🎉でも計算コスト課題あり。KANの可能性と課題を示唆 #VisionTransformer #KAN #LearnableAttention


---


# PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity

[View Paper](http://arxiv.org/abs/2503.07677v1)

## 1. 既存研究では何ができなかったのか

既存の拡散モデルにおけるガイダンス技術（Classifier-Free Guidance (CFG)など）は、高品質な条件付きサンプル生成において目覚ましい成果を上げていますが、以下の課題を抱えていました。

*   **追加学習または追加のニューラル関数評価 (NFE) の必要性:** 多くの既存手法は、追加の学習や推論時の計算コストを必要とし、ガイダンス蒸留されたモデル（guidance-distilled models）との互換性がありませんでした。
*   **ヒューリスティックなアプローチ:** 既存の手法は、ターゲット層を特定する必要があるヒューリスティックなアプローチに依存しており、汎用性に欠けていました。
*   **ガイダンス蒸留モデルへの適用不可:** CFGなどの既存手法は、条件付きモデルと無条件モデルまたはweakモデルの差分計算を必要とするため、ガイダンス蒸留モデルには適用できませんでした。
*   **テキストと画像の正確なアラインメントの課題:** 特に複雑なプロンプトや空間的な関係性を表現するプロンプトにおいて、テキストと生成された画像の正確なアラインメントが難しい場合がありました。

これらの問題点を解決し、より普遍的で効率的な拡散モデルの性能向上手法が求められていました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、PLADIS（Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity）という、新しい推論手法を提案しました。PLADISは、以下の主要なアイデアに基づいています。

*   **スパースアテンションの活用:** 拡散モデルのクロスアテンション層において、標準的な（密な）アテンション機構の代わりにスパースアテンション機構を利用します。
*   **クエリ-キー相関の補外:** 推論時に、ソフトマックス関数を用いた標準的なアテンションと、スパースな対応物を用いてクエリ-キー相関を補外します。これにより、追加の学習やNFEを必要とせずに、既存のモデルをブーストできます。
*   **ノイズに対するロバスト性:** スパースアテンションのノイズに対するロバスト性を活用することで、テキストから画像への拡散モデルの潜在能力を引き出し、従来苦戦していた領域での性能を向上させます。
*   **既存のガイダンス技術との統合:** PLADISは、既存のガイダンス技術（ガイダンス蒸留モデルを含む）とシームレスに統合できます。

具体的には、PLADISはクロスアテンション層を以下のように変更します。

```python
def pladis_attention(Q, K, V, alpha, lambda_, d):
  """
  PLADISを適用したアテンションの計算

  Args:
    Q: クエリ行列
    K: キー行列
    V: バリュー行列
    alpha: スパース度合いを制御するハイパーパラメータ
    lambda_: 密なアテンションとスパースなアテンションの混合比を制御するハイパーパラメータ
    d: キーとクエリの次元

  Returns:
    アテンションの出力
  """

  # 1. 密なアテンション (Softmax)
  dense_attention = softmax(Q @ K.T / sqrt(d)) @ V

  # 2. スパースなアテンション (alpha-Entmax)
  sparse_attention = alpha_entmax(Q @ K.T / sqrt(d), alpha) @ V

  # 3. 密なアテンションとスパースなアテンションの補外
  output = dense_attention + lambda_ * (sparse_attention - dense_attention)
  return output
```

ここで、`alpha_entmax(x, alpha)`は、入力`x`に対してalpha-Entmax関数を適用し、スパースな確率分布を出力する関数です。`alpha`はスパース度合いを制御するハイパーパラメータで、`alpha=1`のとき標準的なソフトマックス関数に、`alpha=2`のときSparsemax関数に相当します。`lambda_`は密なアテンションとスパースなアテンションの混合比を制御するハイパーパラメータです。

## 3. 結果、何が達成できたのか

PLADISの導入により、以下の点が達成されました。

*   **テキストアラインメントの向上:** 生成された画像のテキストアラインメントが大幅に向上しました。特に、複雑なプロンプトや空間的な関係性を表現するプロンプトにおいて、その効果が顕著でした。
*   **人間の嗜好の向上:** ユーザーによる評価において、生成された画像の品質とテキストアラインメントの両方で、既存手法と比較して高い評価を得られました。
*   **効率性:** 追加の学習やNFEを必要としないため、既存のモデルに容易に組み込むことができ、推論コストを大幅に増加させることなく性能向上が可能となりました。
*   **普遍性:** 既存のガイダンス技術（CFG、PAG、SEGなど）やガイダンス蒸留モデルとシームレスに統合できるため、様々な拡散モデルアーキテクチャに適用可能な汎用的なソリューションを提供しました。
*   **理論的根拠:** スパースアテンションのノイズに対するロバスト性に関する理論的な分析（現代ホップフィールドネットワークとの関連性）を提供し、PLADISの有効性を裏付けました。

## 4. Limitationや問題点は何か

本研究で提案されたPLADISには、以下の limitation や問題点があります。

*   **複雑なTransformer構造への適用:** 実験は主にU-Netをベースとした拡散モデルで行われており、MMDiT（Multimodal Diffusion Transformer）やFlux modelのような、より複雑なTransformer構造を持つバックボーンアーキテクチャへの適用は検証されていません。
*   **タスクの限定:** テキストから画像への生成タスクに焦点が当てられており、テキストからビデオへの生成、マルチモーダル生成、言語生成などの他のタスクへの拡張可能性は示唆されているものの、具体的な実験結果は示されていません。
*   **ハイパーパラメータの調整:** PLADISには、スパース度合いを制御する`alpha`と密なアテンションとスパースなアテンションの混合比を制御する`lambda_`という2つのハイパーパラメータがあります。論文中では`alpha=2`、`lambda_=1.5`がデフォルト設定として用いられていますが、他の設定での性能評価や、プロンプトやデータセットに応じて最適な値を自動的に調整する手法については十分に検討されていません。
*   **計算コスト:** PLADISは追加の学習やニューラル関数評価を必要としませんが、スパースアテンションの計算には若干の計算コストがかかります。特に、高解像度の画像生成や大規模なモデルにおいては、その影響が無視できない可能性があります。論文中では、計算時間の増加はわずかであるとされていますが、具体的なボトルネックや最適化手法については詳細な分析がなされていません。

個人的に考える問題点としては、以下のような点が挙げられます。

*   **スパースアテンションの解釈可能性:** PLADISはスパースアテンションを活用することで性能向上を実現していますが、なぜスパースアテンションが効果的なのか、その内部メカニズムについては完全には解明されていません。スパースアテンションによって、モデルがどのような情報を重視し、どのような情報を無視しているのかを理解することで、更なる性能向上や汎用性の向上が期待できます。
*   **敵対的攻撃に対する脆弱性:** スパースアテンションはノイズに対するロバスト性を持つ一方で、敵対的な摂動（adversarial perturbation）に対して脆弱である可能性があります。敵対的な攻撃に対するPLADISの堅牢性を評価し、必要に応じて防御メカニズムを組み込むことが重要です。

## 5. 技術的な詳細について

PLADISは、拡散モデルにおけるクロスアテンション層の挙動を改善することで、テキストから画像への生成性能を向上させる手法です。技術的な詳細について、以下にまとめます。

*   **拡散モデルの基礎:** 拡散モデルは、ノイズを加える過程（forward process）と、ノイズを取り除く過程（reverse process）を学習することで、高品質な画像を生成します。条件付き生成の場合、reverse processは条件（テキストなど）に基づいて行われます。
*   **Classifier-Free Guidance (CFG):** CFGは、条件付きモデルと無条件モデルの出力を組み合わせることで、生成される画像の品質と条件への適合性を向上させる一般的な手法です。
*   **アテンション機構:** アテンション機構は、入力系列（テキストプロンプトなど）の各要素間の関連性を学習し、重要な要素に焦点を当てることで、モデルの性能を向上させます。拡散モデルでは、クロスアテンション機構がテキストプロンプトと画像の特徴量間の関連性を学習するために用いられます。
*   **スパースアテンション:** PLADISでは、標準的なアテンション機構の代わりにスパースアテンション機構を利用します。スパースアテンションは、アテンション重みをスパース化することで、計算効率の向上やノイズに対するロバスト性の向上などの効果が期待できます。
*   **Alpha-Entmax:** PLADISでは、アテンション重みをスパース化するために、Alpha-Entmax関数を利用します。Alpha-Entmax関数は、入力されたベクトルに対して、スパースな確率分布を出力します。`alpha`の値によってスパース度合いを制御することができ、`alpha=1`のとき標準的なソフトマックス関数に、`alpha=2`のときSparsemax関数に相当します。
*   **PLADISの実装:** PLADISは、既存の拡散モデルのクロスアテンション層を、以下のように変更することで実装できます。
    1.  クエリ行列`Q`、キー行列`K`、バリュー行列`V`を入力として受け取ります。
    2.  標準的なアテンション（密なアテンション）を計算します。`attention_dense = softmax(Q @ K.T / sqrt(d)) @ V`
    3.  Alpha-Entmax関数を用いて、スパースなアテンションを計算します。`attention_sparse = alpha_entmax(Q @ K.T / sqrt(d), alpha) @ V`
    4.  密なアテンションとスパースなアテンションを、`lambda_`を用いて線形結合します。`attention_pladis = attention_dense + lambda_ * (attention_sparse - attention_dense)`
    5.  `attention_pladis`を出力します。
*   **現代ホップフィールドネットワークとの関連性:** 論文中では、PLADISの理論的な根拠として、現代ホップフィールドネットワークとの関連性が示唆されています。現代ホップフィールドネットワークは、連想記憶モデルの一種であり、入力されたパターンに最も近い記憶パターンを想起することができます。スパースアテンションは、現代ホップフィールドネットワークにおけるスパースな記憶パターンとの類似性があり、ノイズに対するロバスト性などの利点を持つと考えられます。
*   **ノイズに対するロバスト性の理論的分析:** 論文中では、スパースアテンションのノイズに対するロバスト性に関する理論的な分析が提供されています。具体的には、スパースアテンションを用いた場合の記憶パターンの想起誤差の限界を導出しています。

## 6. コストや物理的な詳細について

論文中に記載されているコストや物理的な詳細に関する情報は以下の通りです。

*   **バックボーンモデル:** Stable Diffusion XL (SDXL) をバックボーンモデルとして使用。
*   **GPU:** 単一の NVIDIA H100 GPU で実験を実施。
*   **データセット:** MS-COCO 検証セット (30K ランダムプロンプト)、Drawbench、Pick-a-pic テストセット。
*   **FID計算:** 30K 画像を生成して FID を計算。
*   **サンプリングステップ:** 主に 25 サンプリングステップを使用。一部、4 ステップや 1 ステップの蒸留モデルでの実験も実施。
*   **計算時間:** PLADIS は、わずかな処理時間 (0.56 秒/プロンプト) とメモリ消費量 (0.01 GB) の増加で済む。
*   **パラメータ数:** PLADIS自体は既存モデルのクロスアテンション層の処理を変更するだけなので、学習パラメータは追加されない。
*   **実装:** `alpha_entmax`関数の実装にはオープンソースライブラリを利用。具体的なライブラリ名は論文中に記載なし。

## 7. 参考文献のうち、特に参照すべきもの

PLADIS を理解する上で、特に参照すべき参考文献は以下の通りです。

*   **Denoising Diffusion Probabilistic Models (Jonathan Ho, Ajay Jain, and Pieter Abbeel):** 拡散モデルの基礎を理解するために重要です。
*   **High-Resolution Image Synthesis with Latent Diffusion Models (Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer):** Latent Diffusion Model (LDM) の詳細を理解するために重要です。PLADISはLDMをベースとしたSDXLに適用されています。
*   **From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification (André F. T. Martins and Ramón Fernandez Astudillo):** Sparsemax関数、および一般化したalpha-Entmax関数の理解に不可欠です。PLADISは、このalpha-Entmax関数をアテンション層に適用しています。
*   **Modern Hopfield Networks and Attention (Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, et al.):** 現代ホップフィールドネットワークとアテンション機構の関連性を理解するために重要です。PLADISの理論的根拠の一部となっています。
*   **STANHOP: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction (Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu):** スパースなHopfieldネットワークの特性について理解するのに役立ちます。

## 8. この論文を140字以内のツイートで要約すると？

拡散モデルの推論効率爆上げ！PLADISは、スパースアテンションで追加学習なしにテキストと画像の整合性を向上。既存モデルに組み込みやすく、高品質な画像生成が可能に！ #拡散モデル #画像生成 #AI


---

はい、承知いたしました。以下に、ご質問に対する詳細な回答をmarkdown形式で記述します。


# Learning Few-Step Diffusion Models by Trajectory Distribution Matching

[View Paper](http://arxiv.org/abs/2503.06674v2)

## 1. 既存研究では何ができなかったのか

既存のdiffusion modelのsamplingを高速化する研究では、以下の課題がありました。

*   **複雑なタスクでの性能不足:** distribution matchingに基づくdiffusion distillationは、text-to-image生成のような複雑なタスクにおいて性能が不十分でした。
*   **速度と品質のトレードオフ:** few-step生成は速度と品質のバランスが良いものの、既存のアプローチでは以下のトレードオフが解消できませんでした。
    *   distribution matchingはmulti-step samplingに対する柔軟性に欠ける。
    *   trajectory matchingは画像品質が最適ではないことが多い。
*   **柔軟性の欠如:** deterministic samplingに基づく既存の手法はsampling stepの調整が難しく、stochastic samplingに基づく手法は柔軟性があるものの画像品質が犠牲になる。

## 2. どのようなアプローチでそれを解決しようとしたか

これらの課題を解決するために、Trajectory Distribution Matching (TDM)という新しいdistillationパラダイムを提案しました。

*   **distribution matchingとtrajectory matchingの統合:** TDMは、distribution matchingとtrajectory matchingの長所を組み合わせたunifiedなアプローチです。
*   **data-free score distillation:** studentモデルのtrajectoryをteacherモデルのdistributionレベルでalignさせるdata-free score distillation objectiveを導入しました。
*   **sampling-steps-aware objective:** sampling stepごとに学習ターゲットを分離するsampling-steps-aware objectiveを開発し、柔軟なsamplingを可能にしました。
*   **疑似Huber metric:** Consistency Models (CMs)との共通点に着想を得て、Pseudo-Huber metricを用いたsurrogate training objectiveを導入し、最適化を促進しました。

## 3. 結果、何が達成できたのか

TDMによって、以下の成果が得られました。

*   **高品質なfew-step生成:** SDXLやPixArt-$\alpha$のような様々なbackboneにおいて、既存の手法を上回るstate-of-the-artの性能を達成しました。
*   **効率的な学習:** PixArt-$\alpha$を4-step generatorにdistillする際、teacherモデルのtraining costのわずか0.01%で、real user preferenceにおいてteacherを上回る性能を達成しました。
*   **text-to-video生成への拡張:** TDMはtext-to-video diffusionにも適用でき、CogVideoX-2Bを4 NFEで上回る性能をVBenchで達成しました。
*   **多様なSampling Step数への対応:** sampling-steps-aware objectiveによって、柔軟なSampling Step数でのdeterministic samplingを実現しました。

## 4. Limitationや問題点は何か

*   **teacherモデルへの依存:** data-freeな手法であるため、pretrained diffusion modelの性能が上限となる。特にSD-v1.5のようなモデルでは、training dataの品質が均一でないため、人間の好みに合わない場合がある。
*   **計算コスト:** TDM-unifyは、様々なsampling step数でtrainingする必要があるため、計算コストが高くなる。
*   **Generalな応用に対する検証の必要性:** text-to-image, text-to-video以外のタスクに対する適用可能性は不明確。

**その他に考えられるLimitation:**

*   **diffusion modelのアーキテクチャへの依存性:** 特定のdiffusion modelのアーキテクチャに特化している可能性があり、異なるアーキテクチャへの適用には調整が必要となる可能性がある。
*   **ハイパーパラメータの調整:** 最適な性能を得るためには、ハイパーパラメータ（学習率、batch sizeなど）の慎重な調整が必要となる。
*   **データセットのバイアス:** distillationに使用するデータセットにバイアスがある場合、生成される画像にもバイアスが反映される可能性がある。

## 5. 技術的な詳細について

TDMは、trajectory distillationとdistribution matchingを統合した新しいdiffusion distillationパラダイムです。

1.  **Trajectory Distribution Matching Objective:**
    *   studentモデルのtrajectoryとteacherモデルのdistributionをalignさせるために、marginal reverse KL divergenceを最小化します。
    *   studentモデルからsamplingを行い、そのtrajectory上のsampleとteacherモデルのdistributionとのKL divergenceを計算します。
    *   これにより、teacherモデルからのsamplingを回避し、numerical errorを抑制します。
    ```python
    def loss(theta, phi, x_t, K):
        # theta: student model parameters
        # phi: teacher model parameters
        # x_t: diffused samples at timestep t
        # K: number of steps

        total_kl_divergence = 0
        for i in range(K - 1):
            tau = t_i # time at i-th step in the trajectory
            kl_divergence = kl_divergence(p_theta_tau_given_ti(x_tau, theta, t_i), p_phi_tau(x_tau, phi)) #KL divergence between student and teacher
            total_kl_divergence += kl_divergence
        return total_kl_divergence
    ```
2.  **Sampling-Steps-Aware Objective:**
    *   sampling step数に応じて学習ターゲットを分離することで、柔軟なsamplingを可能にします。
    *   studentモデルとfake scoreにsampling step数をconditionとして与えます。
    ```python
    def sampling_steps_aware_loss(theta, phi, x_t, K):
        # theta: student model parameters
        # phi: teacher model parameters
        # x_t: diffused samples at timestep t
        # K: number of steps

        total_kl_divergence = 0
        for i in range(K - 1):
            tau = t_i # time at i-th step in the trajectory
            p_theta_tau_given_ti = student_model(x_tau, theta, t_i, K) #student model with sampling step K as input
            p_phi_tau = teacher_model(x_tau, phi) #teacher model
            kl_divergence = kl_divergence(p_theta_tau_given_ti, p_phi_tau) #KL divergence between student and teacher
            total_kl_divergence += kl_divergence
        return total_kl_divergence
    ```
3.  **Surrogate Training Objective with Pseudo-Huber Metric:**
    *   Consistency Models (CMs)との類似性に着想を得て、Pseudo-Huber metricを用いたsurrogate training objectiveを導入します。
    *   これにより、generatorの学習を安定化させます。
    ```python
    def pseudo_huber_loss(x_ti, x_ti_revised, c=0.00054):
        # x_ti: sample on the student trajectory
        # x_ti_revised: sample revised from student trajectory by gradient descent
        # c: parameter for pseudo Huber metric
        
        loss = sqrt(mse(x_ti, x_ti_revised) + c**2) - c
        return loss
    ```

## 6. コストや物理的な詳細について

*   **GPU:** A800
*   **training時間:**
    *   PixArt-$\alpha$ distillation: 2 A800 hours (500 iterations)
    *   SDXL distillation: 2 A800 days (TDM-unify-SFT)
    *   SDXL distillation: 3 A800 days (TDM-unify-GAN)
*   **データセット:** JourneyDB dataset
*   **モデルサイズ:**
    *   SDXL: 2.7B parameters
    *   その他モデルは記載なし

## 7. 参考文献のうち、特に参照すべきもの

*   **Denoising Diffusion Probabilistic Models:** diffusion modelの基礎
    *   Jonathan Ho, Ajay Jain, and Pieter Abbeel. Advances in neural information processing systems
*   **Latent Consistency Models:** Consistency modelに関する論文
    *   Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.
*   **Adversarial Diffusion Distillation:** diffusion distillationに関する論文
    *   Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.

## 8. この論文を140字以内のツイートで要約すると？

拡散モデルの高速化！TDMは軌跡と分布のマッチングを統合し、高品質なfew-step生成を実現。PixArt-$\alpha$蒸留で教師超え、SDXLでもSOTA！学習コストも大幅削減 #拡散モデル #AI生成 #蒸留 #TDM



---


# Neighboring Autoregressive Modeling for Efficient Visual Generation

[View Paper](http://arxiv.org/abs/2503.10696v1)

## 1. 既存研究では何ができなかったのか

既存の visual autoregressive モデルは、主にラスタースキャン順の "next-token prediction" パラダイムに従っており、視覚コンテンツに内在する空間的および時間的な局所性を無視していました。具体的には、視覚トークンは、遠くのトークンと比較して、空間的または時間的に隣接するトークンとの相関が著しく強いにもかかわらず、それを考慮できていませんでした。
既存研究の課題は下記のようにまとめられます。

*   **空間・時間的な局所性の無視:** 従来のラスタースキャン順のアプローチでは、隣接するトークン間の強い相関を十分に活用できていませんでした。
*   **非効率な推論:**  画像を生成するために数千のトークンを順番に生成する必要があり、diffusion modelと比較して効率が著しく低下していました。
*   **並列処理の限界:**  並列に複数のトークンを生成する試み（SJDなど）は、文脈情報の不足により性能が低下しました。ランダムな位置やブロックに制約された領域でトークンを生成する方法は、並列処理と空間的な一貫性のバランスを取るために、広範なハイパーパラメータ調整を必要としていました。
*   **マルチスケールアプローチの複雑さ:**  粗いスケールから細かいスケールへとトークンマップを生成する "next-scale prediction" パラダイム（VARなど）は、特殊なマルチスケールイメージトークナイザーが必要となり、トークンシーケンスが長くなるため、トレーニングと推論の両方でメモリオーバーヘッドが増加しました。
*   **両立の難しさ:** 空間/時間的な局所性の維持、推論時の並列処理のサポート、シングルスケールトークナイザーとの互換性、短いトークンシーケンスの維持という要件を同時に満たすことができませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、Neighboring Autoregressive Modeling (NAR) という新しいパラダイムを提案し、これらの課題を解決しようとしました。NAR は、autoregressive な視覚生成を、near-to-far の "next-neighbor prediction" メカニズムに従った progressive outpainting 手順として定式化します。

具体的なアプローチは以下の通りです。

*   **近傍からの逐次的なアウトペインティング:** 初期トークンから開始し、残りのトークンを空間-時間空間における初期トークンからのマンハッタン距離の昇順でデコードすることで、デコードされた領域の境界を徐々に拡張します。これにより、局所性を維持します。
*   **次元指向のデコーディングヘッド:** 空間-時間空間における複数の隣接トークンの並列予測を可能にするために、次元指向のデコーディングヘッドのセットを導入します。各ヘッドは、互いに直交する次元に沿って次のトークンを予測します。例えば、画像生成では水平方向と垂直方向の2つのヘッドを使用し、ビデオ生成では時間軸を加えた3つのヘッドを使用します。
*   **並列推論:** 推論中、デコードされたトークンに隣接するすべてのトークンを並列に処理することで、生成に必要なモデルのフォワードステップを大幅に削減します。
*   **近接性を考慮した注意マスク:** トークンのデコード順序を初期トークンからの近接性に基づいて決定し、近傍性を意識した因果的な注意マスクを使用することで、トレーニングの効率化と性能向上を図ります。同一ステップで生成されたトークン間では双方向の注意を適用し、並列生成中の一貫性を高めます。
*   **オーバーラップ領域の予測の組み合わせ:** 異なるデコーディングヘッドによって予測された隣接トークンがオーバーラップする場合、複数のデコーディングヘッドからの予測を組み合わせることで、モデルアンサンブルのような効果を得て、生成性能を向上させます。

## 3. 結果、何が達成できたのか

NAR は、画像とビデオの生成タスクにおいて、既存の手法と比較して優れた結果を達成しました。主な成果は以下の通りです。

*   **スループットの向上:** ImageNet 256x256 で 2.4 倍、UCF101 で 8.6 倍のスループット向上を達成しました。これは、PAR-4X アプローチと比較して、より高速な画像およびビデオ生成が可能になったことを意味します。
*   **生成品質の向上:** ImageNet 256x256 および UCF101 の両方で、PAR-4X よりも優れた FID/FVD スコアを達成しました。これは、より高品質な画像およびビデオ生成が可能になったことを示しています。
*   **テキストから画像生成の性能:** GenEval ベンチマークで評価した結果、0.8B パラメータの NAR は、Chameleon-7B を上回り、使用するトレーニングデータはわずか 0.4 でした。これは、少ないデータでも高品質なテキストから画像生成が可能になったことを示しています。
*   **LlamaGenと比較して、より少ないステップ数で同等以上のFIDを達成:** ImageNetのクラス条件付き画像生成では、LlamaGen-XLモデルと比較して、生成ステップ数を大幅に削減しました。また、UCF-101のクラス条件付きビデオ生成では、既存のnext-token ARメソッドと比較して、競争力のあるFVDを達成しました。
*   **効率的な学習:** VARのようなマルチスケールアプローチと比較して、NARは特殊なマルチスケールトークナイザーを必要とせず、トークンシーケンスも短いため、学習時のオーバーヘッドを大幅に削減します。

## 4. Limitationや問題点は何か

論文で言及されている制限事項と、それ以外に考えられる問題点を以下に示します。

*   **中規模の画像トークナイザーの使用:** NAR の性能を既存の手法と公平に比較するために、中規模の画像トークナイザーを使用しています。より高度なビジュアルトークナイザーとの統合は今後の課題です。より高画質の画像生成には、より高性能なトークナイザーが不可欠です。
*   **大規模ビデオデータセットでの評価:** ビデオ生成の実験は、クラス条件付きベンチマーク UCF-101 に限定されています。大規模なビデオデータセットで NAR をトレーニングすることで、さらに優れたビデオ生成結果が得られると期待されます。
*   **オーバーラップ領域の予測の組み合わせ:** 異なるデコーディングヘッドによって予測された隣接トークンがオーバーラップする場合に、予測を単純に混合するアンサンブル的なアプローチを取っています。より洗練された組み合わせ方法（例えば、学習可能な重み付けなど）を検討することで、更なる性能向上が期待できます。
*   **計算コスト:** 並列処理により推論速度は向上していますが、次元指向のデコーディングヘッドの追加や、近接性を考慮した複雑な注意マスクを使用しているため、モデルのパラメータ数が増加し、計算コストが増加する可能性があります。
*   **初期トークンの選択:** 初期トークンの位置（通常は左上）に依存しているため、初期位置が生成結果に与える影響を十分に調査する必要があります。初期位置の変更や、複数の初期位置からの生成を組み合わせることで、生成品質が向上する可能性があります。
*   **長距離依存性のモデリング:** NARは局所性を重視していますが、長距離の依存関係を捉える能力は、既存のTransformerモデルと比較して低い可能性があります。グローバルな情報を効率的に組み込むメカニズムを検討する必要があります。

## 5. 技術的な詳細について

NAR の技術的な詳細を以下に示します。

*   **アーキテクチャ:** デコーダーのみの Transformer アーキテクチャを採用し、次元指向のデコーディングヘッドを追加します。Transformerの層数はモデルのサイズによって調整されます（例：NAR-L, NAR-M）。
*   **次元指向のデコーディングヘッド:** 空間（画像）の場合、水平方向と垂直方向の2つのヘッドを使用し、ビデオの場合は時間軸を追加して3つのヘッドを使用します。各ヘッドは、Transformerブロックと全結合層で構成され、相互に直交する次元に沿って次のトークンを予測します。
*   **トレーニング:**
    *   画像トークナイザーには、オフザシェルフのものが使われており、ImageNetでトレーニングされた72Mパラメータのものなどが利用可能です。
    *   すべてのデコーディングヘッドは、クロスエントロピー損失でトレーニングされ、異なる次元に沿って次のトークンを予測します。
    *   近接性を考慮した因果的な注意マスクを使用し、同一ステップで生成されたトークン間では双方向の注意を適用します。
*   **推論:**
    1.  初期トークン（通常は左上）から開始します。
    2.  各ステップで、以前に生成されたトークンを入力として、次元指向のデコーディングヘッドを使用して隣接するトークンを並列に生成します。
    3.  オーバーラップ領域の予測は、複数のデコーディングヘッドからの予測を混合します。
    4.  マンハッタン距離に基づいてトークンを生成する順序を決定します。

疑似コード:

```python
def generate_image_nar(model, initial_token, image_size, num_heads):
    """NARによる画像生成."""

    tokens = [[None] * image_size for _ in range(image_size)] # 初期化
    tokens[0][0] = initial_token  # 初期トークンを配置
    decoded = {(0, 0)} # デコード済みトークンの集合

    # 各トークンのマンハッタン距離を計算
    distances = {}
    for r in range(image_size):
        for c in range(image_size):
            distances[(r, c)] = abs(r - 0) + abs(c - 0)

    # 距離が近い順にソート
    sorted_coords = sorted(distances.items(), key=lambda item: item[1])

    step = 1
    while len(decoded) < image_size * image_size:
        # 各デコード済みトークンの隣接トークンを予測
        to_decode = set()
        for r, c in list(decoded):
            neighbors = [(r + 1, c), (r, c + 1)] # 2次元の場合
            for nr, nc in neighbors:
                if 0 <= nr < image_size and 0 <= nc < image_size and (nr, nc) not in decoded:
                    to_decode.add((nr, nc))

        # モデルに入力する準備
        input_tokens = [] # モデルへの入力トークンを格納
        input_coords = [] # 入力トークンの座標を格納
        for r,c in decoded:
            input_tokens.append(tokens[r][c])
            input_coords.append((r,c))

        # 並列に予測
        predictions = model.predict(input_tokens, input_coords) # model.predict は各headからの予測を返す

        # オーバーラップを処理し、トークンを更新
        for i, (r,c) in enumerate(to_decode):
            head_predictions = [pred[i] for pred in predictions] # 各ヘッドからの予測

            # オーバーラップがあれば、予測を組み合わせる (例: 平均)
            if (r,c) in decoded:
                tokens[r][c] = combine_predictions(head_predictions)
            else:
                tokens[r][c] = head_predictions[0] # 最初のヘッドの予測を採用

            decoded.add((r, c))  # デコード済みに更新
        step += 1
    return tokens
```

## 6. コストや物理的な詳細について

論文に記載されている情報に基づいて、コストと物理的な詳細を以下に示します。

*   **ハードウェア:** 実験は、A100 GPU で行われました。
*   **データセット:**
    *   ImageNet: クラス条件付き画像生成に使用。
    *   UCF-101: クラス条件付きビデオ生成に使用。
    *   LAION-COCO (4M) + 高品質画像 (2M): テキストから画像生成に使用。
*   **モデルサイズ:**
    *   NAR-L: 372M パラメータ
    *   NAR-M: パラメータ数は不明だが、NAR-L より少ない
    *   NAR-XL: パラメータ数は不明
    *   Chameleon-7B: 7B パラメータ
*   **トレーニング:**
    *   ImageNet: 300 エポック、学習率 10^-4
    *   UCF-101: 3000 エポック、学習率 10^-4
    *   テキストから画像生成: LAION-COCO (4M) で 60 エポック、高品質画像 (2M) で 40 エポック。

具体的な GPU の数やトレーニング時間は明記されていませんが、大規模なモデルとデータセットを使用していることから、相当な計算リソースが必要とされます。

## 7. 参考文献のうち、特に参照すべきもの

NAR の理解を深めるために、以下の参考文献は特に参照すべきです。

*   **PixelCNN (Van den Oord et al., 2016):** Autoregressive モデルの基礎となる研究。
*   **Taming Transformers for High-Resolution Image Synthesis (Esser et al., 2021):** 画像生成における Transformer の利用。
*   **Language model beats diffusion–tokenizer is key to visual generation (Yu et al., 2023):** トークナイザーの重要性を示した研究。
*   **Llama: Open and efficient foundation language models. (Touvron et al, 2023):** ベースとなっているLLM
*   **Autoregressive model beats diffusion: Llama for scalable image generation. (Sun et al., 2023):** 比較対象となっているLlamaGen

## 8. この論文を140字以内のツイートで要約すると？

新しい視覚生成モデルNAR登場！近傍トークン予測で高速＆高画質化を実現。次元指向デコーディングヘッドで並列処理も。ImageNetで2.4倍速、テキストから画像生成でもChameleon-7B超え！ #AI #画像生成 #Autoregressive


---


# ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges

[View Paper](http://arxiv.org/abs/2503.06553v1)

## 1. 既存研究では何ができなかったのか

既存のマルチモーダル大規模言語モデル（MLLM）のプロセス評価に関する研究は、以下の点で限界がありました。

*   **網羅的な評価ベンチマークの欠如:** MLLMベースのプロセス評価 judge の能力を評価するための、包括的なベンチマークが存在しませんでした。既存のベンチマークは、単一のモダリティ、単一の専門分野、または限られた難易度の問題に限定されていることが多く、現実世界の推論タスクの多様な課題を捉えきれていませんでした。
*   **詳細なエラー診断能力の評価不足:** 既存のベンチマークは、主にエラー検出に焦点が当てられており、judge の詳細なエラー診断能力の評価が不足していました。エラーの検出だけでなく、エラーの種類を特定し、その根本原因を診断する能力は、judge の説明可能性を高め、モデルの弱点を明らかにするために重要です。
*   **現実的なエラーパターンの反映不足:** 既存のベンチマークの多くは、特定のポリシーモデルに特化しているか、または合成エラーデータ（例：GPTで修正されたエラー）に基づいており、実際にはモデルの解法で観察される多様なエラーパターンを反映していませんでした。そのため、現実世界のシナリオでの適用が制限されていました。
*   **オープンソースのプロセス評価 judge の開発遅延:** 既存の評価はプロプライエタリモデルに依存しており、コストが高く、再現性が不安定でした。オープンソースのプロセス評価 judge の開発が急務でした。
*   **中間推論ステップの重要性の軽視:** 既存のベンチマークは最終的な回答の正確性に重点を置いており、中間的な推論ステップの重要な情報を見落としている可能性がありました。これにより、不正確な推論による正しい回答が生み出され、モデルの能力が過大評価される可能性がありました。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、上記の問題を解決するために、以下の新しいアプローチを提案しました。

*   **ProJudgeBench の導入:** MLLMベースのプロセス評価 judge の能力を評価するために特別に設計された、初の包括的なベンチマークである ProJudgeBench を導入しました。ProJudgeBench は、数学、物理学、化学、生物学の4つの科学分野にまたがる、多様な難易度レベルとマルチモーダルコンテンツを備えた 2,400 のテストケースと 50,118 のステップレベルのラベルで構成されています。各ステップは、人間expertによって、正確さ、エラーの種類、および説明について細心の注意を払って注釈が付けられています。これにより、judge のエラーの検出、分類、および診断能力を体系的に評価できます。

*   **ProJudge-173k の提案:** 大規模なインストラクションチューニングデータセットである ProJudge-173k を提案しました。このデータセットは、多様性と現実世界との関連性の両方を確保するために、2つの相補的な経路で構築されています。厳格なフィルタリングにより、高品質な合成アノテーションが保証され、オープンソースモデルをプロセス評価 judge としてファインチューニングするための基盤が提供されます。データセットは、正解に意図的にエラーを注入するパスと、多様なMLLMで生成された解答をアノテーションするパスで構成されています。

*   **Dynamic Dual-Phase (DDP) ファインチューニング戦略の提案:** モデルがソリューションを評価する前に、問題解決のステップを明示的に推論することを奨励する、Dynamic Dual-Phase ファインチューニング戦略を提案しました。この戦略は、人間の専門家の行動を模倣し、モデルの問題理解を深め、プロセス評価 judge のロバスト性と一般化可能性を向上させます。DDPは、Direct EvaluateフェーズとSynthesize-then-Evaluateフェーズを動的に切り替えることで、多様な学習を促します。

## 3. 結果、何が達成できたのか

この論文の結果として、以下の成果が達成されました。

*   **オープンソースモデルのプロセス評価能力の大幅な向上:** ProJudge-173k と DDP 戦略でファインチューニングすることにより、オープンソースモデルのプロセス評価能力が大幅に向上し、プロプライエタリシステムとの性能差が縮まりました。例えば、InternVL2.5-8BとQwen2.5-VL-3Bは、ファインチューニング後、ステップの正しさの精度がそれぞれ58.92%と70.45%向上しました。
*   **詳細なモデルの弱点の特定:** ProJudgeBench を使用した広範な実験により、現在のモデルにおける主要な課題と限界が明らかになり、マルチモーダル推論とプロセス評価に関する貴重な洞察が得られました。例えば、多くのMLLMは生物学と化学のタスクでより高い精度を示しましたが、数学と物理学では精度が低く、画像を含むタスクではオープンソースモデルが課題に直面していることが明らかになりました。
*   **信頼性の高いマルチモーダルプロセス評価のための基盤の確立:** この研究は、マルチモーダル推論におけるプロセス評価の基礎を築き、この重要な分野における今後の研究を促進することが期待されます。ProJudgeBench、ProJudge-173k、および DDP 戦略は、今後の研究のための貴重なリソースとして役立ちます。
*   **Dynamic Dual-Phase(DDP) fine-tuning戦略の有効性:**DDP戦略は、標準的なfine-tuningと比較して、一貫してパフォーマンスが向上し、特にオリンピックアリーナのテストセットにおける汎化性能が向上しました。

## 4. Limitationや問題点は何か

この論文にはいくつかの limitation と問題点があります。

*   **データセットの偏り:** ProJudgeBench は、特定の科学分野と難易度レベルに焦点を当てているため、データセットに偏りが存在する可能性があります。より多様な分野と難易度レベルを網羅した、より大規模なデータセットが必要となる場合があります。また、annotationの品質は人expertに依存するため、annotationの偏りも考慮する必要があります。
*   **評価指標の限界:** 精度は主要な評価指標として使用されていますが、他の評価指標（例：適合率、再現率、F1スコア）も、モデルのパフォーマンスをより包括的に評価するために考慮されるべきです。また、エラーの種類分類の精度だけでなく、エラーの重大度や影響も考慮した評価指標を導入することで、より詳細な分析が可能になります。
*   **計算コスト:** ProJudge-173k を用いた DDP 戦略によるファインチューニングは、計算コストが高く、リソースに制約のある研究者にとっては利用が難しい場合があります。より効率的なファインチューニング手法の開発が望まれます。
*   **オープンソースモデルの性能限界:** ファインチューニング後も、オープンソースモデルの性能はプロプライエタリモデルに完全に匹敵するわけではありません。これは、モデルのアーキテクチャ、学習データ、またはトレーニング手順の根本的な違いが原因である可能性があります。
*   **評価judge自体の信頼性の問題:** MLLMを評価judgeとして利用すること自体の信頼性について、さらなる検討が必要です。Judgeが持つバイアスやエラーが、評価結果に影響を与える可能性があります。

## 5. 技術的な詳細について

*   **ProJudgeBench の構築:** ProJudgeBench は、数学、物理学、化学、生物学の4つの科学分野にまたがる 400 の科学問題から構成されています。問題は、小学校、中学校、高校、および競技レベルの問題を網羅するようにキュレーションされています。10種類の MLLM を使用して解答を生成し、Qwen2.5-72B-Instruct を使用して解答を論理的で段階的なステップに分割しました。次に、ドメインexpertが各ステップの正しさ、エラーの種類、および説明を注釈しました。7種類のエラータイプが定義され、アノテーションされています。
*   **ProJudge-173k の構築:** ProJudge-173k は、2つの相補的な経路で構築されています。1つ目の経路では、GPT-4o を使用して正解に意図的にエラーを注入しました。2つ目の経路では、9種類の MLLM を使用して解答を生成し、GPT-4o を使用して各ステップの正しさ、エラーの種類、および説明を評価しました。厳格なフィルタリングプロセスを適用して、データ品質を確保しました。
*   **Dynamic Dual-Phase (DDP) ファインチューニング戦略:** DDP 戦略は、Direct Evaluate フェーズと Synthesize-then-Evaluate フェーズの2つのフェーズで構成されています。Direct Evaluate フェーズでは、モデルは生徒の解答を直接評価します。Synthesize-then-Evaluate フェーズでは、モデルは最初に問題を解き、次にその解答に基づいて生徒の解答を評価します。ファインチューニング中、モデルは確率 $p$ で2つのフェーズを動的に切り替えます。

    ```python
    # Dynamic Dual-Phase fine-tuningの疑似コード
    for epoch in range(num_epochs):
        for batch in data_loader:
            # DDPにおけるフェーズの選択
            phase = random.choice(["Direct Evaluate", "Synthesize-then-Evaluate"])

            if phase == "Direct Evaluate":
                # Direct Evaluate フェーズ
                student_solution = batch["student_solution"]
                problem = batch["problem"]
                ground_truth_annotations = batch["ground_truth_annotations"]

                # モデルによる評価
                predictions = model(problem, student_solution)

                # 損失計算
                loss = calculate_loss(predictions, ground_truth_annotations)

            elif phase == "Synthesize-then-Evaluate":
                # Synthesize-then-Evaluate フェーズ
                problem = batch["problem"]
                ground_truth_solution = batch["ground_truth_solution"]
                student_solution = batch["student_solution"]
                ground_truth_annotations = batch["ground_truth_annotations"]

                # モデルによる問題解決 (解の合成)
                synthesized_solution = model(problem, instruction="問題を解いてください")

                # 合成解に基づく生徒の解答の評価
                predictions = model(synthesized_solution, student_solution, instruction="生徒の解答を評価してください")

                # 損失計算 (2つのタスクに対する損失)
                loss_synthesis = calculate_loss(synthesized_solution, ground_truth_solution)
                loss_evaluation = calculate_loss(predictions, ground_truth_annotations)

                loss = loss_synthesis + loss_evaluation

            # 勾配計算とモデル更新
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
    ```

## 6. コストや物理的な詳細について

*   **データセット:** ProJudgeBench は 2,400 のテストケースと 50,118 のステップレベルのラベルで構成されています。ProJudge-173k は、173,000 件のインストラクションチューニングデータで構成されています。
*   **モデル:** InternVL2.5-8B、Qwen2.5-VL-3B-instruct、および Qwen2.5-VL-7B-Instruct を使用して実験を実施しました。
*   **ハードウェア:** トレーニングは 8 つの H100 GPU で実行されました。
*   **トレーニングの詳細:** すべてのモデルは、LoRA を使用して 1 エポックファインチューニングされました。グローバルバッチサイズは 16 に設定され、デバイスごとのバッチサイズは 4、勾配累積ステップは 2 でした。InternVL2.5-8B では、学習率は 4e-5 に設定され、コサイン学習率スケジューラと 0.03 のウォームアップ比を使用して最適化されました。重み減衰は 0.05 に設定されました。Qwen2.5-VL-3B と Qwen2.5-VL-7B では、学習率は 1.0e-4 に設定され、コサイン学習率スケジューラと 0.1 のウォームアップ比が使用されました。混合精度トレーニング (bf16) が使用され、InternVL2.5-8B では勾配チェックポイントが有効になりました。

## 7. 参考文献のうち、特に参照すべきもの

*   **OlympiadBench:** 数理オリンピックレベルのバイリンガルマルチモーダル科学問題のベンチマーク。ProJudgeBench の問題の難易度設定や、MLLMの推論能力評価の参考になる。
*   **VLMEvalKit:** 大規模マルチモーダルモデルを評価するためのオープンソースツールキット。実験設定やハイパーパラメータの参考になる。
*   **LoRA:** 大規模言語モデルの低ランク適応に関する論文。ファインチューニング手法の参考になる。

## 8. この論文を140字以内のツイートで要約すると？

MLLMのプロセス評価benchmark #ProJudgeBench を発表！多様な科学問題と詳細なエラー注釈で、モデルの推論能力を徹底評価。大規模データセット #ProJudge173k と新fine-tuning戦略で、オープンソースモデルの性能を大幅向上！ #MLLM #AI


---


# Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?

[View Paper](http://arxiv.org/abs/2503.11207v1)

## 1. 既存研究では何ができなかったのか

既存研究における課題は、主に以下の2点です。

*   **知覚的不確実性の考慮欠如:** 既存の抽象視覚推論ベンチマーク、特に I-RAVEN-X は、理想的な（oracle）知覚を前提としており、実際の視覚情報抽出に伴う不確実性を考慮していませんでした。つまり、入力属性の値が完全に正確かつ離散的であると仮定していました。これは、現実世界のシナリオとは大きく異なります。
*   **大規模推論モデル（LRM）の脆弱性評価の不足:** 最新の LRM は、流暢なテキスト生成能力を持つ一方で、推論能力、特に抽象視覚推論における能力は、知覚的不確実性の影響を受けやすいことが示唆されていました。しかし、その脆弱性を詳細に評価した研究は不足していました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の課題を解決するために、以下の2つの主要なアプローチを採用しました。

*   **I-RAVEN-X データセットの拡張:** 知覚的不確実性をシミュレートするために、I-RAVEN-X データセットを拡張しました。具体的には、以下の2つの要素を導入しました。
    1.  **撹乱属性（Confounding Attributes）:** パズルの正解予測に寄与しないランダムな属性を導入しました。これにより、モデルはノイズの多い属性をフィルタリングする必要が生じます。
    2.  **属性値分布の平滑化（Smoothing Attribute Values）:** 元の I-RAVEN-X における属性値の縮退分布を緩和しました。ガウシアンフィルタまたは3ビン戦略を用いて、属性値の確率分布にばらつきを持たせました。
*   **LRM と NeSy モデルのベンチマーク:** 拡張された I-RAVEN-X データセットを用いて、OpenAI の o3-mini と DeepSeek R1 という2つの最先端 LRM をベンチマークしました。さらに、神経記号確率的アブダクションモデルである ARLC を評価し、LRM との比較を行いました。

## 3. 結果、何が達成できたのか

本研究の結果、主に以下の3点が明らかになりました。

*   **LRM の知覚的不確実性に対する脆弱性:** OpenAI の o3-mini と DeepSeek R1 は、I-RAVEN-X において、入力の長さと範囲が増加し、知覚的不確実性が導入されると、タスクの精度が大幅に低下しました。o3-mini の精度は、元の I-RAVEN の 86.6% から I-RAVEN-X のわずか 17.0% に低下しました。DeepSeek R1 も同様の傾向を示し、80.6% から 23.2% に低下しました。
*   **NeSy モデルのロバスト性:** 神経記号確率的アブダクションモデルである ARLC は、これらの分布外テストにおいてロバストに推論することができ、精度を大幅に維持しました。ARLC の精度は、98.6% から 88.0% へのわずかな低下にとどまりました。
*   **撹乱属性フィルタリングにおけるアブダクションモデルの優位性:** ARLC は、エントロピーベースの信頼性指標を用いることで、撹乱属性を効果的にフィルタリングできることが示されました。

## 4. Limitationや問題点は何か

本研究の limitation と問題点は以下の通りです。

*   **LRM のプロンプトエンジニアリングの限界:** LRM の性能はプロンプトに大きく依存しますが、本研究ではプロンプトの複雑さを一定に保つために、エンタングルドプロンプトを使用しました。より高度なプロンプトエンジニアリングによって、LRM の性能が向上する可能性はあります。
*   **テストセットのサイズ:** LRM の評価には、計算コストの制約から、I-RAVEN と I-RAVEN-X のサブセット（500サンプル）を使用しました。より大規模なテストセットを使用することで、より信頼性の高い結果が得られる可能性があります。
*   **ARLC の汎用性の限界:** ARLC は、RPM タスクにおいて高い性能を発揮しますが、そのアーキテクチャはドメイン固有であり、LRM のように異なるドメインに直接汎化することはできません。
*   **シミュレートされた知覚的不確実性:** 本研究では、知覚的不確実性をシミュレートするために、撹乱属性と属性値分布の平滑化という 2 つの戦略を採用しましたが、これは現実世界の知覚的不確実性の完全な表現ではありません。例えば、本研究では視覚的な要素を考慮していません。
*   **データセットの偏り:** I-RAVEN と I-RAVEN-X は生成的なデータセットですが、特定のルールに基づいて生成されているため、現実世界の視覚推論タスクの複雑さを完全に捉えているとは限りません。
*   **実験設定の簡略化:** 3ビン戦略は prompt の複雑さを抑えるために採用されましたが、ガウシアンフィルタによる平滑化と比べて、分布のバリエーションが限られています。

## 5. 技術的な詳細について

*   **データセット拡張:**
    *   **撹乱属性:** 各パネルの属性セットに、[0, dynamic_range] の範囲で一様にサンプリングされた撹乱因子を追加しました。
        ```python
        def add_confounders(panel_attributes, num_confounders, dynamic_range):
          for _ in range(num_confounders):
            confounder_value = random.randint(0, dynamic_range)
            panel_attributes.append(confounder_value)
          return panel_attributes
        ```
    *   **属性値分布の平滑化:** 属性値の確率分布を、ガウシアンフィルタまたは3ビン戦略を用いて平滑化しました。3ビン戦略では、真の値 *T* の確率 *p(T)* を U(p_L, 1) からサンプリングし、隣接する2つの値 *N1* と *N2* の確率を U(0, 1 - p(T)) からサンプリングしました。
        ```python
        def smooth_distribution_3bins(true_value_index, num_values, p_L=0.5):
          p_T = random.uniform(p_L, 1)
          p_N1 = random.uniform(0, 1 - p_T)
          p_N2 = 1 - p_T - p_N1
          distribution = [0] * num_values
          distribution[true_value_index] = p_T
          if true_value_index > 0:
            distribution[true_value_index - 1] = p_N1
          else:
            distribution[num_values - 1] = p_N1
          if true_value_index < num_values - 1:
            distribution[true_value_index + 1] = p_N2
          else:
            distribution[0] = p_N2
          return distribution
        ```

*   **モデル評価:**
    *   **LRM:** OpenAI API を通じて o3-mini を利用し、DeepSeek R1 は Together AI の API またはローカル環境（8 x NVIDIA A100 GPU）で実行しました。最大推論トークン数を 25,000 に設定しました。
    *   **ARLC:** 確率的アブダクションフレームワークに、エントロピーベースの信頼性指標を導入しました。具体的には、各属性のスコア/損失への貢献度を、その属性の信頼値のエントロピーで正規化しました。
        ```python
        def entropy_regularization(loss_attr, confidence_vector):
          entropy = -sum([p * math.log(p) for p in confidence_vector if p > 0])
          if entropy == 0:
              return loss_attr # avoid division by zero
          return loss_attr / entropy
        ```

## 6. コストや物理的な詳細について

*   **モデル:**
    *   OpenAI o3-mini (クローズドソース)
    *   DeepSeek R1 (671Bパラメータ, Together AI API または 8 x NVIDIA A100 GPU)
    *   ARLC (NeSy probabilistic abductive model)
*   **データセット:** I-RAVEN, I-RAVEN-X
*   **実験設定:**
    *   LRM の評価には、I-RAVEN と I-RAVEN-X のサブセット（500 サンプル）を使用
    *   o3-mini は最大 100,000 トークンで評価 (ただし、コスト効率のため 25,000 トークンが標準)
    *   DeepSeek R1 (および distilled 版) は 25,000 トークンで評価
*   **コスト:**
    *   OpenAI o1: $15 / $60 (入力/出力 100万トークンあたり)
    *   OpenAI o3-mini: $1.1 / $4.4 (入力/出力 100万トークンあたり)

## 7. 参考文献のうち、特に参照すべきもの

*   **Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. RAVEN: A Dataset for Relational and Analogical Visual REasoNing. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5312–5322, Long Beach, CA, USA, June 2019. IEEE.** I-RAVEN データセットのオリジナル論文。
*   **Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, and Abbas Rahimi. A Neuro-vector-symbolic Architecture for Solving Raven’s Progressive Matrices. https://www.nature.com/articles/s42256-023-00630-8.** ARLC のベースとなる neuro-symbolic アーキテクチャに関する論文。

## 8. この論文を140字以内のツイートで要約すると？

LRM(o3-mini, DeepSeek R1)は知覚的不確実性下での類推推論に脆弱。I-RAVEN-Xを拡張し評価した結果、精度が大幅低下。一方、NeSyモデル(ARLC)はロバスト。オープンな類推推論モデルへの道は遠い #AI #推論 #視覚 #類推


---


# Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption

[View Paper](http://arxiv.org/abs/2503.09279v1)

## 1. 既存研究では何ができなかったのか

既存のVideo Detailed Captioning (VDC) モデルは、以下の2つの主要な課題を抱えていました。

*   **偏った詳細なビデオキャプションの整合性 (Imbalanced Fine-grained Video-Caption Alignment)**: 既存のモデルは、ビデオの特定の側面 (主要なオブジェクト、背景、カメラの動きなど) に注力しがちで、他の側面が不足していました。つまり、すべての側面で高い精度と網羅性を持つ包括的なキャプションを生成できるモデルが存在しませんでした。VDCSCOREという指標で評価したところ、特定の側面では優れているものの、他の側面では性能が低いという傾向が見られました。

*   **人間の好みとの不一致 (Misalignment with Human Preference)**: 既存のVDCモデルは、主に合成データで訓練されており、人間の好みを考慮していません。そのため、人間が好むような自然で詳細なキャプションを生成することが困難でした。公開されている人間によるアノテーションデータが不足していることが原因の一つです。商業モデル（GPT-4oなど）で生成したデータを使用することも考えられますが、一般的な視覚対話とVDCとのタスクギャップ、API呼び出しのコストと効率の問題がありました。

## 2. どのようなアプローチでそれを解決しようとしたか

これらの課題を解決するために、Cockatielという名前の新しい3段階の学習パイプラインを提案しました。このパイプラインは、合成データと人間が好むデータを組み合わせてVDCの性能を向上させることを目的としています。

1.  **高品質な合成データのキュレーション**: まず、人間がアノテーションしたデータセットから、キャプションの品質を評価するスコアラーを学習します。このスコアラーは、ビデオとキャプションの整合性、特に人間が好むかどうかを評価します。このスコアラーを用いて、複数の既存のVDCモデルが生成したキャプションを評価し、特定の側面で性能が高く、かつ人間が好むキャプションを選択します。低いスコアのキャプションは除外します。

2.  **Cockatiel-13Bの訓練**: キュレーションされたデータセットを用いて、Cockatiel-13Bという130億パラメータのモデルを訓練します。これにより、既存モデルの強みと人間の好みが組み込まれたモデルを作成します。

3.  **Cockatiel-8Bの蒸留**: 最後に、Cockatiel-13BからCockatiel-8Bという80億パラメータのモデルを蒸留します。これにより、より使いやすいサイズのモデルを提供します。Cockatiel-13Bを教師モデルとして、より小規模なCockatiel-8Bを訓練することで、性能を維持しながらモデルサイズを削減します。

## 3. 結果、何が達成できたのか

提案手法であるCockatielは、以下の成果を達成しました。

*   **VDCSCOREでの最先端性能**: VDCSCOREにおいて、既存のVDCモデルを上回る最先端の性能を達成しました。特に、すべての側面においてバランスの取れた性能を示しました。
*   **人間による評価で高い評価**: 人間による評価において、他のモデルと比較して、Cockatielが生成したキャプションが最も好まれるという結果が得られました。
*   **詳細なキャプションの生成**: Cockatiel-13Bは、入力ビデオのすべての視覚要素と一貫性のある詳細なキャプションを生成することができます。

これらの成果は、合成データと人間が好むデータを組み合わせた訓練が、VDCの性能向上に効果的であることを示しています。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

論文で言及されている制限事項:

*   **スコアラーの性能と基盤モデルの能力への依存**: メソッドの性能は、キャプション品質を評価するスコアラーの精度と、アンサンブルされた基盤モデルの能力に制限されます。基盤モデルが特定の側面で性能が低い場合、高品質な合成データを作成できず、その側面に関する知識をキャプションモデルに注入できません。スコアラーの精度が低い場合も同様に、高品質な訓練データを得ることができません。
*   **LLMにおけるハルシネーション**: LLMを使用する他の研究と同様に、Cockatielもハルシネーションが発生する可能性があります。つまり、もっともらしいものの、事実と異なる、または入力ビデオと一致しないコンテンツを生成する可能性があります。
*   **計算資源の制限**: この論文では、単一のGPUに収まるVDCモデル（34Bパラメータ以下）に焦点を当てています。したがって、34Bを超える大規模なモデルには、論文の結論が当てはまらない可能性があります（特にLoRAランクとトレーニングデータセットのサイズに関する結論）。
*   **短いビデオとシーンのトランジションがないビデオへの制限**: キャプションモデルは主に60秒未満のビデオで訓練されており、シーンのトランジションがないビデオで訓練されているため、より長いビデオやシーンのトランジションを含むビデオを処理する際にはパフォーマンスが制限されます。

その他の問題点:

*   **データセットの偏り**: OpenVid-1Mからビデオをサンプリングしているため、このデータセットの偏りがモデルの性能に影響を与える可能性があります。
*   **評価指標の限界**: VDCSCOREは有用な指標ですが、キャプションの品質を完全に反映しているとは限りません。より多様な評価指標（例えば、多様性や流暢さを評価する指標）を使用することで、モデルの性能をより詳細に評価できる可能性があります。
*   **人間による評価の主観性**: 人間による評価は主観的であり、評価者のバックグラウンドやバイアスに影響される可能性があります。評価者の多様性を確保し、評価基準を明確にすることで、この問題を軽減できる可能性があります。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

Cockatielは、以下の3つの主要なコンポーネントで構成されています。

1.  **人間が好むキャプション品質スコアラー (Human-Aligned Caption Quality Scorer)**:
    *   **アーキテクチャ**: VILA-v1.5-13Bを基盤として、LoRA (Low-Rank Adaptation) を用いてファインチューニングします。
    *   **訓練データ**: OpenVid-1Mからサンプリングされたビデオと、複数の基盤モデルが生成したキャプションからなるデータセットを使用します。各ビデオ-キャプションペアに対して、人間がアノテーションした構造化された品質スコア（オブジェクト、オブジェクトの特徴、オブジェクトの動作、カメラの動き、背景の5つの側面）が付与されます。
    *   **損失関数**: 5つの側面ごとに、0から5までの整数値で評価します。評価の際に、各要素について質問応答タスクを設定し、その平均値を最終的な品質スコアとします。
    *   **実装**: 質問応答ペアの形式で20,040件のデータを使用し、VILA-v1.5-13BモデルのコネクタとLLMモジュールの線形レイヤーをLoRAでファインチューニングします。LoRAのランクは256、アルファは512に設定します。
    *   Python風疑似コード:

    ```python
    def calculate_quality_score(video, caption, scorer_model):
      """
      Calculates the quality score of a video-caption pair.

      Args:
        video: Input video.
        caption: Generated caption for the video.
        scorer_model: Fine-tuned MLLM scorer.

      Returns:
        Quality score of the video-caption pair.
      """
      scores = []
      element_types = ["object", "object_feature", "object_action", "camera_movement", "background"]

      for element_type in element_types:
        score = scorer_model.evaluate(video, caption, element_type) # 1 to 5

        if score != 0:
          scores.append(score)

      if not scores:
        return 0 # Handle the case when all scores are 0

      quality_score = sum(scores) / len(scores)
      return quality_score
    ```

2.  **アンサンブル訓練 (Ensemble Training)**:
    *   **データキュレーション**: VDCSCOREの5つの側面（"Main Object", "Background", "Camera", "Short", "Detailed"）ごとに、OpenVid-1Mから100kのビデオをサンプリングし、3つの基盤モデルでキャプションを生成します。スコアラーを用いて各キャプションの品質を評価し、最も高いスコアを持つキャプションを、設定された閾値（3.5）を超える場合に限り、訓練データとして選択します。各側面につき20kのビデオ-キャプションペアを選択し、合計100kのペアからなる訓練データセットを作成します。
    *   **モデル訓練**: キュレーションされたデータセットで、VILA-v1.5-13Bをファインチューニングします。
    *   Python風疑似コード:

    ```python
    def curate_training_data(videos, base_models, scorer_model, threshold=3.5, num_videos_per_perspective=20000):
      """
      Curates a high-quality training dataset by ensembling captions from multiple models.

      Args:
        videos: A list of videos to be captioned.
        base_models: A list of VDC models used to generate captions.
        scorer_model: The caption quality scorer.
        threshold: Minimum quality score for a caption to be included in the training set.

      Returns:
        A curated training dataset consisting of video-caption pairs.
      """
      training_data = []
      perspectives = ["Main Object", "Background", "Camera", "Short", "Detailed"]

      for perspective in perspectives:
        candidate_captions = []
        for video in videos:
          best_caption = None
          best_score = -1

          for model in base_models:
            caption = model.generate_caption(video, perspective)
            score = scorer_model.calculate_quality_score(video, caption)

            if score > best_score:
              best_caption = caption
              best_score = score

          if best_score >= threshold:
            candidate_captions.append((video, best_caption))

        # Randomly sample to balance the inter-dimension proportion
        training_data.extend(random.sample(candidate_captions, num_videos_per_perspective))

      return training_data
    ```

3.  **モデル蒸留 (Model Distillation)**:
    *   Cockatiel-13Bを教師モデルとして、VILA-v1.5-8Bを生徒モデルとして訓練します。蒸留の際、基盤モデルのアンサンブル訓練と同様の手法を用います。つまり、Cockatiel-13Bもキャプションを生成するモデルの一つとして加え、スコアラーを用いて最も適切なキャプションを選択します。これにより、Cockatiel-13Bの知識をCockatiel-8Bに効果的に伝達します。
    *   Python風疑似コード (蒸留プロセスはアンサンブル訓練と類似):

    ```python
    def distill_model(teacher_model, student_model, videos, base_models, scorer_model, threshold=3.5, num_videos_per_perspective=20000):
        """
        Distills knowledge from a teacher model (Cockatiel-13B) to a student model (Cockatiel-8B).

        Args:
            teacher_model: The teacher model to distill knowledge from.
            student_model: The student model to train.
            videos: A list of videos to be captioned.
            base_models: A list of VDC models used to generate captions (including teacher_model).
            scorer_model: The caption quality scorer.
            threshold: Minimum quality score for a caption to be included in the training set.

        Returns:
            The distilled student model.
        """
        training_data = []
        perspectives = ["Main Object", "Background", "Camera", "Short", "Detailed"]

        for perspective in perspectives:
            candidate_captions = []
            for video in videos:
                best_caption = None
                best_score = -1

                models = base_models + [teacher_model] # Include teacher model

                for model in models:
                    caption = model.generate_caption(video, perspective)
                    score = scorer_model.calculate_quality_score(video, caption)

                    if score > best_score:
                        best_caption = caption
                        best_score = score

                if best_score >= threshold:
                    candidate_captions.append((video, best_caption))

            training_data.extend(random.sample(candidate_captions, num_videos_per_perspective))

        student_model.train(training_data) # Train the student model on the distilled data

        return student_model
    ```

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

*   **モデルサイズ**:
    *   Cockatiel-13B: 130億パラメータ
    *   Cockatiel-8B: 80億パラメータ
*   **データセット**:
    *   OpenVid-1Mからサンプリングされたビデオを使用。
    *   4,008のビデオ-キャプションペアに、構造化された品質スコアのアノテーションを付与。これにより、20,040件の質問応答ペアを生成。
    *   アンサンブル訓練において、各側面（"Main Object", "Background", "Camera", "Short", "Detailed"）ごとに20kのビデオ-キャプションペアを選択し、合計100kのペアからなる訓練データセットを作成。
*   **ハードウェア**:
    *   スコアラーの訓練: 8 NVIDIA A100 GPU
    *   バッチサイズ: 16
*   **訓練**:
    *   スコアラーの学習率: 最初の3%のステップで0から2e-5までウォームアップし、コサイン学習率スケジューラを適用。
    *   LoRAランク: 256
    *   LoRAアルファ: 512

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下の通りです。

*   **VILA [Lin et al., 2023]**: Cockatielの基盤モデルとして使用されているため、モデルのアーキテクチャや事前学習に関する情報が重要です。
*   **OpenVid-1M [Kepan Nan et al., 2024]**: データセットに関する情報です。
*   **VDCSCORE**: これに関する説明が書かれている論文（論文中に明記されていないため、不明）

これらの論文を参照することで、Cockatielの基盤となる技術やデータセットについてより深く理解することができます。

## 8. この論文を140字以内のツイートで要約すると？

Cockatiel：合成データと人間評価を融合したVDCモデル。既存モデルの偏りを解消し、人間好みの詳細なキャプションを生成。VDCSCOREでSOTA達成！ #VDC #AI #動画キャプション


---


# GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving

[View Paper](http://arxiv.org/abs/2503.05689v3)

## 1. 既存研究では何ができなかったのか

既存のend-to-end自動運転システムにおける多峰性軌道生成において、以下の課題がありました。

*   **軌道品質の低下:** 拡散モデルを用いた場合、制約がないと軌道が発散しやすく、HDマップなどの外部情報に頼らざるを得ない。また、目標地点（ゴール）が曖昧だと、低品質な軌道が生成されやすい。
*   **走行可能領域外への逸脱:** 既存手法は衝突回避やL2損失の最適化に偏重し、車両が走行可能な領域内に留まるかを軽視する傾向があった。
*   **計算効率の悪さ:** DDPMのような拡散モデルは、推論時に多数のノイズ除去ステップを必要とし、リアルタイム性を要求される自動運転には不向きだった。
*   **ゴール地点の分布の考慮不足:** GoalGANなどのゴール地点を利用する手法では、ゴール地点のサンプリングにグリッドセルを使用しており、ゴール地点の分布を考慮していない。

## 2. どのようなアプローチでそれを解決しようとしたか

GoalFlowでは、上記の課題を解決するために以下の要素を取り入れています。

*   **ゴール地点による制約:** 軌道生成プロセスにゴール地点を導入することで、拡散モデルの軌道発散を抑制し、軌道品質を向上させます。
*   **シーン情報に基づくゴール地点選択:** 周囲のシーン情報に基づいて最適なゴール地点を選択するスコアリングメカニズムを導入し、走行可能領域を考慮した質の高い軌道生成を可能にします。
*   **Flow Matchingによる効率的な軌道生成:** Flow Matchingの中でもRectified Flowを使用することで、少ない推論ステップで高精度な軌道生成を実現します。
*   **シャドウ軌道によるゴール地点の信頼性検証:** 生成された軌道と並行して、ゴール地点をマスクしたシャドウ軌道を生成し、両者の乖離が大きい場合はゴール地点が信頼できないと判断し、シャドウ軌道を選択する機構を導入しました。
*   **BEV特徴量への補助的な損失の適用:** HDマップとバウンディングボックスからの損失を通じて、BEV特徴量を強化します。

## 3. 結果、何が達成できたのか

GoalFlowは、Navsim環境での実験において、以下の成果を達成しました。

*   **最先端の性能:** 他の手法を大幅に上回るPDMS（Performance Driven Motion Score）90.3を達成しました。
*   **DAC（Drivable Area Compliance）スコアの大幅向上:** ゴール地点選択メカニズムにより、走行可能領域遵守の性能が向上しました。
*   **ロバストな軌道生成:** 推論時のノイズ除去ステップ数を減らしても性能劣化が少なく、リアルタイム性能への適合性が高いことが示されました。特に、ノイズ除去ステップが1回の場合でも、最適な場合と比較してスコアの低下はわずか1.6%に留まりました。

## 4. Limitationや問題点は何か

*   **ノイズ分散の影響:** 論文中では、Flow Matchingにおける初期ノイズの分散が軌道の多様性に影響することが示唆されています。ノイズが大きすぎると軌道が不安定になり、小さすぎると多様性が失われる可能性があります。
*   **HDマップへの依存:** ゴール地点の評価にDrivable Area Compliance Score (DAC) を使用していますが、これは走行可能領域を定義するために HDマップ を必要とする可能性があります。HDマップが利用できない環境では、別の走行可能領域の評価方法が必要になります。
*   **実験環境の限定:** Navsim環境でのみ検証されているため、現実世界の複雑な交通状況や多様な環境への汎用性が不明です。
*   **ゴール地点のサンプリング:** ゴール地点の候補数を増やすと計算コストが増大するため、効率的なサンプリング方法が求められます。
*   **モデルの複雑さ:** GoalFlowは複数のモジュールで構成されており、全体として複雑なモデルです。各モジュールの相互作用を理解し、調整するには高度な専門知識が必要です。

## 5. 技術的な詳細について

GoalFlowは、以下の3つの主要モジュールで構成されています。

1.  **Perception Module:** 画像とLiDARデータを融合し、BEV（Bird's-Eye View）特徴量を作成します。 Transformerベースのアーキテクチャを使用し、異なるセンサーからの情報を効率的に統合します。HDマップとバウンディングボックスからの補助的な損失を用いて、BEV特徴量の精度を向上させます。
2.  **Goal Point Construction Module:** 離散化された軌道の終点空間から、候補となるゴール地点の集合を生成します。各ゴール地点に対し、距離スコア（Ground Truthとの近さ）と走行可能領域コンプライアンススコア（走行可能領域内にあるか）を計算します。Transformerベースのスコアラーデコーダを用いて、各スコアを予測します。最終的なスコアに基づいて最適なゴール地点を選択します。
3.  **Trajectory Planning Module:** Rectified Flow（Flow Matchingの一種）をベースとした生成モデルを用いて、多峰性軌道を生成します。BEV特徴量と選択されたゴール地点を条件として、ノイズ分布から目標軌道分布への変換を学習します。生成された軌道に対し、ゴール地点への距離と進行度合いを考慮したスコアリングを行い、最適な軌道を選択します。ゴール地点のエラーを緩和するために、シャドウ軌道を用いた検証を行います。

Python風疑似コード:

```python
# Perception Module
def create_bev_feature(image, lidar, hd_map, bounding_boxes):
  bev_feature = fuse_image_lidar(image, lidar)
  bev_feature = apply_transformer(bev_feature)
  bev_feature = apply_auxiliary_losses(bev_feature, hd_map, bounding_boxes)
  return bev_feature

# Goal Point Construction Module
def construct_goal_point(bev_feature, trajectory_endpoints):
  goal_point_vocabulary = discretize_trajectory_endpoints(trajectory_endpoints)
  distance_scores = calculate_distance_scores(goal_point_vocabulary, ground_truth_endpoint)
  drivable_area_compliance_scores = calculate_drivable_area_compliance_scores(goal_point_vocabulary, drivable_area)
  final_scores = combine_scores(distance_scores, drivable_area_compliance_scores)
  optimal_goal_point = select_optimal_goal_point(goal_point_vocabulary, final_scores)
  return optimal_goal_point

# Trajectory Planning Module
def generate_trajectory(bev_feature, optimal_goal_point, noise_distribution):
  trajectory = rectified_flow(noise_distribution, bev_feature, optimal_goal_point)
  shadow_trajectory = rectified_flow(noise_distribution, bev_feature, mask_goal_point(optimal_goal_point))
  trajectory_score = score_trajectory(trajectory, optimal_goal_point)
  shadow_trajectory_score = score_trajectory(shadow_trajectory, optimal_goal_point)
  if is_goal_point_unreliable(trajectory, shadow_trajectory):
    return shadow_trajectory
  else:
    return trajectory
```

## 6. コストや物理的な詳細について

論文中には、以下のコストに関する情報が記載されています。

*   **データセット:** Opensceneデータセットを使用。120時間の自動運転データが含まれています。Navsim環境は、trainvalに1192シナリオ、テストに136シナリオを使用しています。
*   **学習環境:** 4ノード、各ノードに8つのRTX 4090またはRTX 3090 GPUを搭載した環境で学習。
*   **生成軌道数:** 128/256個の軌道を生成し、軌道スコアラーが最適な軌道を選択。

モデルサイズ、学習時間、消費電力などの詳細な情報については論文中に記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

*   **Navsim:** Data-driven non-reactive autonomous vehicle simulation and benchmarking. (Dauner et al.) - GoalFlowの性能評価に使用されたシミュレーション環境の詳細が記載されています。
*   **Flow straight and fast: Learning to generate and transfer data with rectified flow.** (Liu et al.) - GoalFlowの軌道生成に用いられているRectified Flowの基礎となる論文です。
*   **Vadv2: End-to-end vectorized autonomous driving via probabilistic planning.** (Chen et al.) - ゴール地点の離散化のアイデアの元になった論文です。

## 8. この論文を140字以内のツイートで要約すると？

GoalFlow: ゴール地点を導入したFlow Matchingで自動運転の多峰性軌道生成をSOTAに！シーン情報から最適なゴールを選択、走行可能領域も考慮。拡散モデルより効率的でロバスト！ #自動運転 #軌道生成 #FlowMatching


---


# FlowTok: Flowing Seamlessly Across Text and Image Tokens

[View Paper](http://arxiv.org/abs/2503.10772v1)

## 1. 既存研究では何ができなかったのか

既存のクロスモーダル生成モデル、特にテキストから画像生成モデルは、いくつかの課題を抱えていました。

*   **複雑なアーキテクチャ**: 従来のテキストから画像生成は、テキストを条件信号として扱い、ノイズからターゲット画像への拡散プロセスを誘導していました。これは、複雑な条件付けメカニズムやノイズスケジューリングを必要とし、モデル全体の複雑さを増していました。
*   **計算コスト**: 拡散モデルは反復的なノイズ除去プロセスを伴うため、トレーニングと推論に大量の計算リソースを必要としました。特に大規模モデルでは、GPUリソースとトレーニング時間が大きな課題でした。また、効率的なサンプリング手法の開発も重要な課題でした。
*   **異なるモーダリティの表現**: テキストと画像は本質的に異なる表現を持っています。テキストは意味的で1Dトークンとしてエンコードされる一方、画像は空間的に冗長で2D潜在埋め込みとして表現されます。これらの異なる表現を共通の潜在空間に投影することは、困難な課題でした。CrossFlowなどの一部の手法は、テキストを2D潜在空間にマッピングすることでこの問題に対処しましたが、追加の計算コストが発生し、効率性の向上という当初の目標に反していました。
*   **メモリ効率**: 既存の手法は、特に高解像度の画像生成において、大きな潜在空間サイズを必要とし、メモリ効率に課題がありました。
*   **統一的なフレームワークの欠如**: 既存の研究では、マルチモーダル理解と生成を統一的なフレームワークで扱うことができていませんでした。多くのモデルは、理解（異なるモーダリティを共通の潜在空間に投影）と生成（テキストを条件として画像を生成）を異なるパラダイムとして扱っていました。

## 2. どのようなアプローチでそれを解決しようとしたか

FlowTokは、上記の課題を解決するために、以下のアプローチを採用しました。

*   **フローマッチングによる直接的なモーダリティ変換**: 拡散モデルの代わりに、テキストと画像を直接、フローマッチングによって変換するパラダイムを導入しました。これにより、ノイズ除去プロセスや複雑な条件付けメカニズムが不要になりました。
*   **コンパクトな1Dトークン表現**: 画像をコンパクトな1Dトークン表現にエンコードすることで、テキストと画像を共通の潜在空間に投影することを可能にしました。具体的には、TA-TiTokという既存の画像トークナイザーを改良し、位置情報の処理と再構成品質を向上させました。
*   **軽量なテキストプロジェクター**: テキストエンコーダーからの出力を、画像潜在空間の次元に合わせるための軽量なテキストプロジェクターを導入しました。これにより、テキストの1D構造を維持しながら、次元を削減することができました。
*   **自己注意ベースの生成モデル**: FlowTokは、完全な自己注意ベースの生成モデルであり、複雑な条件付けメカニズムを必要としません。これにより、異なるモーダリティ間の直接的なフローが可能になりました。
*   **統一的な潜在空間**: テキストと画像を同じ1D低次元空間に表現することで、マルチモーダル理解と生成を統一しました。
*   **効率的なトレーニング**: より少ない計算リソースでトレーニングできるように設計されました。例えば、FlowTok-H（1.1Bパラメータ）は、8 A100 GPUで8Kのバッチサイズでトレーニングできます。

## 3. 結果、何が達成できたのか

FlowTokによって、以下の成果が達成されました。

*   **メモリ効率の向上**: 画像をコンパクトな1Dトークン表現にエンコードすることで、潜在空間サイズを3.3倍削減しました（256解像度の画像の場合）。
*   **計算コストの削減**: 複雑な条件付けメカニズムやノイズスケジューリングが不要になり、トレーニングと推論に必要な計算リソースを大幅に削減しました。
*   **高速なサンプリング**: 10倍以上高速なサンプリング速度を実現しました。
*   **同等の性能**: 最新のモデルと同等の性能を達成しました。
*   **イメージからテキストへの拡張**: 同じフレームワークでイメージからテキストへの生成をサポートしています。
*   **少ないリソースでの学習**: 一般公開されているデータセットのみを使用して学習しています。
*   **トレーニング時間の短縮**: SD 2.1と比較してトレーニング時間が大幅に短縮されました。

## 4. Limitationや問題点は何か

FlowTokの制限事項と問題点は次のとおりです。

*   **テキストから画像生成時の情報損失**: 画像の潜在空間の次元に合わせるために、CLIPテキスト埋め込みを低次元の潜在空間に投影する必要があるため、テキストの意味情報がいくらか失われる可能性があります。結果として、テキストと生成された画像の間のアラインメントが、クロスアテンションメカニズムを使用する最新のモデルと比較して弱くなる可能性があります。より強力なアラインメント損失を導入するか、画像トークナイザのトレーニング中にビジョン基盤モデルとアラインメントすることにより、画像潜在空間のチャネル次元を増やすことが、この問題の解決策となる可能性があります。
*   **フローマッチング技術の活用**: FlowTokは、フレームワークの有効性を検証するために、バニラフローマッチング手法のみを利用します。最先端のフローマッチング技術(logit-normal samplingなど)を組み込むことで、収束を加速し、パフォーマンスを向上させることができます。
*   **多様なモーダリティへの拡張**: FlowTokは、テキストと画像のモーダリティ間の効率的な直接進化を調査するための出発点として機能します。より一般的なフレームワークに拡張して、より広範囲のモーダリティに対応し、同じ統合された定式化の下で追加のタスクをサポートすることができます。
*   **データセットのバイアス**: FlowTokの性能は、トレーニングに使用するデータセットの品質に依存します。公開されているデータセットのみを使用しているため、高品質のプロプライエタリデータを使用するモデルと比較して、性能に差が出る可能性があります。
*   **一般的な評価指標の限界**: FIDやBLEUなどの一般的な評価指標は、生成された画像の品質やキャプションの適切性を完全に反映しているとは限りません。より高度な評価指標や人間による評価が必要となる場合があります。
*   **アーキテクチャの最適化**: FlowTokのアーキテクチャはまだ最適化の余地があります。例えば、Transformerブロックの構成やテキストプロジェクターの設計などを改善することで、性能や効率をさらに向上させることができます。
*   **倫理的な懸念**: テキストから画像を生成するモデルは、悪意のあるコンテンツや誤情報の拡散に利用される可能性があります。FlowTokの開発と利用には、倫理的な配慮が必要です。

## 5. 技術的な詳細について

FlowTokは、テキストと画像を1Dトークンに変換し、フローマッチングによって直接変換するフレームワークです。主要な技術要素は以下のとおりです。

1.  **画像トークナイザー**:
    *   TA-TiTokをベースに、RoPE（Rotary Positional Embedding）を導入し、位置情報の処理を改善。
    *   Vision Transformer (ViT) の MLPブロックを改良し、より効果的な潜在空間を学習。
    *   CLIPテキストエンコーダーの出力シーケンス長に合わせて、潜在トークン数を設定。
    *   これにより、画像はコンパクトな1Dトークンシーケンスにエンコードされます。

    ```python
    def image_tokenize(image):
      patches = extract_patches(image) # 画像からパッチを抽出
      latent_tokens = create_latent_tokens() # 潜在トークンを生成
      sequence = concatenate(patches, latent_tokens) # パッチと潜在トークンを連結
      embeddings = vision_transformer(sequence) # ViTに通す
      image_tokens = extract_latent_token_embeddings(embeddings) # 潜在トークンの埋め込みを抽出
      return image_tokens
    ```

2.  **テキストプロジェクター**:
    *   CLIPテキストエンコーダーからの出力を、画像潜在空間の次元に合わせるための軽量なTransformerブロックを使用。
    *   テキスト埋め込みのチャネル数を調整しながら、1D形状を保持。
    *   KLダイバージェンス正則化を適用し、テキスト潜在空間をガウス分布としてモデル化。
    *   テキストアラインメント損失を導入し、意味情報の損失を抑制。

    ```python
    def project_text(text_embedding):
      # Transformerブロックに通す
      projected_embedding = transformer_blocks(text_embedding)
      # KLダイバージェンス正則化
      kl_loss = calculate_kl_divergence(projected_embedding)
      # テキストアラインメント損失
      alignment_loss = calculate_alignment_loss(projected_embedding, clip_text_embedding)
      return projected_embedding, kl_loss, alignment_loss
    ```

3.  **フローマッチング**:
    *   画像とテキストを同じ潜在空間にマッピングした後、バニラフローマッチングを適用。
    *   テキストをフローマッチングのソース分布として直接扱い、DiTブロック内の連結やクロスアテンションを不要にする。
    *   これにより、テキストから画像への直接的な変換が可能になる。

    ```python
    def flow_matching(source, target, t):
      # 中間表現を計算
      Xt = (1 - t) * source + t * target
      # 速度場を推定
      Vt = flow_matching_model(Xt, t)
      return Vt
    ```

4.  **損失関数**:
    *   フローマッチング損失
    *   KLダイバージェンス損失（テキストから画像生成時）
    *   テキストアラインメント損失（テキストから画像生成時）

5.  **アーキテクチャ**:
    *   DiT（Diffusion Transformer）ブロックを基本単位として使用。
    *   FlowTok-B、FlowTok-XL、FlowTok-Hの3つの構成を用意。

## 6. コストや物理的な詳細について

*   **モデルサイズ**:
    *   FlowTok-H: 1.1Bパラメータ
*   **GPU**:
    *   トレーニングにはA100 GPUを使用。
    *   FlowTok-Hは、8 A100 GPUで8Kのバッチサイズでトレーニング可能。
*   **トレーニング時間**:
    *   FlowTok-XL: 20.4 8-A100 days
    *   FlowTok-H: 26.1 8-A100 days
    *   SD 2.1: 1041.6 8-A100 GPU days
*   **データセット**:
    *   画像トークナイザー: DataComp-1B
    *   テキストトークナイザー: COCO
    *   テキストから画像生成: DataComp-1B、LAION-artなど
    *   イメージからテキスト生成: COCO Karpathy split
*   **推論速度**:
    *   FlowTok-XL: 22.7 images per second (256px resolution, single A100, batch size 64)
    *   FlowTok-H: 18.2 images per second (256px resolution, single A100, batch size 64)

## 7. 参考文献のうち、特に参照すべきもの

*   **Flow Matching**: Lipman et al., 2022; Albergo & Vanden-Eijnden, 2022; Tong et al., 2023a, 2023b
    *   FlowTokの基盤となるフローマッチングの理論と実装について理解を深めるために重要です。
*   **TA-TiTok**: Kim et al., 2024
    *   FlowTokが画像トークナイザーとして使用しているTA-TiTokのアーキテクチャと性能について理解するために重要です。
*   **DiT**: Peebles & Sutskever, 2023
    *   FlowTokのアーキテクチャの基本単位であるDiTブロックについて理解するために重要です。

## 8. この論文を140字以内のツイートで要約すると？

FlowTokは、テキストと画像を1Dトークンに変換し、フローマッチングで直接生成する新手法。高速な学習と推論、高画質を実現！ #AI #画像生成 #FlowMatching


---


# Group-robust Machine Unlearning

[View Paper](http://arxiv.org/abs/2503.09330v1)

## 1. 既存研究では何ができなかったのか

従来の機械学習における忘却（Machine Unlearning）の研究では、忘却対象のデータ（forget set）が、訓練データ全体から一様に分布していることを前提としていました。しかし、実際の忘却要求は、特定のグループに偏っていることが多く、例えば、「特定の年齢層の男性」など、あるグループのデータが忘却セットに集中していると、そのグループに対するモデルの性能が低下し、公平性の問題が生じるという課題がありました。既存研究では、この非一様な分布に対する頑健性が考慮されていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、この「グループ頑健な機械学習における忘却（group-robust machine unlearning）」という課題に対して、以下の2つのアプローチで解決を試みました。

1.  **サンプル分布の再重み付け (Sample Distribution Reweighting)**: 忘却セットの分布の偏りを補正するために、リトレイン時にサンプル分布を再重み付けする戦略を提案しました。具体的には、忘却によって情報が失われたグループのサンプリング確率を上げることで、訓練データセットのグループ統計に合わせるようにバランスを取ります。

2.  **相互情報量に着目した忘却 (Mutual Information-aware Machine Unlearning, MIU)**:  近似的な忘却手法として、モデルの特徴量とグループ情報との間の相互情報量を最小化するMIUを提案しました。これにより、忘却セットの優勢グループにおける性能劣化を抑制します。さらに、サンプル分布の再重み付けと、オリジナルモデルとの相互情報量のキャリブレーションを組み合わせることで、グループの頑健性を維持します。

## 3. 結果、何が達成できたのか

本研究により、以下の点が達成されました。

*   **グループ頑健性の問題提起**: 近似的な忘却において、既存の忘却アルゴリズムがモデルのグループ頑健性を低下させることを明らかにしました。
*   **単純かつ効果的な再重み付け戦略**: 正確な忘却において、グループの精度低下を緩和するための、サンプル分布の再重み付け戦略を提案し、その有効性を示しました。
*   **MIU**: グループ頑健性を考慮した初の近似忘却手法であるMIUを提案しました。MIUは、忘却セットの相互情報量を最小化し、オリジナルモデルのグループ頑健性に合わせるように調整することで、既存手法よりも高い忘却性能とグループ頑健性を両立しました。
*   **実験による有効性の確認**: 3つのデータセット（CelebA, Waterbirds, FairFace）を用いた実験により、MIUが既存手法を上回り、モデルの頑健性を損なうことなく忘却を達成できることを示しました。

## 4. Limitationや問題点は何か

### 論文で言及されているもの

*   **グループ情報の既知性**: 本研究では、データポイントのグループ情報を既知として扱っています。しかし、現実にはグループ情報を取得することが難しい場合があります。
*   **画像分類タスクへの限定**: 評価は画像分類タスクに限定されています。他のタスクへの適用可能性は不明です。
*   **Waterbirdsデータセットでの過学習**: Waterbirdsデータセットにおいて、忘却セットのサンプル数が少ない場合、再重み付けによって過学習が生じ、頑健性の維持が制限される可能性があります。

### その他考えられるもの

*   **計算コスト**: MIUは相互情報量の推定にMLPを使用しており、計算コストが増加する可能性があります。特に大規模なモデルやデータセットでは、その影響が大きくなる可能性があります。
*   **ハイパーパラメータ調整**: MIUには、相互情報量の最小化とキャリブレーションのバランスを調整するためのハイパーパラメータλが含まれています。このパラメータの調整が、性能に大きく影響する可能性があります。
*   **忘却の定義**: 忘却の度合いを測る指標として、UA（Forget Accuracy）を使用していますが、UAが向上する場合（忘却セットの精度が上がる場合）に、本当に忘却が達成できているのか評価が難しい場合があります。

## 5. 技術的な詳細について

本研究におけるMIU (Mutual Information-aware Machine Unlearning) の技術的な詳細について説明します。

1.  **定式化**:
    *   訓練データセット:  `D_tr = {(x_i, y_i, a_i)}`。ここで、`x_i` は入力データ、`y_i` はラベル、`a_i` はグループ属性を表します。
    *   忘却データセット: `D_f ⊂ D_tr`
    *   保持データセット: `D_r = D_tr \ D_f`
    *   特徴抽出器: `f_θ(x): X -> Z`。パラメータ`θ`を持つ非線形特徴抽出器です。
    *   分類器: `h_φ(z): Z -> Y`。パラメータ`φ`を持つ分類器です。

2.  **相互情報量の最小化**:
    *   モデル特徴とグループ属性の間の相互情報量 `I(Z; G)`を最小化します。ここで、`Z` は特徴量空間、`G` はグループ属性を表します。
    *   相互情報量の推定には、MLP `T_ψ`を使用し、次の損失関数を最小化します。

    ```python
    def mutual_information_loss(D, psi, theta):
        """
        D: データセット
        psi: MLP T_psi のパラメータ
        theta: 特徴抽出器 f_theta のパラメータ
        """
        z = f_theta(x) # 特徴抽出
        term1 = E[(x, g) ~ D][T_psi(z, g)]  # Joint distribution
        term2 = log(E[(x, g_bar) ~ D][exp(T_psi(z, g_bar))]) # Product of marginals

        loss = term1 - term2
        return loss
    ```
    ここで、`(x, g_bar)` は、周辺分布の積からのサンプリングを表します。

3.  **相互情報量のキャリブレーション**:
    *   忘却後のモデルが他のグループに影響を与えないように、元のモデルの相互情報量に一致するように、以下の損失関数を最小化します。
    ```python
    def calibration_loss(omega_Dr, psi, theta, theta_o):
        """
        omega_Dr:  重み付けされた保持データセット
        psi: MLP T_psi のパラメータ
        theta:  忘却後の特徴抽出器 f_theta のパラメータ
        theta_o: オリジナルモデルの特徴抽出器 f_theta_o のパラメータ
        """
        M_unlearn = mutual_information_loss(omega_Dr, psi, theta)
        M_original = mutual_information_loss(omega_Dr, psi, theta_o)
        loss = ||M_unlearn - M_original||^2  # L2 loss
        return loss
    ```

4.  **リトレイン損失**:
    *   忘却後もモデルの識別能力を維持するため、再重み付けされた保持データセット上でクロスエントロピー損失を最小化します。
    ```python
    def retain_loss(omega_Dr, phi, theta):
        """
        omega_Dr: 重み付けされた保持データセット
        phi: 分類器 h_phi のパラメータ
        theta:  特徴抽出器 f_theta のパラメータ
        """
        loss_ce = cross_entropy_loss(h_phi(f_theta(x)), y) #クロスエントロピー誤差
        return loss_ce
    ```

5.  **全体の損失関数**:
    *   上記の損失関数を組み合わせ、以下の全体の損失関数を最小化します。
    ```python
    def total_loss(omega_Dr, Df, psi, theta, theta_o, phi, lmd):
        """
        omega_Dr: 重み付けされた保持データセット
        Df: 忘却データセット
        psi: MLP T_psi のパラメータ
        theta: 忘却後の特徴抽出器 f_theta のパラメータ
        theta_o: オリジナルモデルの特徴抽出器 f_theta_o のパラメータ
        phi: 分類器 h_phi のパラメータ
        lmd: キャリブレーション項の重み
        """
        loss_retain = retain_loss(omega_Dr, phi, theta)
        loss_unlearn = mutual_information_loss(Df, psi, theta)
        loss_calibration = calibration_loss(omega_Dr, psi, theta, theta_o)

        total_loss = loss_retain + loss_unlearn + lmd * loss_calibration
        return total_loss
    ```

## 6. コストや物理的な詳細について

実験は単一の A100 Nvidia GPU 上で PyTorch を使用して実行されました。

*   **データセット**: CelebA, Waterbirds, FairFace
*   **モデル**: 論文内には具体的なモデルアーキテクチャの記述はありません。
*   **学習**: SGD optimizer (momentum 0.9, weight decay) を使用し、学習率は cosine annealing scheduler で調整しました。
*   **バッチサイズ**: 論文内には具体的な記述はありません。
*   **MIU のオーバーヘッド**: MIU は、相互情報量の推定に MLP を使用するため、計算コストが増加します。実験では、アンラーニング時間がわずかに増加しました (約 4%)。
*   **微調整**: その他のハイパーパラメータの最適な設定を探索しながら、アンラーニング手法のために微調整を 10 エポックに制限しました。

## 7. 参考文献のうち、特に参照すべきもの

*   **Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation.**：相互情報量の推定方法に関する基礎的な研究です。
*   **Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information.**：グループ情報なしにグループの頑健性を改善する手法に関する研究です。

## 8. この論文を140字以内のツイートで要約すると？

機械学習の忘却で特定のグループの性能が落ちる問題に、サンプル再重み付けと相互情報量最小化で対処！ グループ頑健性を保ちつつ忘却を実現するMIUを提案。公平な忘却に貢献 #機械学習 #忘却 #公平性


---


# ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness

[View Paper](http://arxiv.org/abs/2503.10624v1)

## 1. 既存研究では何ができなかったのか

既存研究は、3Dの衣服を着た人物の点群から人体をフィッティングする際に、以下の点で課題がありました。

*   **初期姿勢への依存:** 従来の最適化ベースの手法は、マルチステージのパイプラインを使用しており、姿勢の初期化に大きく依存していました。
*   **汎化性能の限界:** 近年の学習ベースの手法は、多様な姿勢や衣服の種類に対する汎化性能に苦労していました。特に、ゆったりとした衣服や、動的な動きによる大きな変形に対してロバスト性に欠けていました。
*   **体型と衣服の分離:** 既存手法では、体型と衣服の変形をうまく分離できず、特に衣服が体から大きく離れている場合に、フィッティング精度が低下していました。Scalar tightnessを用いた手法も提案されているが、out-of-distributionの姿勢や体型に対しては汎化性能が低い。
*   **EquivarianceとTightnessの両立:** Articulated SE(3) Equivarianceを用いた手法は、関節の動きには対応できるが、衣服の大きな変形には対応できない。また、Scalar tightnessを用いた手法は、衣服の変形には対応できるが、out-of-distributionの姿勢や体型に対しては汎化性能が低い。
*   **疎なマーカーと密な対応点の活用:** 密な対応点を用いた手法は、外れ値に弱く、局所解に陥りやすい。一方、疎なマーカーを用いた手法は、外れ値に強く、大域的な構造を捉えやすいが、詳細な形状を捉えることが難しい。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、これらの課題を解決するために、以下の新しいアプローチを提案しました。

*   **Equivariant Tightness Fitting (ETCH):** 衣服表面から人体表面へのマッピングを、局所的に近似的なSE(3) equivarianceを通じて推定する新しいパイプラインを提案します。
*   **Tightnessのエンコード:** cloth表面からbody表面への変位ベクトルとしてtightnessをエンコードします。このエンコードは、衣服の変形に対してローカルにSE(3) equivariantであり、衣服の種類や姿勢に関わらず、内側のbodyを指し示すことを保証します。
*   **ポーズ不変な特徴量:** ポーズ不変なbody特徴量を用いて、疎なbodyマーカーを回帰します。これにより、衣服を着た人物のフィッティングを、inner-bodyマーカーのフィッティングタスクに簡略化します。
*   **疎なマーカー回帰:** 密な対応点を直接回帰するのではなく、疎なマーカーを回帰することで、外れ値に対するロバスト性を向上させます。
*   **EquivarianceとTightnessの統合:** EquivarianceとTightnessの両方の利点を活用し、それぞれの欠点を補完することで、より汎用的なフィッティングを実現します。具体的には、tightness vectorの方向をequivariance特徴から学習し、大きさをinvariance特徴から学習することで、両方の利点を統合します。

## 3. 結果、何が達成できたのか

提案手法ETCHは、以下の点で既存手法を大幅に上回る成果を達成しました。

*   **フィッティング精度の向上:** CAPEおよび4D-Dressデータセットにおいて、既存の最先端手法（tightnessを考慮しないものと考慮するもの両方）と比較して、ゆったりとした衣服に対するbodyフィッティング精度（16.7% ~ 69.5%）および形状精度（平均49.9%）を大幅に向上させました。
*   **方向誤差の削減:** equivariant tightnessのデザインにより、one-shot設定（またはout-of-distribution設定）において、方向誤差を(67.2% ~ 89.8%)削減しました。
*   **汎化性能の向上:** 挑戦的なポーズ、未知の体型、ゆったりとした衣服、および非剛体ダイナミクスに関わらず、ETCHの強力な汎化性能を定性的に示しました。
*   **疎なマーカーの有効性:** 密な対応点を用いる手法と比較して、疎なマーカーを用いる手法の方が、よりロバストで正確なフィッティングを実現できることを示しました。
*   **Tightness Vectorの有効性:** tightness scalarのみを用いる手法と比較して、tightness vectorを用いる手法の方が、より汎用的なフィッティングを実現できることを示しました。

## 4. Limitationや問題点は何か

本研究のLimitationや問題点は以下の通りです。

*   **点群の欠損:** 疎なマーカーに依存しているため、点群の欠損によりマーカーを正確に捉えられない場合、フィッティングが失敗する可能性があります。
*   **顔や指のマーカーの欠如:** 現在のマーカー設定では、顔のランドマークや指をカバーしていません。これらを拡張するには、より複雑なモデルが必要になる可能性があります。特に顔や手の領域では、皮膚が露出している場合があるため、tightnessのモデリングを緩和する必要があるかもしれません。
*   **データ量の依存性:** one-shot設定では優れた性能を発揮しますが、データ量が増加すると、既存手法との性能差が縮まる可能性があります。大規模なデータセットでの性能向上がどこまで見込めるかは不明です。
*   **データセットの課題:** 大量のscan-bodyペアを持つ高忠実度の衣服データセットは不足しており、合成データセットの利用も考えられますが、ドメインギャップの問題が残ります。
*   **倫理的な懸念:** ポルノ業界での悪用やプライバシー侵害の可能性があり、コードの公開は非営利の研究目的に限定されます。

### (個人的に考える問題点)

*   **計算コスト:** Equivariantな特徴抽出には、通常の点群処理よりも高い計算コストがかかる可能性があります。
*   **パラメータ調整:** 損失関数の重みや、その他のハイパーパラメータの調整に手間がかかる可能性があります。
*   **データセットへの偏り:** CAPEや4D-Dressデータセットに特化した最適化が行われている可能性があり、他のデータセットへの汎化性能は不明です。
*   **衣服のトポロジー:** 衣服のトポロジーが大きく異なる場合(例: ドレスとコート)への対応が難しい可能性があります。
*   **自己交差:** 衣服の自己交差が発生した場合、フィッティング精度が低下する可能性があります。

## 5. 技術的な詳細について

ETCHは、以下の主要な技術要素で構成されています。

1.  **Equivariant特徴抽出:**
    *   入力: 3D点群 `X = {x_i ∈ R^3}_N`
    *   処理: EPN (Equivariant Point Network) を用いて、各点に対してSE(3)-equivariantな特徴 `f_i^(equiv)`を抽出する。
    *   詳細:
        *   EPNは、点群を回転させる操作に対して特徴量も同様に回転するという性質を持つ。
        *   EPNの出力`f_i^(equiv)`は、回転グループの次元を持つ`R^(N x O x C)`のテンソルである。
2.  **Invariant特徴抽出:**
    *   処理: EPNの出力`f_i^(equiv)`を平均プーリングすることで、回転不変な特徴`f_i^(inv)`を抽出する。さらに、Point Transformer (PT-1)を用いて、`f_i^(inv)`にコンテキスト情報を付加する。
3.  **Tightness Vector予測:**
    *   処理: equivariantな特徴 `f_i^(equiv)` と invariant な特徴 `f_i^(inv)` を用いて、tightness vector `v_i`を予測する。
    *   詳細:
        *   `v_i`は、方向 `d_i` と大きさ `b_i` で構成される。
        *   `d_i` は、`f_i^(equiv)` を用いて学習された回転行列 `R_i` と基準ベクトル `v_s = [0, 0, 1]` の積として計算される。
            ```python
            # 回転行列の計算
            def compute_rotation_matrix(f_equiv, g):
                # f_equiv: equivariant特徴量 (R^(O x C))
                # g: 回転グループ (SO(3)の離散化)
                
                # 回転グループの各要素に対してMLPを適用し、重みを計算
                w = mlp(self_attention(f_equiv, g))  # self_attention: rotation group dimensionに対する処理
                
                # 重み付き平均回転行列を計算 (SVD分解を利用)
                U, D, V = svd(sum([w[j] * R_j for j in range(len(g))])) # R_jは回転グループgの各要素に対応する回転行列
                R_hat = U @ [[1, 0, 0], [0, 1, 0], [0, 0, det(U @ V.T)]] @ V.T
                return R_hat
            
            R_i = compute_rotation_matrix(f_equiv_i, g) # 回転行列を計算
            d_i = R_i @ [0, 0, 1]   # 回転行列をz軸ベクトルに適用 (基準ベクトルv_sはz軸方向)
            ```
        *   `b_i` は、`f_i^(inv)` を用いて学習される。
4.  **マーカーアグリゲーション:**
    *   処理: 各点に対して、どのbodyマーカーに対応するかを表すラベル `l_i` と、その信頼度 `c_i` を予測する。
    *   詳細:
        *   ラベル `l_i` は、`f_i^(inv)` を用いて学習されたラベル予測器から計算される。
        *   信頼度 `c_i` は、`f_i^(inv)` を用いて学習された信頼度予測器から計算される。
        *   各マーカーに対応する点の集合に対して、tightness vectorを用いて得られた点 `y_i = x_i + v_i` を信頼度で重み付け平均することで、最終的なマーカー位置 `m_k` を推定する。
            ```python
            # マーカーアグリゲーション
            def aggregate_markers(points, labels, confidences, alpha):
                # points: inner points (y_i)
                # labels: 各点のマーカーラベル (l_i)
                # confidences: 各点の信頼度 (c_i)
                # alpha: 信頼度の影響を調整するパラメータ

                # 各マーカーラベルごとに、重み付き平均位置を計算
                m_k = {}
                for k in unique(labels):  #unique(labels)でlabelsに含まれるユニークな値を抽出
                    # ラベルがkである点の集合を抽出
                    points_k = points[labels == k]
                    confidences_k = confidences[labels == k]

                    # 重み付き平均位置を計算
                    m_k[k] = sum(points_k * (confidences_k ** alpha)) / sum(confidences_k ** alpha)
                return m_k
            ```
5.  **SMPL最適化:**
    *   処理: 推定されたマーカー位置を用いて、SMPLモデルのパラメータ（形状 `β`、ポーズ `θ`、位置 `t`）を最適化する。
    *   詳細:
        *   マーカー位置とSMPLモデルの対応する関節位置との距離を最小化するように、パラメータを最適化する。
        *   Levenberg-Marquardtアルゴリズムに基づくdamped Gauss-Newton optimizerを使用する。
6.  **損失関数:**
    *   方向誤差、大きさ誤差、ラベル誤差、信頼度誤差の重み付き和を最小化する。
    ```python
    # 損失関数の定義
    def compute_loss(d_hat, d, b_hat, b, P, l, c_hat, c, w_d, w_b, w_l, w_c):
        # d_hat: 推定されたtightness vectorの方向
        # d: ground truthのtightness vectorの方向
        # b_hat: 推定されたtightness vectorの大きさ
        # b: ground truthのtightness vectorの大きさ
        # P: マーカーラベルの確率分布
        # l: ground truthのマーカーラベル
        # c_hat: 推定された信頼度
        # c: ground truthの信頼度
        # w_d, w_b, w_l, w_c: 各損失の重み

        # 方向誤差
        L_d = sum(d_hat * d) / (norm(d_hat) * norm(d))

        # 大きさ誤差
        L_b = mean((b_hat - b)**2)

        # ラベル誤差
        L_l = -mean(log(P[range(len(l)), l]))  # range(len(l))で0からlen(l)-1までの整数列を生成

        # 信頼度誤差
        L_c = mean((c_hat - c)**2)

        # 全体誤差
        L = w_d * L_d + w_b * L_b + w_l * L_l + w_c * L_c

        return L
    ```

## 6. コストや物理的な詳細について

*   **データセット:**
    *   CAPE: 15 subjects with different body shapes. Training: 26,004 frames, Validation: 1,021 frames (subsampled by factors of 5 and 20).
    *   4D-Dress: 32 subjects with 64 outfits across over 520 motion sequences. Training: 59,395 frames, Validation: 1,943 frames (subsampled by factors of 1 and 10).
*   **学習:**
    *   Optimizer: Adam (learning rate = 1e-4, batch size = 2)
    *   Epochs: CAPE (21 epochs), 4D-Dress (39 epochs)
    *   GPU: NVIDIA GeForce RTX 4090 (single GPU)
    *   Training time: Approximately 4 days
*   **点群:** サンプリングされた点の数: 5000
*   **マーカーフィッティング:** 1サブジェクトあたり約5秒 (80ステップ)

## 7. 参考文献のうち、特に参照すべきもの

*   **Loper et al., SMPL: A Skinned Multi-Person Linear Model:** SMPLモデルの詳細について理解するために必須。
*   **Marin et al., NICP: Neural ICP for 3D Human Registration at Scale:** 最も近いベースラインであり、性能比較の基準となる手法。
*   **Feng et al., Generalizing neural human fitting to unseen poses with articulated SE(3) equivariance:** Equivarianceの概念と、既存のEquivarianceを用いた人体フィッティング手法について理解するために参照。
*   **Qi et al., Pointnet: Deep learning on point sets for 3d classification and segmentation:** 点群処理の基礎となるPointNetアーキテクチャについて理解するために参照。
*   **Atzmon et al., Approximately Piecewise E (3) Equivariant Point Networks:** Equivariant Point Network (EPN) の詳細について理解するために参照。

## 8. この論文を140字以内のツイートで要約すると？

衣服を着た3D人体点群からの体型推定に、SE(3) EquivarianceなTightness Vectorを用いたETCHを提案！ 既存手法を大幅に凌駕する精度と汎化性能を実現。ゆったりした衣服や複雑なポーズにも対応。 #3D人体 #体型推定 #AI #CV
