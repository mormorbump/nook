
# 今話題のAIエージェントについてまとめてみた | ちゅらデータ株式会社のフィード

[View on Company Blog](https://zenn.dev/churadata/articles/5de07c1a6eefb4)

ちゅらデータのがく氏による、昨今話題のAIエージェントに関するまとめ。AIエージェントという言葉が、作業自動化を行うものと、開発・コーディングを支援するものの2つのグループを指しており、情報が錯綜している状況を整理するために資料が作成された。LayerXの名村氏によるイベントも背景として挙げられている。


---

# Solr って何者？⑤：解析内容を調整する② (Synonym) | SIOS Tech. Lab

[View on Company Blog](https://tech-lab.sios.jp/archives/46951)

この記事では、SolrのSynonym（同義語・類義語）フィルタについて、設定方法と注意点を解説しています。

**Synonymとは:**

Synonymは、検索時に同義語や類義語をまとめて検索できるようにする仕組みです。例えば、「青」で検索した際に「ブルー」を含むドキュメントもヒットするように設定できます。

**Synonymの設定方法:**

1.  **辞書の作成:**
    *   `synonyms.txt`ファイルに、同義語・類義語の対応を記述します。
    *   書式は2種類あります。
        *   `類義語A, 類義語B => 類義語C, 類義語D`: 類義語AまたはBを、類義語CおよびDに変換します。
        *   `類義語A, 類義語B, 類義語C`: 列挙された類義語のいずれかを、列挙された類義語全てに変換します。
2.  **スキーマファイルの設定:**
    *   `managed-schema.xml`ファイルに、Synonymフィルタの設定を追加します。
    *   `<analyzer>`要素内に、`<filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>`のような記述を追加します。
    *   `<filter>`要素の位置（フィルタの順序）は重要です。
3.  **設定の適用:**
    *   Solrコアをリロードして、設定を適用します。

**Synonym設定時の注意点:**

*   **フィルタの順序:** フィルタの処理順序は解析結果に影響を与えるため、Synonymフィルタを適用する時点での解析結果に合致するように記述する必要があります。
*   **再インデックス:** Synonymを追加した場合やフィルタを追加・変更した場合は、原則として登録済みのドキュメント・インデックスを削除して再登録（再インデックス）が必要です。

**高度な設定:**

*   インデックス登録時と検索時で解析方法を別々にする設定も可能です。
    *   `<analyzer type="index">`と`<analyzer type="query">`を使い分けます。
    *   検索時のみSynonym処理を行うことで、再インデックス処理を回避できます。
    *   ただし、解析方法を変えることで検索結果に悪影響が出る可能性もあるため、注意が必要です。

**まとめ:**

Synonymフィルタは、Solrの検索精度を向上させる強力な機能ですが、設定には注意が必要です。フィルタの順序や再インデックスの必要性などを考慮し、慎重に設定を行うようにしましょう。

---

# LangChainのChatPromptTemplateでSystemMessage・HumanMessageのプレースホルダーが置換されない理由と対応方法 #LangChain | Generative Agents Tech Blog

[View on Company Blog](https://blog.generative-agents.co.jp/entry/2025/03/19/213328)

LangChainのChatPromptTemplateでSystemMessageやHumanMessageを使用する際、プレースホルダーが置換されない理由とその対応方法について解説されています。

**背景:**

LangChainのChatPromptTemplateはプロンプトのテンプレートを扱うためのものですが、SystemMessageやHumanMessageのようなBaseMessageを継承したクラスはテンプレートではなくメッセージを表すため、そのままではプレースホルダーが置換されません。

**解決策:**

SystemMessagePromptTemplateやHumanMessagePromptTemplateといったSystemMessageやHumanMessageのテンプレート版クラスを使用することで、プレースホルダーが正常に置換されるようになります。
または、("system", "Speak in {lang}.")や("human", "{message}")のように記述する方法でも解決可能です。

**まとめ:**

LangChainのChatPromptTemplateでSystemMessageやHumanMessageのプレースホルダーが置換されない場合は、SystemMessagePromptTemplateやHumanMessagePromptTemplateを使用するか、("system", "Speak in {lang}.")のような形式で記述することで解決できます。


---

# 【AWS】ボリューム容量が足りず、クリーンアップ等しても容量に余裕がない場合の対処方法 | ヘッドウォータースのフィード

[View on Company Blog](https://zenn.dev/headwaters/articles/9be517bb2079f8)

AWSのEC2でサーバーを運用中にEBSボリュームの容量が100%になり、500エラーが発生した際の対処方法について。
ディスククリーンアップやEBSの領域拡張、ファイルシステムの拡張を試みたものの、容量不足は解消されなかった。
解決策として、以下の手順を実施。
1. 既存EBSのスナップショットを取得
2. スナップショットから拡張したEBSを新規作成
3. 新規EBSをデータディスクとしてEC2にマウント
4. 容量を圧迫しているデータを移行してサーバー起動領域を確保
詳細な手順は割愛されている。


---

# RAGのオフライン評価ファーストステップ | サーバーワークスエンジニアブログ

[View on Company Blog](https://blog.serverworks.co.jp/rag-offline-eval)

この記事では、RAG（Retrieval Augmented Generation）のオフライン評価について、AWSのRAG評価機能とRagasというツールを使った評価方法を解説しています。

まず、オフライン評価とは、ユーザーに実際に使ってもらうのではなく、評価データセットに対して評価スコアを算出する手法であり、オンライン評価に比べて工数を削減できるメリットがあります。

記事では、RAGがPoCレベルでも導入済みで、ユーザーの使用履歴が保存されている状況を想定しています。

**AWSのRAG評価機能を使う場合**

1.  **評価データセットの作成:** プロンプトに対して、検索されるべきコンテキスト（referenceContexts）または望ましい回答（referenceResponses）を準備し、Jsonl形式でAmazon S3に保存します。
2.  **RAG評価ジョブの作成:** 作成した評価データセットを使ってRAGを評価し、メトリクスを取得します。
3.  **精度が悪いケースの特定:** 取得したメトリクスから、回答精度が低いケースを特定し、原因を分析して改善策を検討します。
4.  **以前のバージョンとの比較:** 設定変更後に再度評価ジョブを作成し、以前の評価ジョブと比較して、メトリクスの変化を確認します。

**Ragasを使う場合**

1.  **Response Relevancyの算出:** ユーザーの入力とLLMからの出力があれば算出できる、回答の関連性を測るメトリクスです。ただし、回答の正確性は測れません。
2.  **Faithfulnessの算出:** LLMの出力がRAGで検索されたコンテキストにどれだけ一致しているかを測るメトリクスです。ハルシネーションの発見に役立ちます。算出にはコンテキストとLLMからの出力が必要です。
3.  **評価データセットを作成しAnswer Correctnessなどを算出:** ユーザーの入力に対する望ましい回答（ground truth）などを評価データセットとして用意し、回答の正確性など追加のメトリクスを取得します。

記事の結論として、手っ取り早く評価したい場合は、RagasのResponse Relevancyを使用し、プロンプトに対する参照回答を用意できる場合は、AWSのRAG評価機能を使用することを推奨しています。AWSの機能が要件に合わない場合は、Ragasなどのツールを活用します。

重要なのは、どのメトリクスを取得すればどのような情報が得られ、そのためにどのような情報が必要なのかを整理することであると述べています。


---

# 「AIエージェントキャッチアップ #25 - ControlFlow」を開催しました | Generative Agents Tech Blog

[View on Company Blog](https://blog.generative-agents.co.jp/entry/2025/03/19/190030)

ジェネレーティブエージェンツの大嶋氏が「AIエージェントキャッチアップ #25 - ControlFlow」という勉強会を開催。今回は、Agentic AI WorkflowのPythonフレームワーク「ControlFlow」について、公式ドキュメントを読み、実際に動かしてみた。

ControlFlowは、Pythonのワークフローオーケストレーションツール「Prefect」の開発元がOSSとして公開しているフレームワーク。処理は「タスク」「エージェント」「フロー」の3つで構成される。フローの実装例として、`cf.run`でタスクを実行する際に、`context`を通じて値を共有する仕組みが紹介された。`@cf.flow`をつけた関数では、`context`の値が自動でプロンプトに注入される。

ControlFlowのプロンプトは、ソースコード内の`src/controlflow/orchestration/prompt_templates`ディレクトリにあり、`task.jinja`などで確認できる。ControlFlowは、標準で提供されるプロンプト構成をベースとして使用することを想定しており、細かいカスタマイズは想定されていない。Prefectとの連携に強みがある可能性がある。

次回の「AIエージェントキャッチアップ #26」では、メモリ・知識・ツールを備えたマルチモーダルエージェントを構築するための軽量ライブラリ「Agno」をテーマにする予定。

---

# 【SASE簡単解説】SASEとは？ 初心者が1から解説 | TechHarmony

[View on Company Blog](https://blog.usize-tech.com/what-is-sase-for-beginners/)

この記事は、SASE（Secure Access Service Edge）について、初心者向けにわかりやすく解説しています。

まず、SASEを一言で「ネットワーク」と「ネットワークセキュリティ」をクラウドベースで統合したものと定義しています。ネットワークとは、SD-WAN、MPLS、VPNなどの通信の提供と制御の仕組みであり、ネットワークセキュリティとは、ファイアウォール、セキュアゲートウェイ(SGW)、CASB、DLPなどの脅威を防御する仕組みのことです。

次に、ゼロトラストの概念について説明しています。ゼロトラストとは、「すべて信用しない」という前提に立ち、適切な認証を受けたユーザーと端末だけが、許可されたアプリケーションやデータにアクセスできるようにするという考え方です。SASEは、ゼロトラストアーキテクチャ上で、主にネットワークセキュリティの要素/機能を提供するソリューションであり、ゼロトラストを実現する要素の1つと位置づけています。

SASEを構成する機能として、ネットワーク機能（プライベートバックボーン、SD-WAN、Proxyなど）とネットワークセキュリティ機能（ファイヤーウォール、SWG、ZTNA、CASB、RBIなど）を挙げています。これらの機能はクラウドベースで提供されることが大前提となります。

SASE導入のメリットとして、以下の4点を挙げています。

1.  ネットワーク構成がシンプルになる：ネットワークとネットワークセキュリティの機能がクラウドサービスで提供されるため、ネットワーク領域を集約化できる。運用負荷の軽減、コスト削減、セキュリティレベルの向上につながる。
2.  全ての通信をリアルタイムに一元管理できる：通信量や通信内容を可視化できるため、通信帯域の逼迫原因の特定、ネットワーク障害やセキュリティインシデント発生時の対処が迅速に行える。
3.  境界だけでなく、拠点/端末間の通信も全てセキュアに管理できる：巧妙化するサイバー攻撃（ゼロデイ攻撃、サプライチェーン攻撃など）への対策として有効。
4.  すぐに利用できる・スモールスタートできる：クラウドサービスの特性を活かし、インターネット回線があれば迅速に利用開始でき、小規模から始められる価格形態のSASEも多い。

最後に、SASEは初心者にとって理解が難しい領域だが、この記事がSASEに対する理解を深める一助となれば幸いであると述べています。


---

# Starlink miniが変えた世界、各国の認証を追ってみました | IIJ Engineers Blog

[View on Company Blog](https://eng-blog.iij.ad.jp/archives/30711)

Starlink miniが日本で利用可能になった背景と、その過程で取得された各国の認証について解説されています。

1. **Starlink miniの利用可能範囲拡大:**
   - 以前は利用地域が制限されていたStarlinkの契約プラン"ROAM"が改訂され、無線機として認証されている国であればどこでも利用可能になった。

2. **日本登場までの道のり:**
   - Starlink miniが公式に発表された国々を調査した結果、日本での登場までに約半年を要した。ただし、公式発表がない国でも利用可能な場合がある。

3. **本体に印刷された認証ロゴ:**
   - Starlink mini本体に印刷されている認証ロゴの変化を比較し、初期から徐々に認証の種類が増えていった様子を紹介。

4. **認証ロゴの詳細:**
   - 各認証ロゴの意味と、それが示す認証機関や規格について解説。
   - 日本の技適マーク、FCC（米国）、CONATEL（ベネズエラ）、アメリカ・カナダ・メキシコの安全規格、EU基準適合マーク、カナダのEMI規格、メキシコのIFT、アルゼンチン、オーストラリア・ニュージーランド、セルビア、フィリピンの認証ロゴ、WEEE指令（リサイクルに関するマーク）などが紹介されている。

5. **まとめ:**
   - Starlink miniには、電波関係だけでなく製品として必要な様々な認証が組み込まれており、今後利用可能国が増えるにつれて認証マークも増える可能性があると述べています。

---

# SageMaker AI Studioのユーザープロファイルに特定のユーザーだけアクセスできるようにする | サーバーワークスエンジニアブログ

[View on Company Blog](https://blog.serverworks.co.jp/2025/03/19/181816)

SageMaker AI Studio のユーザープロファイルへのアクセスを特定のユーザーに制限する方法について解説する。

**設定の概要**

IAMユーザーとSageMaker AI Studioのユーザープロファイルにタグを付与し、そのタグをアクセス制御に利用する。IAMロールを使用する場合も同様の手順となる。

**設定手順**

1.  **SageMaker AI Studio のユーザープロファイル作成:** 通常の手順でユーザープロファイルを作成し、タグを付与する。
2.  **IAM ユーザーへのタグ付与:** IAM ユーザーにタグを付与する。この際、タグの Value はユーザープロファイルに付与したタグの Value と同じにする。
3.  **IAM ポリシーの作成:** IAM ポリシーを作成し、JSON で編集する。公式ドキュメントとは異なり、Deny ステートメントを使用する。これにより、指定されたタグを持つユーザープロファイル以外へのアクセスを拒否し、他の IAM ポリシーで許可されているアクセスも制限できる。
4.  **IAM ユーザーへの IAM ポリシー適用:** 作成した IAM ポリシーを IAM ユーザーに適用する。複数の IAM ユーザーがいる場合は、IAM グループから適用することを推奨する。

**動作確認**

*   別のユーザープロファイルへのアクセスは拒否される。
*   自分のユーザープロファイルへのアクセスは許可される。

この設定により、IAM ユーザーが Administrator 権限などの広範な権限を持っている場合でも、Deny ステートメントによってアクセス制限が有効になる。

---

# Unity Sentis 姿勢推定 | ヘッドウォータースのフィード

[View on Company Blog](https://zenn.dev/headwaters/articles/53c3477a9df662)

Unity Sentis を利用した姿勢推定に関する記事の要約です。

**概要:**

Unity Sentis を用いて人体の姿勢推定を行う方法について解説しています。OpenPose、MediaPipe Pose、HRNet などのモデルを ONNX 形式で用意し、Unity にインポートして Sentis の API で推論を実行します。Webカメラや VR デバイスのカメラから入力映像を取得し、モデルの出力から関節位置を取得して 3D オブジェクトに適用することで、骨格の描画やアバターへの適用を実現します。

**詳細な手順:**

1.  **姿勢推定モデルの準備:** MediaPipe Pose (ONNX 形式に変換)、OpenPose、HRNet などの事前学習済み ONNX モデルを準備します。
2.  **Unity Sentis のセットアップ:** Unity 2022 以降で Sentis パッケージをインストールし、ONNX モデルを Unity プロジェクトにインポートします。Sentis の API を使用してモデルをロードします。
3.  **カメラ入力の取得:** Web カメラや VR カメラからリアルタイムで映像を取得します。
4.  **推論の実行:** カメラ画像を Sentis に渡し、推論を実行します。
5.  **推論結果の処理:** 姿勢推定モデルの出力から関節位置を取得し、Unity のオブジェクトに適用します。
6.  **Unity 上で可視化:** 取得した関節位置データを、3D モデルやラインレンダラーで描画します。

**応用例:**

*   Meta Quest 2 のカメラを利用したアバターのリアルタイムな動きの推定
*   Unity 上でのトラッキングによる動作改善のフィードバック
*   プレイヤーの姿勢解析によるアクションのトリガー

**まとめ:**

最先端テクノロジーを活用し、共に未来を創造するエンジニアを募集しています。

---

# 【個人開発】リリース1ヶ月で月5万円(理論値)のサービスを作ったのでノウハウを全公開してみる（Next.js / Rails）

[View on Qiita Trend](https://qiita.com/tomada/items/b992245a4162ddeb1f6e?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事は、フリーランスのWebエンジニアであるとまだ氏が、個人開発したUdemy講座のアフィリエイトサービス「Learning Next」の開発背景、技術的な工夫、集客・マネタイズ戦略を公開したものです。

**サービスの概要**

Learning Nextは、Udemy講座のレビューをAIで分析・要約し、講座選びの課題を解決するサービスです。客観的な評価基準を提供し、ユーザーが自分に合った講座を見つけやすくすることを目指しています。

**リリースの成果**

*   リリース1ヶ月で月5万円の収益（理論値）を達成
*   ドメイン評価が低いにもかかわらず、SEO対策によりGoogle検索からの流入だけで成果を出す

**主な特徴**

*   **AIを活用したレビュー分析:** ClaudeのAPIを使用して、レビューのポジティブ・ネガティブな面や、どのような人に向いているかを要約
*   **4つの評価軸でのスコアリング:** わかりやすさ、実践力、サポート体制、教材品質の4つの観点で講座を評価
*   **柔軟な検索・フィルタリング:** カテゴリやトピックを細かく設定し、絞り込み検索を可能にする
*   **学習ロードマップの提供:** フロントエンドエンジニアやバックエンドエンジニアなど、目標に合わせた学習ロードマップを提供

**SEO対策**

Google検索からの流入を増やすために、以下のSEO対策を実施しました。

*   **検索キーワードの調査:** ラッコキーワードなどのツールで検索ボリュームを調査し、需要のあるキーワードをターゲットにコンテンツを作成
*   **構造化データの設定:** 検索結果に星評価や料金が表示されるように、構造化データを埋め込む
*   **メタデータの最適化:** タイトルや説明文を最適化し、検索結果やSNSでのシェア時にユーザーの興味を引くようにする
*   **パンくずリストの実装:** サイトの階層構造を示すパンくずリストを実装し、ユーザーとGoogleのボットがサイト構造を理解しやすくする
*   **Core Web Vitalsの最適化:** ページの読み込み速度や初期表示までの時間など、ユーザー体験に直結する指標を改善
*   **ページ読み込み速度の最適化:** コンテンツの遅延読み込みやキャッシュの活用により、ページ読み込み速度を向上
*   **モバイルフレンドリー対応:** スマホでも快適に閲覧できるように、レスポンシブデザインやアコーディオン、フローティングアクションボタンなどを導入

**運用コスト**

*   Vercel Proプラン: 月額$20
*   Supabase無料プラン（自動バックアップなし）
*   Rails API は Render.com + Neon DB で無料運用

**まとめ**

個人開発でサービスを成功させるには、集客・マネタイズ戦略が重要です。SEO対策やユーザー目線でのUI/UX改善を行うことで、検索エンジンからの流入を増やし、収益につなげることができます。


---

# Linux で 確定申告 2024年度版

[View on Qiita Trend](https://qiita.com/nanbuwks/items/3ceb0b3f8e15a8aa3dbf?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事は、Linux (Ubuntu 22.04 LTS) 環境で確定申告を行う方法を、UserAgent偽装によって実現した手順を解説しています。

以前の記事ではWindows PCを使わざるを得なかった状況を打破するため、読者からのコメントを参考に、Google ChromeのDeveloper ToolsのNetwork conditions機能を用いてUserAgentを偽装し、「確定申告書作成コーナー」を攻略しています。

具体的な手順として、まず国税庁の確定申告サイトにアクセスし、「確定申告書等を作成する」から「作成開始」に進みます。「はい」「はい」「スマートフォンを使用する」を選択するところでブロックされるため、DevToolsを開き、Network conditionsでUserAgentをWindows版Chromeに偽装します。その後、ページをリロードし、再度同じ選択をすることで、次の画面に進むことができます。

確定申告書の作成では、途中で同様のエラーが発生した場合、再度UserAgentを偽装し、ページをリロードします。医療費控除などのデータ入力後、e-Taxを使ってデータを送信します。


---

# Copilot Studio と Power Apps/Power Automate×AI Builder の使い分け

[View on Qiita Trend](https://qiita.com/Takashi_Masumori/items/8f4f638005203ce09912?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事では、Copilot Studio と Power Apps/Power Automate × AI Builder の使い分けについて、判断基準とシナリオを整理しています。

**判断基準:**

1.  **ロジック化の可否:**
    *   明確な条件分岐でプログラム化できる場合は、Power Automate × AI Builder が適しています。
    *   情報が膨大で複雑、または業務に「あいまいさ」や「例外処理」が多い場合は、Copilot Studio が適しています。RAG（Retrieval-Augmented Generation）や生成 AI を活用し、複雑な情報を扱えるためです。

2.  **ユーザーインターフェイス:**
    *   チャットベースの UI を希望する場合は、Copilot Studio が適しています。Teams や Microsoft 365 Copilot Chat への組み込みも可能です。
    *   自由度の高い UI を希望する場合や、UI が不要な場合は、Power Apps や Power Automate が適しています。

**シナリオ例:**

*   **Power Automate × AI Builder が適したシナリオ:**
    *   請求書メールから必要な情報を抽出してシステムに転記する（ロジックが明確）。
    *   メール本文から情報を抽出し、システムに転記する。
    *   契約書を比較して差分を抽出する。
    *   ロゴが正しく利用されているかを判定する。

*   **Copilot Studio が適したシナリオ:**
    *   社内 FAQ を基にユーザーの質問に回答する（質問パターンや回答パターンを網羅するのが困難）。
    *   社内規定ドキュメントを基に出張申請を自動承認する（ドキュメントのロジック化が困難）。

**まとめ:**

Copilot Studio は、RAG や生成 AI を活用して、ロジック化が難しい複雑な業務や、チャットベースの UI に適しています。Power Apps/Power Automate × AI Builder は、ロジックが明確な業務や、自由度の高い UI に適しています。

---

# 【Hono】作成したAPIをSwagger UI上で動作確認したい

[View on Qiita Trend](https://qiita.com/ryouryou_34/items/a5b566e0feb6457275cb?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事は、HonoというTypeScript/JavaScript向けのWebフレームワークで作成したAPIをSwagger UI上で動作確認する方法を紹介しています。

著者はPythonのFastAPIの経験から、HonoでもSwagger UIを使ってエンドポイントをテストし、APIドキュメントを自動生成したいと考えました。ユーザーに関連したCRUD操作のエンドポイントを作成し、Swagger UIでまとめることを目標としています。

記事では、まず必要なライブラリ（`@hono/swagger-ui`, `@hono/zod-openapi`, `zod`）をインストールし、フォルダ構成について説明しています。次に、`create.ts`ファイルでリクエストボディやレスポンス形式、ハンドラーを定義する方法を示しています。`userRoutes`というモジュールでユーザー関連のルーティングをまとめ、`app.ts`で全てのルーティングを統合しています。OpenAPIのドキュメントを定義し、Swagger UIを `/docs` で公開しています。

最後に、`localhost:8080/docs` でSwagger UIが正しく表示され、定義したレスポンス形式も確認できることを示しています。著者はHonoが使いやすく、カスタマイズ性も高いと感じており、今後も活用していきたいと述べています。

---

# 【Ruby on Rails】稼働中アプリケーションでいきなりremove_columnするのが危険な理由

[View on Qiita Trend](https://qiita.com/lafool_fh/items/b55b98ae3d4464d4f182?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

Railsアプリケーションで、カラムを削除する際に、`remove_column`をいきなり実行することの危険性と、より安全な削除方法について解説しています。

**危険な理由:**

*   ActiveRecordがテーブル情報をキャッシュしており、`eager_load`などのメソッドでキャッシュされた情報を使ってクエリを組み立てるため。
*   複数サーバー構成の場合、サーバーごとに変更反映のタイミングがずれ、キャッシュの不整合によってエラーが発生する可能性がある。例えば、あるサーバーでカラムが削除された後も、別のサーバーが古いキャッシュを参照し続けることでエラーが発生する。

**安全なカラム削除方法:**

1.  `ignored_columns`を使用する。削除予定のカラムを`ignored_columns`に指定することで、ActiveRecordがそのカラムを参照しなくなる。これにより、キャッシュに影響を与えなくなる。
2.  `remove_column`を実行する。`ignored_columns`の設定を反映後、`remove_column`を実行してカラムを削除する。

この2段階のアプローチにより、複数サーバー環境でも安全にカラムを削除できる。

**まとめ:**

カラム削除時は、キャッシュの影響を考慮し、`ignored_columns`と`remove_column`を組み合わせて段階的に行うのが安全。

---

# Terraformのバージョン管理を劇的に快適に！「tenv」のインストールから使い方まで解説

[View on Qiita Trend](https://qiita.com/kazushi_ohata/items/d1cd1b921798325dd1a9?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事では、Terraformのバージョン管理ツール「tenv」について、その重要性、インストール方法、基本的な使い方を解説しています。

まず、Terraformのバージョン管理の重要性について、異なるバージョン間での機能の違いや非互換性によるエラーを避けるために、バージョンを揃えて開発する必要性を説明しています。`required_version`属性によるバージョン固定の方法を紹介しつつ、新機能の試用や案件ごとのバージョン指定など、柔軟なバージョン切り替えのニーズに応えるツールとしてtenvを紹介しています。

tenvは、Terraformだけでなく、OpenTofu、Terragrunt、Atmosといったツールのバージョン管理も統合的に行えるツールです。類似ツールであるtfenvとの違いとして、tenvがGo言語で実装されており、Windows環境を含む様々なプラットフォームで利用可能である点、tfenvの開発が停止状態にある点を挙げています。

tenvのインストール方法については、MacOS、Windows、Linuxなど、主要なOSに対応した手順を説明しています。Windows環境での手動インストール（バイナリダウンロード）の手順も解説しています。

tenvの基本的な使い方として、特定バージョンのインストール、インストール済みバージョンの確認、インストール可能なバージョンの確認、バージョンの切り替え、バージョンのアンインストールといった操作方法をコマンド例とともに紹介しています。

最後に、tenvの導入がTerraformプロジェクトの安定性と開発効率の向上に貢献するとして、Terraformエンジニアに推奨しています。

---

# 画像生成AIツール「Recraft」で簡単に画像が作れて感動した話

[View on Qiita Trend](https://qiita.com/Aichi_Lover/items/2ebf5e204b3eab468fd8?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

画像生成AIツール「Recraft」の紹介記事です。Recraftは、テキストから高品質な画像を生成できるAIツールで、直感的なインターフェース、多彩なスタイル設定、高品質な画像生成能力、無料プランでも十分に使える機能性が特徴です。

Recraftで画像を生成する手順は以下の通りです。

1.  Recraftの公式サイトにアクセスし、アカウントを作成・連携してログインする。
2.  画面左側のテキスト欄に、プロンプト（生成したい画像の内容）を入力し、「Recraft」を選択する。
3.  少し待つと画像が出力される。
4.  画像を保存したい場合は画像を左クリックし、「Copy Image to clipboard」を選択し、ペイントなどに貼り付けて保存する。

Recraftでは、絵のタッチや画像サイズを変更することも可能です。プロンプトのコツとしては、具体的な描写を心がける、色や質感を指定する、光源や時間帯を指定すると、より意図に近い画像を生成できます。

Recraftは、誰でも簡単に高品質な画像を生成できるAIツールであり、無料プランでも十分に活用できます。直感的な操作性で初心者でも使いやすく、色々な雰囲気の画像を生成できるので、ぜひ活用してみてください。

---

# 【Oracle】はじめてのPL/SQL

[View on Qiita Trend](https://qiita.com/horyyyyyy/items/ce980fe4a262b5fe69e8?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事は、Oracle Databaseで使用されるプログラミング言語であるPL/SQLの入門記事です。

PL/SQLはSQLを拡張したもので、条件分岐やループ処理、エラーハンドリングなどのプログラミング機能を追加することで、データベース操作をより柔軟かつ効率的に行えます。PL/SQLの基本的な構文は、DECLARE（変数の宣言）、BEGIN...END（実行する処理の記述）、EXCEPTION（エラーハンドリング）の3つのブロックで構成されます。

記事では、社員情報を処理する簡単なサンプルコードを通じて、PL/SQLの具体的な使用例を示しています。このサンプルコードでは、SELECT INTO文で社員IDに基づいて社員名と給与を取得し、DBMS_OUTPUT.PUT_LINEで結果を表示します。また、NO_DATA_FOUNDエラーやその他のエラーを捕捉するエラーハンドリングの例も紹介しています。

PL/SQLを効率的に実行するために、DBMS_OUTPUTを有効にする必要があることに言及しています。

最後に、PL/SQLはデータベース内での処理を効率化するための強力なツールであり、基本的な構文を習得することでOracle DBの処理をより柔軟に操作できるようになることを強調しています。さらに、PL/SQLパッケージを用いたリソース管理についても触れられています。


---

# metabaseのv0.52系からv0.53系のバージョンアップで、APIの変数の指定方法が変わった

[View on Qiita Trend](https://qiita.com/erikaadno/items/4659bfdc191566e04551?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

Metabaseのv0.52系からv0.53系へのバージョンアップに伴い、APIの変数の指定方法が変更された。

**対象APIエンドポイント:**

*   `/api/card/${card-id}/query/${export-format}`

**変更点:**

*   **v0.52系:**
    *   `parameters` はクエリパラメータとして指定
    *   `POST https://$domain/api/card/$card-id/query/json?parameters=[{ "target": ["variable", ["template-tag", "${変数名}"]],"type": "category", "value": "${変数の値}"}]`
*   **v0.53系:**
    *   `parameters` はリクエストボディのJSONとして指定
    *   `POST https://$domain/api/card/$card-id/query/json`
    *   リクエストボディ:
        ```json
        {
          "format_rows": false,
          "pivot_results": false,
          "parameters": [
            {
              "type": "category",
              "target": ["variable", ["template-tag", "${変数名}"]],
              "value": "${変数の値}"
            }
          ]
        }
        ```

バージョンアップの際は、API呼び出し時の変数の指定方法に注意する必要がある。


---

# バーチャルキャストでAITuberと会話する放送をする為のシステム作成

[View on Qiita Trend](https://qiita.com/syota_aiai/items/04edbb59121f074e3ab3?utm_campaign=popular_items&utm_medium=feed&utm_source=popular_items)

この記事は、バーチャルキャストでAITuberと会話する放送を行うためのシステム構築手順を解説するものです。

**システム概要**

*   AI: GoogleのGeminiを使用 (2025年3月時点では無料または安価だが、価格変更の可能性あり)
*   音声合成: VOICEVOXを使用 (初期設定は四国めたん)
*   開発環境: Unity、VirtualCastToolKit

**必要なもの**

*   ある程度のUnity、バーチャルキャストの知識
*   グラフィックボード搭載のゲーミングPC
*   VRM1.0アバター
*   Google AI Studio APIキー

**構築手順**

1.  **VirtualCastToolKitの導入**: プロジェクト版をダウンロードし、Unity HubでUnity 2022.3.20f1を使用してUnityプロジェクトを起動
2.  **VRM1.0アバターの設定**: @toRisouP様のQiita記事を参考に、NuGetForUnity、VoicevoxClientSharp.Unity、UniTaskをインストール。プロジェクトを「.NET Standard 2.1」に変更し、VRoidで作成したVRM1.0アバターをUnityにインポートして、アバターがしゃべるように設定
3.  **Gemini連携設定**: yu様のno+eの記事を参考に、Google AI StudioでAPIキーを取得し、Apps ScriptにGASのソースコードを貼り付け。デプロイIDを取得し、UnityでGeminiが使えるように設定
4.  **uOSCのインストール**: hecomi様のuOSCをインストールし、アバターの口パクをバーチャルキャストに送信
5.  **VCI化**: 新しいUnityシーンを作成し、GameObjectに「Gemini_VCI」という名前を付けてVCI Objectを追加。LUAスクリプトを記述し、VRM0.0アバターを子オブジェクトとして追加。VCI Sub Item、Sphere Collider、VCI Spring Boneなどのコンポーネントを追加し、VCIをバーチャルキャストにアップロード

**配信時の設定**

*   バーチャルキャストでアイテム「Gemini_VCI」を設置
*   バーチャルキャスト内で「マイク」を手元に配置
*   PC画面上に「VOICEVOX」と「Unity」を起動
*   Windows11の音声認識をオンにする
*    バーチャルキャストでOSC通信の設定をする

**配信時の注意点**

*   質問は6文字以上になるように話す
*   Geminiの最初の返答は少し時間がかかる
*   Geminiの返答の文字数が多いときは、音声認識が止まる場合がある

---

# FP8 trainingを支える技術 1

[View on Zenn Trend](https://zenn.dev/kaz20/articles/52fee6a3c9ea3b)

## FP8 Trainingを支える技術：詳細な要約

この記事は、FP8（8ビット浮動小数点）を用いたモデル学習の基礎知識を解説しています。東京科学大学の藤井氏が、FP8の概要から、その利用理由、スケーリング、そしてPyTorchにおける実装までを詳細に説明しています。

**1. FP8の概要:**

*   FP8は、Hopper世代のGPUからサポートされた数値精度で、E4M3とE5M2の2つの形式があります。
*   E4M3は、指数部4ビット、仮数部3ビットで構成され、最大値は448、最小の正の値は2<sup>-6</sup>（subnormal数利用時は2<sup>-9</sup>）です。
*   E5M2は、指数部5ビット、仮数部2ビットで構成され、E4M3よりも広い範囲の値を表現できますが、精度は粗くなります。最大値は57,344、最小の正の値は2<sup>-14</sup>です。
*   E5M2にはInfinitiesと複数のNaN表現があります。
*   E4M3の方が精度は高いが、表現できる範囲が狭く、E5M2はその逆です。
*   絶対値が大きいほど表現可能な値の間隔は広がり、量子化誤差が大きくなる可能性があります。

**2. FP8を利用する理由:**

*   メモリ使用量の削減、計算量の削減、FP8 Tensor Coreによる処理の高速化が主な理由です。
*   学習においては、処理高速化が特に重要視されています。

**3. FP8スケーリング:**

*   FP8は表現可能な範囲が狭いため、高精度テンソルを直接FP8に変換するとオーバーフローが発生しやすくなります。
*   スケーリングは、テンソルの値をFP8の表現可能な範囲に収めるために行います。
*   スケーリングには、テンソル全体に対して1つのスケーリングファクターを適用するTensor Wiseと、テンソルの軸ごとに異なるスケーリングファクターを適用するAxis Wiseの2つの粒度があります。
*   Axis Wiseの方が精度は高くなりますが、実装が複雑でオーバーヘッドも大きくなります。
*   Dynamic Scalingでは、FP8に変換する対象からスケールを算出しますが、Delayed Scalingでは過去の履歴（amax history）を参照してスケールを決定します。
*   DynamicScalingでは速度面で不利ですが、Delayed Scalingにおけるclamp処理による学習の不安定化を防ぐことができます。
*   torchaoでは、Delayed Scalingは廃止されました。

**4. FP8 Linear Matmulの実装:**

*   torchaoにおけるFP8 Linear Matmulの実装を解説しています。
*   まず、weightをFP8に変換するためのスケーリングファクターを計算し、FP8に変換します。
*   次に、activationをFP8に変換し、matmulを実行します。
*   PyTorchでは、入力テンソルと出力テンソルの数値精度が異なる形式のmatmulが定義されていません。
*   Linearの出力をBF16で受け取れないことは、不要なFP8への変換を挟むことになり、量子化誤差やオーバーヘッドを増大させるため問題です。

**5. まとめ:**

この記事では、FP8学習を支える基礎技術として、FP8の数値表現、スケーリング手法、PyTorchでの実装について詳細に解説しました。FP8を利用することで、メモリ効率や計算速度を向上させることができますが、同時に精度に関する課題も存在します。今後の続編では、より踏み込んだ内容や、低精度計算における問題点について解説される予定です。

---

# 野村総研が2015年に発表した「AIに代替される可能性が高い職業、低い職業」リストを今見ると、真逆になってる例がある

[View on Hatena Trend](https://togetter.com/li/2527286)

2015年に野村総研が発表した「AIに代替される可能性が高い職業、低い職業」リストが、現状と逆になっている例が見られるという話題。

当時、代替されにくいとされた職業がAIによって代替され始め、代替しやすいとされた受付係やホテル客室係のような接客業は、機械化が難しいという意見も出ている。肉体労働はロボット導入コストや安全面のリスクから代替が遅れているという指摘もある。

また、アナウンサーのような職業はAIで代替可能ではないかという意見や、知的労働であるホワイトカラーの方がAIによって代替されやすいという意見もある。生成AIの登場によってAIの性能評価が変わり、以前の予測が外れているのは当然という見方も。

コスト対効果の高いものや、データ蓄積によってAIが学習しやすい専門性の高い職種から代替が進むという意見、社会的な責任感の観点からAI依存が進まないのではないかという意見も存在する。

さらに、野村総研のレポート自体が、自社のセールスのために書かれたのではないかという指摘もある。

---

# 読み取り専用 DB を Aurora から SQLite に移行してコストを 1/8 に削減した話 - エムスリーテックブログ

[View on Hatena Trend](https://www.m3tech.blog/entry/sqlite-on-ecs-fargate)

エムスリーデジカルのDBをAurora MySQLからFargate上のSQLiteへ移行し、コスト削減と性能向上を実現した事例。

**背景:**
*   デジカルの過去のカルテ入力傾向を学習し、入力候補を表示する自動学習機能のインフラコストが高かった。
*   セルベースアーキテクチャを採用しており、各シャードごとにdb.r6g.largeインスタンスを2つずつ配置していたため、コストが無視できないレベルになっていた。

**移行:**
*   Aurora MySQLからSQLiteへ移行。
*   日次のデータ洗替えは、CSVファイルからSQLiteファイルを生成し、S3に配置。
*   各Taskは起動時にS3からSQLiteファイルを取得し、インメモリモードのDBに全件コピーしてクエリを実行。

**効果:**
*   インフラコストを1/8程度に削減。Aurora MySQLが不要になったことが大きい。
*   Fargate Taskのリソースは増強（0.25vCPU/0.5GiB RAM → 1vCPU/4GiB RAM）
*   性能も向上。スループット・レイテンシが改善し、約750rpsの負荷でも100ms台のレイテンシで安定。

**SQLite選定理由:**
*   サービス無停止で日次の全件洗替えが可能。
*   読み取り専用でSQLが使える。
*   データ量は1.5億レコード・約15GBで、今後漸増する。
*   各シャードでピーク時に200rps程度のスループットが必要。
*   Aurora Serverless、RDS、DynamoDBと比較し、コスト削減効果が大きかった。

**インメモリSQLite:**
*   当初はSQLiteファイルをディスクに配置する予定だったが、Ephemeral Diskの性能制限とEBSのコスト高からインメモリに移行。
*   不要なデータを削減し、データ量を数GB程度まで削減。
*   SQLiteのBackup APIを利用し、ファイルモードからインメモリモードへ全件コピー。

**まとめ:**
読み取り専用DBの場合、SQLiteは本番環境でも実用的な選択肢となり得る。

---

# Devindabot: Devinで実現するライブラリの脆弱性自動対応システム - freee Developers Hub

[View on Hatena Trend](https://developers.freee.co.jp/entry/devindabot)

freeeでは、ライブラリの脆弱性対策としてDependabotを活用しているが、Pull Request作成後の対応（影響分析、破壊的変更への対応、QA）に人的コストがかかるという課題があった。そこで、これらのステップをAIエージェントDevinに任せる取り組みを開始した。

具体的には、Dependabotのアラートを収集し、Directな依存ライブラリの脆弱性アラートをDevinが処理するようにした。Devinには、GitHub Advisory Databaseの情報に基づき、Pull Requestの影響分析や修正案の作成を指示。初期段階では、Devinの修正案作成能力に課題が見られたため、指示内容を改善した結果、脆弱性対応の修正部分まで対応できるようになった。

この取り組みにより、初動トリアージの工数が削減され、開発チームへの具体的な調査結果の提案が可能になり、修正/QAコストも大幅に削減された。Devinの稼働コストは1アラートあたり700円程度だが、コンテキストスイッチによる生産性低下を考慮すると、十分な投資対効果が得られると結論付けている。

---

# デジタル相“「通信の秘密」侵害せず”「能動的サイバー防御」 | NHK

[View on Hatena Trend](https://www3.nhk.or.jp/news/html/20250319/k10014754331000.html)

デジタル相は、「能動的サイバー防御」導入のための法案審議で、政府による恣意的な運用は行われず、憲法が保障する「通信の秘密」は侵害されないと強調した。

この法案は、政府が電気や鉄道などの重要インフラ関連事業者と協定を結び、サイバー攻撃の兆候を監視するために通信情報を取得できるようにするもの。

デジタル相は、国家を背景とした高度なサイバー攻撃への懸念を踏まえ、対処能力の強化が喫緊の課題だと述べ、法案によって対応能力を欧米主要国と同等以上に向上させることが不可欠だと説明。

民間通信情報の取得については、不正が疑われる情報のみが自動的に選別され、選別基準は新設される独立機関が事前に審査するなど、恣意的な運用が行われない仕組みを確保しており、「通信の秘密」の侵害は十分に防止されると強調した。

---

# 常駐エンジニア、顧客に「本当の所属会社はどこか」と聞かれ正直に答えてしまった

[View on Hatena Trend](https://xtech.nikkei.com/atcl/nxt/column/18/00084/00362/)

ソフトハウスA社に勤務するエンジニアが、大手IT企業X社の下請けとして顧客先Y社で作業中、Y社の社員に「本当の会社はどこか」と聞かれ、A社と答えたところ、X社のリーダーに叱責されたという相談事例。

記事では、X社リーダーが下請けの存在を顧客に伝えておくべきだったとし、質問者は気にする必要はないと述べている。Y社は質問者の所属会社を問題視しておらず、X社リーダーが神経質になっている可能性を指摘。

大手IT企業ではパートナー会社（下請け会社）へのシステム開発案件の発注は一般的であり、通常は問題にならない。ただし、顧客先での常駐作業においては、エンジニアの身分を明確にする必要があり、X社はY社に対し、A社のエンジニアであることを明示すべきだったと筆者は考えている。

---

# 【連載】Cybozu.comクラウド基盤の全貌 - Cybozu Inside Out | サイボウズエンジニアのブログ

[View on Hatena Trend](https://blog.cybozu.io/entry/2025/03/19/112000)

サイボウズのクラウド基盤「cybozu.com」の全貌を紹介する連載のイントロダクション。

- cybozu.comは2011年にサービスを開始し、パッケージ製品のクラウド化やkintoneの開発を通じて、より多くのユーザーにサービスを提供してきた。
- クラウド基盤本部は、24時間365日の安定稼働を維持しつつ、ビジネスの成長に合わせて基盤を刷新する役割を担う。
- Necoプロジェクトによるクラウド基盤のアーキテクチャを紹介し、Kubernetesの導入と活用について解説する。
- クラウド基盤は、データセンター管理とネットワーク設計、Kubernetesの導入とクラスタ構築、データの永続化とKubernetes上のストレージ、MySQLクラスタ、アプリケーション実行環境（AP）という階層で構成されている。
- オンプレミスで構築されたデータセンターをセキュアなネットワークで接続し、Kubernetesクラスタのノードとして統合。
- Ciliumによるコンテナネットワーク、TopoLVMとRookによるストレージ管理、MOCOによるMySQLクラスタの自動セットアップなどを導入。
- kintoneやGaroonなどのアプリケーションはAPと呼ばれるpod群で実行され、ロードバランサーを介してリクエストが処理される。
- 今後、各層の内容を専門のエンジニアが詳細に解説していく。

---

# 令和なのに「Core 2 Duo」登場？と思ったら古くて新しいスマートウォッチだった

[View on Hatena Trend](https://pc.watch.impress.co.jp/docs/news/1671353.html)

スタートアップ企業のCore DevicesとEric Migicovsky氏が、Googleがオープンソース化したPebble OSを採用したスマートウォッチ「Core 2 Duo」と「Core Time 2」を発表した。

*   **Core 2 Duo**: 2025年7月出荷予定、149ドル。1.26型モノクロディスプレイ、4つのボタン、6軸IMU、コンパス、気圧計、マイク、スピーカー、バックライト、バイブレーター搭載。30日間のバッテリー駆動、IPX8防水を目標。
*   **Core Time 2**: 2025年12月出荷予定、225ドル。1.5型64色カラーディスプレイ、4つのボタン、タッチスクリーン、6軸IMU、心拍数計、マイク、スピーカー、バックライト、バイブレーター搭載。推定30日間のバッテリー駆動、IPX8防水を目標。

Pebbleは、Kickstarterで資金を集め、電子ペーパー採用のスマートウォッチとして4年間で200万台を販売。2016年にFitbitに買収され、その後FitbitがGoogleに買収されたことでPebble OSもGoogleに移管されたが、2025年1月にGoogleによりPebble OSのほとんどがオープンソース化された。今回、Eric Migicovsky氏がCore Devicesに資金を提供して製品を開発。Core 2 Duoは「Pebble 2」、Core Time 2は「Pebble Time 2」のアップグレード版として登場する。Pebble OSの採用により、既存の1万を超えるウォッチフェイスおよびアプリが利用できる。AndroidおよびiOS向けのコンパニオンモバイルアプリが公開予定。新しいPebble OS対応のウォッチフェイスやアプリを作成するために更新されたSDKにも取り組む予定。

---

# 電子工作メディア「fabcross」がサイトの終了と閉鎖を発表。4月1日以降は閲覧不可に【やじうまWatch】

[View on Hatena Trend](https://internet.watch.impress.co.jp/docs/yajiuma/2000030.html)

電子工作メディア「fabcross」が2025年3月31日をもってサイトを閉鎖し、4月1日以降は閲覧できなくなることが発表されました。株式会社メイテックが運営する「fabcross」は2013年にオープンし、電子工作を行う人向けの記事を十数年にわたって配信してきましたが、閉鎖の理由は特に触れられていません。「fabcross forエンジニア」も合わせて閉鎖され、メールマガジンも3月18日で終了しています。


---

# Devinとの日々を振り返る

[View on Hatena Trend](https://zenn.dev/smartshopping/articles/032148d8b85aac)

エスマットエンジニアの金尾氏が、AIエージェント「Devin」を2ヶ月ほど使用した所感と今後の活用についてまとめた記事です。

**Devinの特徴**

*   **遅い:** 他のAIエージェントと比較して作業スピードは速くない。
*   **やらかす:** 指示が曖昧だと予想外の挙動をすることがある。
*   **自律で作業できる:** セッションごとに独立した環境で作業するため、並列化が容易。

**しくじりと学び**

*   **しくじり1:** 雑な依頼で暴走。
    *   **学び1:** 公式ガイドラインを読み、タスクを細分化し、作業内容・完了条件を明確に定義する。
*   **学び2:** Devinは軽微な修正が得意。自己完結型であるため、割り込みのバグ修正などに活用できる。VSCode拡張が便利。
*   **しくじり2:** 大きな修正だとタスク準備とPRレビューに追いつかない。
    *   **学び3:** 進捗管理をDevin自身に記述できる場所で行う（issueや進捗管理ファイル）。
    *   **学び4:** Devinへの依頼プロンプト自体を別のAIエージェントに作成してもらう（APIを利用）。
    *   **学び5:** Feature Flagやスプラウトクラス/メソッドなどのプラクティスを活用し、安全に作業を進める。
*   **しくじり3:** タスクを細かくしても暴走することがある。
    *   **学び6:** すべてをDevinに任せず、linterや自動テストなどの自動化ツールを併用する。pre-commitなどで強制的に実行させる。

**まとめ**

Devinは癖はあるものの、使いどころがマッチすれば非常に優れた働きをする。プロダクトのデプロイ頻度も向上し、改善を実感している。今後は大規模改修でのさらなる活用やコスト面での最適化を図っていく。


---

# 生成AIによるウォーターマーク剥がしは著作権侵害になり得るか？（栗原潔） - エキスパート - Yahoo!ニュース

[View on Hatena Trend](https://news.yahoo.co.jp/expert/articles/c8fe8b6b8868ad830dac2f0180ecda9911ce56f7)

生成AIによるウォーターマーク剥がしが著作権侵害になり得るかについて、著者の見解が述べられています。

記事では、画像や動画の無許諾コピーを防ぐ手段であるウォーターマーク（透かし）に着目し、生成AIによって容易に除去できる現状を踏まえ、その行為が著作権法に抵触するかを検討しています。

著者は、ウォーターマークを外しても著作権がなくなるわけではない点を強調した上で、Xで提唱されている「同一性保持権侵害」について、著作物の本質的特徴が改変されるとは言えないため、侵害は問にくいとの見解を示しています。

一方、「氏名表示権」については、著作者名を示すウォーターマークの場合、侵害に当たる可能性が高いと指摘しています。判例を基に、著作者氏名を表示するウォーターマークを外す行為は、氏名表示権の侵害とされる可能性が高いとしています。

著作財産権に関しては、権利管理情報の削除を侵害とみなす規定があるものの、目に見えるウォーターマークに適用されるかは微妙としています。また、商標法については、ウォーターマークが登録商標である場合でも、著作権とは異なり「業としての」行為にしか及ばないため、このケースでの検討はあまり意味がないとしています。

結論として、著作者氏名を含むウォーターマークを外すことは氏名表示権の侵害にあたるが、それ以外のパターンでは違法性を問うのは難しいとしています。

---

# GitLab、SAML SSO認証に関わる重大な脆弱性を修正

[View on CodeZine Trend](http://codezine.jp/article/detail/21208)

GitLabは3月12日にクリティカルパッチをリリースし、重大なSAML SSO認証の脆弱性を修正した。GitLab Community Edition（CE）および Enterprise Edition（EE）のバージョン 17.9.2、17.8.5、17.7.7がリリースされ、アップデートが強く推奨されている。特定の条件下では、IdPからの有効な署名済みSAMLドキュメントにアクセスできる攻撃者が、別の有効なユーザーとして認証できる可能性があった。GitLab.comでは既にパッチ適用バージョンが稼働しており、GitLab Dedicatedのユーザーは対応不要。すぐに更新できない場合の緩和策も提示されている。

---

# さくらインターネット、パブリッククラウド「さくらのクラウド」に13の新機能を追加

[View on CodeZine Trend](http://codezine.jp/article/detail/21199)

さくらインターネットは、パブリッククラウド「さくらのクラウド」に13の新機能を追加した。
新機能として、2月21日から「さくらのクラウド」各サービスおよび各種サイトの稼働状況を表示する「サービス・ウェブサイトの稼働情報」が利用可能になった。
また、以下の機能がβ版として無料で利用できる。

*   AppRun：コンテナイメージからアプリケーションを簡単にデプロイし、自動的にスケーリングできるマネージドサービス
*   シンプルMQ：ソフトウェアコンポーネント間でのデータ送受信が可能なマネージド型のメッセージキューサービス
*   シンプル通知：メールやWebhookを利用して簡単に通知を送信できるサービス
*   EventBus：イベント検知サービスとジョブスケジュールサービスを統合したマネージドサービス
*   シークレットマネージャ：シークレット情報を管理・保管するためのサービス
*   KMS（Key Management Service）：暗号鍵のライフサイクル管理を行うためのサービス
*   APIゲートウェイ：Web APIのルーティング／リクエストレスポンス変換／認証認可を行うマネージドサービス
*   NoSQL：Apache Cassandra互換のマネージドデータベースサービス
*   クラウドHSM：HSM（Hardware Security Module）のリソースをクラウド上で提供するサービス
*   モニタリングスイート：システム監視を行うためのプラットフォーム
*   Workflows：「さくらのクラウド」上にワークフローを実行する基盤を提供するサービス
*   マイグレーションサービス（移行ツール）：VMware環境から「さくらのクラウド」への移行を効率的に行うためのツール

---

# 「ガートナー データ＆アナリティクス サミット」、5月20日〜22日にグランドニッコー東京 台場で開催

[View on CodeZine Trend](http://codezine.jp/article/detail/21200)

ガートナージャパンは、「ガートナー データ＆アナリティクス サミット」を5月20日から22日にかけて、グランドニッコー東京 台場にて開催する。
このカンファレンスは、データ／アナリティクス（D＆A）、AI担当リーダー向けに、データ管理、ガバナンス、アーキテクチャなどD＆Aに関する最新トレンドや知見を提供する。
Gartnerのエキスパートや業界リーダーとの交流、双方向型セッション、One-on-Oneミーティング、ソリューション展示などを予定。
主な参加対象は、CDAO、データサイエンス/AI責任者、D＆Aガバナンス責任者、アナリティクス/BI責任者、データエンジニア、AIエンジニア/アーキテクト、ビジネスインテリジェンスアーキテクト、D＆Aアーキテクトなど。
参加費は、早期割引で213,400円（税込）、通常価格で243,100円（税込）、官公庁・公社・団体向け価格で204,600円（税込）。参加には事前登録が必要。

---

# ガートナージャパン、AIリスクへの対処に関する日本企業への提言を発表

[View on CodeZine Trend](http://codezine.jp/article/detail/21201)

ガートナージャパンが日本企業向けにAIリスクへの対処に関する提言を発表した。提言では、企業におけるAIリスクへの取り組みは、AIガバナンス、AIリスクマネジメント（一般従業員向け）、AIリスクマネジメント（開発／運用者向け）の3つに集約されると指摘。

取り組む順番としては、まずAIガバナンスを確立し、次に一般従業員向け、自社開発を行う場合は開発／運用者向けに進めることを推奨。ただし、緊急性から一般従業員向けから着手する組織も多い。

一般従業員向けのリスクマネジメントを進めるには組織全体の連携が不可欠であり、AIガバナンスと同時並行で進める必要性を指摘。

セキュリティ／リスクマネジメント（SRM）のリーダーは、リスク低減だけでなくビジネス価値の創造やスピードを妨げないバランスの取れたリスクコントロールを設計し、継続的に改善していく必要がある。

AIガバナンスにおいては、経営者がAIリーダーや責任者を任命し、複数部署の連携を促すことが重要。大規模な組織ではAIリーダーを支えるチームやAI委員会の設置も推奨している。

一般従業員向けのリスクマネジメントでは、従業員が理解しやすいガイドラインを作成し、急速な変化に耐えられるようにすることが重要。

開発／運用者向けのリスクマネジメントでは、システムのライフサイクル全体を通してリスクを軽減するためのガイドラインが必要となる。

---

# ストックマーク、1000億パラメータの日本語特化型LLM「Stockmark-2-100B-Instruct-beta」を公開

[View on CodeZine Trend](http://codezine.jp/article/detail/21202)

ストックマークが、経済産業省とNEDOのプロジェクト「GENIAC」で開発中の日本語特化型LLM「Stockmark-2-100B-Instruct-beta」（1000億パラメータ）を公開した。このモデルは、ストックマークがゼロから開発したもので、既存のオープンソースLLMモデルを使用していない。ハルシネーションを大幅に抑制したLLM「Stockmark-LLM-100b」の開発で得られた知見を活かし、ビジネスドメインでの高い回答性能、高い日本語能力と深いビジネス知識を持つ。商用利用可能なオープンソースとして公開され、日本の組織が開発したモデルと比較して高い性能を示す。米MetaのLlama3.1に日本語を追加学習した「Llama 3.1 Swallow」をも上回り、「Stockmark Business Questions」を使用したビジネスドメイン知識の評価では、GPT-4oをわずかに上回る正解率90％を達成した。

---

# 2025年4月に開催される注目のITエンジニア向けカンファレンス5選

[View on CodeZine Trend](http://codezine.jp/article/detail/21203)

2025年4月に開催されるITエンジニア向けの注目カンファレンス5選が紹介されています。

1.  **try! Swift Tokyo 2025**：4月9日～11日に立川で開催されるSwiftの国際カンファレンス。Swift開発者が集まり、最新情報を共有し、交流を深めます。参加費は24,900円（事前登録制）。

2.  **PHPカンファレンス小田原2025**：4月11日（前夜祭）、12日（本編・懇親会）に小田原で開催されるPHPエンジニア向けイベント。「チーム対抗バトル」など参加者同士のコミュニケーションを重視した内容です。参加費は前夜祭3,000円、本編のみ3,000円、本編＋懇親会8,500円（いずれも事前登録制）。

3.  **DevOpsDays Tokyo 2025**：4月15日～17日に大崎で開催されるDevOpsカンファレンスの東京版。自動化、テスト、セキュリティ、組織文化に焦点を当て、国内外の事例が共有されます。現地参加25,000円、オンライン参加15,000円（事前登録制）。

4.  **RubyKaigi 2025**：4月16日～18日に松山で開催されるRubyプログラミング言語に関する国際カンファレンス。Ruby創始者のMatz氏をはじめ、著名な開発者が登壇します。一般30,000円、学生5,000円（事前登録制）。

5.  **Qiita Conference 2025**：4月23日～25日にオンラインで開催されるテックカンファレンス。「ソフトウェア開発に関わる人々の、新たなきっかけを創出する」をコンセプトに、技術的な挑戦や知見が共有されます。参加費は無料（事前登録制）。

記事では、これらのカンファレンスへの参加を推奨し、技術力向上やコミュニティとの連携を促しています。

---

# SaaS開発／運用支援プラットフォーム「SaaSus Platform」、わずか60分でAPI実装ができる新機能を追加

[View on CodeZine Trend](http://codezine.jp/article/detail/21195)

アンチパターン社が提供するSaaS開発・運用支援プラットフォーム「SaaSus Platform」に、API実装を60分で可能にする新機能「Smart API Gateway」が追加された。この機能はAPI開発・公開プロセスを効率化し、SaaSの相互運用性を高める。既存アプリのソースコードに指定のアノテーションを付加することで、SaaSus Platformが各種機能をAPIとして外部公開できる。API提供に必要なインフラや認証機能が自動で提供されるため、API構築や運用ノウハウがなくてもセキュアなAPI公開が可能。主な機能として、APIキーの発行・管理、役割ベースのアクセス制御（RBAC）、エンドポイントごとのスロットリング設定、API仕様の自動解析とドキュメント生成、特定のIPアドレスからのアクセス制限がある。
